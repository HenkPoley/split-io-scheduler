Index: linux-4.4/fs/buffer.c
===================================================================
--- linux-4.4.orig/fs/buffer.c	2016-04-27 09:36:34.391673116 +0200
+++ linux-4.4/fs/buffer.c	2016-04-27 09:36:34.388339783 +0200
@@ -43,6 +43,7 @@
 #include <linux/mpage.h>
 #include <linux/bit_spinlock.h>
 #include <trace/events/block.h>
+#include <linux/cause_tags.h>
 
 static int fsync_buffers_list(spinlock_t *lock, struct list_head *list);
 static int submit_bh_wbc(int rw, struct buffer_head *bh,
@@ -1985,12 +1986,15 @@
 EXPORT_SYMBOL(__block_write_begin);
 
 static int __block_commit_write(struct inode *inode, struct page *page,
-		unsigned from, unsigned to)
+		unsigned from, unsigned to, long pos)
 {
 	unsigned block_start, block_end;
 	int partial = 0;
 	unsigned blocksize;
 	struct buffer_head *bh, *head;
+	struct request_queue *q = NULL;
+	elevator_causes_dirty_fn *causes_dirty_fn = NULL;
+	int cause_added;
 
 	bh = head = page_buffers(page);
 	blocksize = bh->b_size;
@@ -2004,6 +2008,37 @@
 		} else {
 			set_buffer_uptodate(bh);
 			mark_buffer_dirty(bh);
+
+#ifndef DISABLE_CAUSES
+
+			spin_lock(&bh->causes_lock);
+			// add this cause (data write)
+			cause_added = cause_list_add(&bh->causes, current);
+			set_cause_list_type(bh->causes, SPLIT_DATA);
+
+			// dirty hook
+			q = inode_to_request_queue(inode);
+			if (q && bh->causes && cause_added) {
+				spin_lock_irq(q->queue_lock);
+
+				if (bh->causes->callback_q == NULL) {
+					// TODO(tyler): need refcount get on q?
+					bh->causes->callback_q = q;
+					bh->causes->sched_uniq = q->sched_uniq;
+					bh->causes->size = bh->b_size;
+				}
+				// one buffer can't correspond to many disks, right?
+				WARN_ON(bh->causes->callback_q != q);
+
+				causes_dirty_fn = q->elevator->type->ops.elevator_causes_dirty_fn;
+				if (causes_dirty_fn && q->sched_uniq == bh->causes->sched_uniq)
+					causes_dirty_fn(q, bh->causes, current, inode, pos);
+
+				spin_unlock_irq(q->queue_lock);
+			}
+			spin_unlock(&bh->causes_lock);
+
+#endif
 		}
 		clear_buffer_new(bh);
 
@@ -2081,7 +2116,7 @@
 	flush_dcache_page(page);
 
 	/* This could be a short (even 0-length) commit */
-	__block_commit_write(inode, page, start, start+copied);
+	__block_commit_write(inode, page, start, start+copied, pos);
 
 	return copied;
 }
@@ -2399,7 +2434,7 @@
 int block_commit_write(struct page *page, unsigned from, unsigned to)
 {
 	struct inode *inode = page->mapping->host;
-	__block_commit_write(inode,page,from,to);
+	__block_commit_write(inode,page,from,to,-1);
 	return 0;
 }
 EXPORT_SYMBOL(block_commit_write);
@@ -3042,6 +3077,9 @@
 	if (buffer_prio(bh))
 		rw |= REQ_PRIO;
 
+	if (rw & WRITE)
+		move_causes_bh_to_bio(bh, bio);
+
 	submit_bio(rw, bio);
 	return 0;
 }
@@ -3324,6 +3362,12 @@
 {
 	struct buffer_head *ret = kmem_cache_zalloc(bh_cachep, gfp_flags);
 	if (ret) {
+
+#ifndef DISABLE_CAUSES
+		spin_lock_init(&ret->causes_lock);
+		BUG_ON(ret->causes);
+#endif
+
 		INIT_LIST_HEAD(&ret->b_assoc_buffers);
 		preempt_disable();
 		__this_cpu_inc(bh_accounting.nr);
@@ -3336,6 +3380,9 @@
 
 void free_buffer_head(struct buffer_head *bh)
 {
+	put_cause_list(bh->causes);
+	bh->causes = NULL;
+
 	BUG_ON(!list_empty(&bh->b_assoc_buffers));
 	kmem_cache_free(bh_cachep, bh);
 	preempt_disable();
