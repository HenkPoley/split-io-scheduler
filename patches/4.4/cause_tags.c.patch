Index: linux-3.3/block/cause_tags.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-3.3/block/cause_tags.c	2016-04-25 12:03:22.109777693 +0200
@@ -0,0 +1,364 @@
+#include <linux/slab.h>
+#include <linux/sched.h>
+#include <linux/cause_tags.h>
+#include <linux/blk_types.h>
+#include <linux/blkdev.h>
+#include <linux/buffer_head.h>
+#include <linux/workqueue.h>
+#include <linux/module.h>
+#include <linux/elevator.h>
+
+#ifndef DISABLE_CAUSES
+
+static atomic_t cause_list_alloc = ATOMIC_INIT(0);
+static atomic_t cause_list_free = ATOMIC_INIT(0);
+static atomic_t cause_item_alloc = ATOMIC_INIT(0);
+static atomic_t cause_item_free = ATOMIC_INIT(0);
+
+void cause_list_debug(void) {
+	int alloc = atomic_read(&cause_list_alloc);
+	int free = atomic_read(&cause_list_free);
+	printk(KERN_INFO "cause_list alloc=%d, free=%d",
+		   alloc, free);
+}
+EXPORT_SYMBOL(cause_list_debug);
+
+#ifndef DISABLE_CAUSES_DEBUG
+// return 1 if magic is wrong or mem is bad
+int cause_list_check(struct cause_list *cl) {
+	char tmp;
+	if (!cl) {
+		WARN_ON("cannot check_magic on NULL causes");
+		return 1;
+	}
+	if (probe_kernel_address(cl, tmp)) {
+		WARN_ON("invalid causes address");
+		return 1;
+	}
+	if (cl->magic == CAUSES_MAGIC_BAD) {
+		WARN_ON("causes was kfreed'd");
+		return 1;
+	}
+	if (cl->magic != CAUSES_MAGIC_GOOD) {
+		WARN_ON("causes is corrupt");
+		return 1;
+	}
+	return 0;
+}
+EXPORT_SYMBOL(cause_list_check);
+#endif
+
+struct cause_list* __new_cause_list(int line) {
+	struct cause_list* cl = kmalloc(sizeof(*cl), GFP_ATOMIC);
+	if (cl != NULL) {
+		memset(cl, 0, sizeof(*cl));
+#ifndef DISABLE_CAUSES_ALLOC_COUNTERS
+		atomic_inc(&cause_list_alloc);
+#endif
+#ifndef DISABLE_CAUSES_DEBUG
+		cl->magic = CAUSES_MAGIC_GOOD;
+		cl->new_line = line;
+#endif
+		set_cause_list_type(cl, SPLIT_UNKNOWN);
+		INIT_LIST_HEAD(&cl->items);
+		cl->inode = NULL;
+		cl->private = -1;
+		kref_init(&cl->refcount);
+	}
+	return cl;
+}
+EXPORT_SYMBOL(__new_cause_list);
+
+static void free_cause_list(struct kref *ref) {
+	struct cause_list* cl;
+	struct list_head *cause_node;
+	struct list_head *tmp;
+	struct request_queue *q = NULL;
+	elevator_causes_free_fn *causes_free_fn = NULL;
+	cl = container_of(ref, struct cause_list, refcount);
+
+	if (cause_list_check(cl))
+		return;
+
+	// notify sched
+	q = cl->callback_q;
+	if (q) {
+		spin_lock_irq(q->queue_lock);
+		if(q->sched_uniq == cl->sched_uniq){
+
+			causes_free_fn = q->elevator->type->ops.elevator_causes_free_fn;
+			if (causes_free_fn) {
+				causes_free_fn(q, cl);
+			}
+		}
+		spin_unlock_irq(q->queue_lock);
+	}
+
+	// cleanup list
+	list_for_each_safe(cause_node, tmp, &cl->items) {
+		struct io_cause *cause;
+		cause = list_entry(cause_node, struct io_cause, list);
+		list_del(cause_node);
+		kfree(cause);
+		atomic_inc(&cause_item_free);
+	}
+
+#ifndef DISABLE_CAUSES_ALLOC_COUNTERS
+	atomic_inc(&cause_list_free);
+#endif
+#ifndef DISABLE_CAUSES_DEBUG
+	cl->magic = CAUSES_MAGIC_BAD;
+#endif
+	kfree(cl);
+}
+EXPORT_SYMBOL(free_cause_list);
+
+struct cause_list* get_cause_list(struct cause_list* cause_list) {
+	if (cause_list == NULL)
+		return NULL;
+	if (cause_list_check(cause_list))
+		return NULL;
+	kref_get(&cause_list->refcount);
+	return cause_list;
+}
+EXPORT_SYMBOL(get_cause_list);
+
+void put_cause_list(struct cause_list* cause_list) {
+	if (!cause_list)
+		return;
+	if (cause_list_check(cause_list))
+		return;
+	kref_put(&cause_list->refcount, &free_cause_list);
+}
+EXPORT_SYMBOL(put_cause_list);
+
+struct put_cause_list_work {
+	struct cause_list* cause_list;
+	struct work_struct work;
+};
+
+void do_put_cause_list_work(struct work_struct *work){
+	struct put_cause_list_work *put_cause_list_work = container_of(work, struct put_cause_list_work, work);
+	if (cause_list_check(put_cause_list_work->cause_list))
+		return;
+	put_cause_list(put_cause_list_work->cause_list);
+	kfree(put_cause_list_work);
+}
+
+void put_cause_list_safe(struct cause_list* cause_list){
+	struct put_cause_list_work *put_cause_list_work = kmalloc(sizeof(*put_cause_list_work), GFP_ATOMIC);
+	if (!put_cause_list_work) {
+		WARN_ON("could not kmalloc causes safe put struct");
+		return;
+	}
+	if (cause_list_check(cause_list))
+		return;
+
+	put_cause_list_work->cause_list = cause_list;
+	INIT_WORK(&put_cause_list_work->work, do_put_cause_list_work);
+	schedule_work(&put_cause_list_work->work);
+}
+EXPORT_SYMBOL(put_cause_list_safe);
+
+int __cause_list_add(struct cause_list* cause_list,
+					 int account_id) {
+	int already_exists = 0;
+	struct list_head *cause_node;
+    struct io_cause *cause;
+	int rv = 0;
+
+	if (cause_list == NULL)
+		return 0;
+
+	if (cause_list_check(cause_list))
+		return 0;
+
+	// have we already listed this cause?
+    list_for_each(cause_node, &cause_list->items) {
+        cause = list_entry(cause_node, struct io_cause, list);
+		if (cause->account_id == account_id) {
+			already_exists = 1;
+			break;
+		}
+	}
+	if (!already_exists) {
+		cause = kmalloc(sizeof(*cause), GFP_ATOMIC);
+		atomic_inc(&cause_item_alloc);
+		if (cause != NULL) {
+			INIT_LIST_HEAD(&cause->list);
+			cause->account_id = account_id;
+			list_add_tail(&cause->list, &cause_list->items);
+			cause_list->item_count++;
+			rv = 1;
+		}
+	}
+	return rv;
+}
+EXPORT_SYMBOL(__cause_list_add);
+
+// either adds task (normal case) or copies tasks (proxy case)
+//
+// return number of new items added
+int cause_list_add(struct cause_list** cause_list,
+				   struct task_struct* task) {
+	if (!*cause_list)
+		*cause_list = new_cause_list();
+
+	if (task->causes) {
+		// proxy for other causes
+		return cause_list_copy(task->causes, cause_list);
+	} else {
+		return __cause_list_add(*cause_list, atomic_read(&task->account_id));
+	}
+}
+EXPORT_SYMBOL(cause_list_add);
+
+int cause_list_copy(struct cause_list* from,
+					struct cause_list** to) {
+	struct list_head *cause_node;
+    struct io_cause *cause;
+	int rv = 0;
+
+	if (!*to)
+		*to = new_cause_list();
+
+	if (from == NULL || *to == NULL)
+		return rv;
+
+	if (cause_list_check(from))
+		return 0;
+	if (cause_list_check(*to))
+		return 0;
+
+    list_for_each(cause_node, &from->items) {
+        cause = list_entry(cause_node, struct io_cause, list);
+		rv += __cause_list_add(*to, cause->account_id);
+	}
+	return rv;
+}
+EXPORT_SYMBOL(cause_list_copy);
+
+void add_causes_zero_to_bio(struct bio *bio, int size) {
+	struct cause_list* causes;
+	int max_pages_per_bio = queue_max_sectors(bdev_get_queue(bio->bi_bdev)) >> 3;
+
+	if (!bio->cll)
+		bio->cll = new_cause_list_list(max_pages_per_bio);
+	if (!bio->cll)
+		return;
+
+	causes = new_cause_list();
+	if (!causes)
+		return;
+
+	set_cause_list_type(causes, SPLIT_ZERO);
+	causes->size = size;
+	cause_list_list_add(bio->cll, causes);
+	put_cause_list(causes);
+}
+EXPORT_SYMBOL(add_causes_zero_to_bio);
+
+void move_causes_bh_to_bio(struct buffer_head *bh,
+						   struct bio *bio) {
+	int max_pages_per_bio = queue_max_sectors(bdev_get_queue(bio->bi_bdev)) >> 3;
+
+	spin_lock(&bh->causes_lock);
+	if (bh->causes) {
+		if (cause_list_check(bh->causes)) {
+			spin_unlock(&bh->causes_lock);
+			return;
+		}
+
+		if (!bio->cll)
+			bio->cll = new_cause_list_list(max_pages_per_bio);
+
+		WARN_ON(bh->causes->item_count == 0 &&
+				bh->causes->type != SPLIT_JOURNAL);
+
+		bh->causes->size = bh->b_size;
+		cause_list_list_add(bio->cll, bh->causes);
+		put_cause_list(bh->causes);
+		bh->causes = NULL;
+	}
+	spin_unlock(&bh->causes_lock);
+}
+EXPORT_SYMBOL(move_causes_bh_to_bio);
+
+void set_cause_list_type(struct cause_list* cause_list, int type) {
+	if (!cause_list)
+		return;
+	if (cause_list_check(cause_list))
+		return;
+	cause_list->type = type;
+}
+EXPORT_SYMBOL(set_cause_list_type);
+
+///////////////////////////
+// cause list list stuff //
+///////////////////////////
+
+struct cause_list_list *new_cause_list_list(int item_capacity) {
+	struct cause_list_list *cll = NULL;
+	struct cause_list *uniq_causes = NULL;
+	int alloc_size = sizeof(*cll) + sizeof(cll->items[0])*item_capacity;
+	if (item_capacity < 0) {
+		WARN_ON("cannot create negative capacity cll");
+		return NULL;
+	}
+	cll = kmalloc(alloc_size, GFP_ATOMIC);
+	uniq_causes = new_cause_list();
+	if (!cll || !uniq_causes) {
+		kfree(cll);
+		kfree(uniq_causes);
+		return NULL;
+	}
+	memset(cll, 0, alloc_size);
+	// size and item_count are 0
+	cll->uniq_causes = uniq_causes;
+	cll->item_capacity = item_capacity;
+	return cll;
+}
+EXPORT_SYMBOL(new_cause_list_list);
+
+void del_cause_list_list(struct cause_list_list *cll) {
+	int i;
+	if (!cll)
+		return;
+	for(i=0; i<cll->item_count; i++)
+		put_cause_list(cll->items[i]);
+	put_cause_list(cll->uniq_causes);
+	kfree(cll);
+}
+EXPORT_SYMBOL(del_cause_list_list);
+
+void cause_list_list_add(struct cause_list_list *cll,
+						 struct cause_list *cl) {
+	if (!cll || !cl)
+		return;
+	if (cll->item_count == cll->item_capacity) {
+		WARN_ON("cll too small!");
+		return;
+	}
+
+	if (cll->item_count == 0 || cl != cll->items[cll->item_count-1])
+		cause_list_copy(cl, &cll->uniq_causes);
+
+	cll->items[cll->item_count] = get_cause_list(cl);
+	cll->item_count++;
+	cll->size += cl->size;
+	cll->uniq_causes->size = cll->size;
+}
+EXPORT_SYMBOL(cause_list_list_add);
+
+
+struct cause_list_mem_desc get_cause_list_mem(){
+	struct cause_list_mem_desc desc;
+	desc.cause_list_alloc = atomic_read(&cause_list_alloc);
+	desc.cause_list_free = atomic_read(&cause_list_free);
+	desc.cause_item_alloc = atomic_read(&cause_item_alloc);
+	desc.cause_item_free = atomic_read(&cause_item_free);
+	return desc;
+}
+EXPORT_SYMBOL(get_cause_list_mem);
+
+#endif
