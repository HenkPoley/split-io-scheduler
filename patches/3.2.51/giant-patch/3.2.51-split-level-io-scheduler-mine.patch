diff -rubBN linux-3.2.51/arch/sh/boot/compressed/vmlinux.scr ../split-release/release/linux-3.2.51/arch/sh/boot/compressed/vmlinux.scr
--- linux-3.2.51/arch/sh/boot/compressed/vmlinux.scr	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/arch/sh/boot/compressed/vmlinux.scr	1970-01-01 01:00:00.000000000 +0100
@@ -1,10 +0,0 @@
-SECTIONS
-{
-  .rodata..compressed : {
-	input_len = .;
-	LONG(input_data_end - input_data) input_data = .;
-	*(.data)
-	output_len = . - 4;
-	input_data_end = .;
-	}
-}
diff -rubBN linux-3.2.51/arch/sh/boot/romimage/vmlinux.scr ../split-release/release/linux-3.2.51/arch/sh/boot/romimage/vmlinux.scr
--- linux-3.2.51/arch/sh/boot/romimage/vmlinux.scr	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/arch/sh/boot/romimage/vmlinux.scr	1970-01-01 01:00:00.000000000 +0100
@@ -1,8 +0,0 @@
-SECTIONS
-{
-  .text : {
-	zero_page_pos = .;
-	*(.data)
-	end_data = .;
-	}
-}
diff -rubBN linux-3.2.51/block/blk-core.c ../split-release/release/linux-3.2.51/block/blk-core.c
--- linux-3.2.51/block/blk-core.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/block/blk-core.c	2015-09-01 05:42:46.000000000 +0200
@@ -136,6 +136,7 @@
 	rq->__sector = (sector_t) -1;
 	INIT_HLIST_NODE(&rq->hash);
 	RB_CLEAR_NODE(&rq->rb_node);
+	RB_CLEAR_NODE(&rq->expire_rb_node);
 	rq->cmd = rq->__cmd;
 	rq->cmd_len = BLK_MAX_CDB;
 	rq->tag = -1;
diff -rubBN linux-3.2.51/block/blk-lib.c ../split-release/release/linux-3.2.51/block/blk-lib.c
--- linux-3.2.51/block/blk-lib.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/block/blk-lib.c	2015-09-01 05:42:46.000000000 +0200
@@ -6,6 +6,7 @@
 #include <linux/bio.h>
 #include <linux/blkdev.h>
 #include <linux/scatterlist.h>
+#include <linux/cause_tags.h>
 
 #include "blk.h"
 
@@ -155,6 +156,9 @@
 		while (nr_sects != 0) {
 			sz = min((sector_t) PAGE_SIZE >> 9 , nr_sects);
 			ret = bio_add_page(bio, ZERO_PAGE(0), sz << 9, 0);
+
+			add_causes_zero_to_bio(bio, sz << 9);
+
 			nr_sects -= ret >> 9;
 			sector += ret >> 9;
 			if (ret < (sz << 9))
diff -rubBN linux-3.2.51/block/cause_tags.c ../split-release/release/linux-3.2.51/block/cause_tags.c
--- linux-3.2.51/block/cause_tags.c	1970-01-01 01:00:00.000000000 +0100
+++ ../split-release/release/linux-3.2.51/block/cause_tags.c	2015-09-01 05:42:46.000000000 +0200
@@ -0,0 +1,364 @@
+#include <linux/slab.h>
+#include <linux/sched.h>
+#include <linux/cause_tags.h>
+#include <linux/blk_types.h>
+#include <linux/blkdev.h>
+#include <linux/buffer_head.h>
+#include <linux/workqueue.h>
+#include <linux/module.h>
+#include <linux/elevator.h>
+
+#ifndef DISABLE_CAUSES
+
+static atomic_t cause_list_alloc = ATOMIC_INIT(0);
+static atomic_t cause_list_free = ATOMIC_INIT(0);
+static atomic_t cause_item_alloc = ATOMIC_INIT(0);
+static atomic_t cause_item_free = ATOMIC_INIT(0);
+
+void cause_list_debug(void) {
+	int alloc = atomic_read(&cause_list_alloc);
+	int free = atomic_read(&cause_list_free);
+	printk(KERN_INFO "cause_list alloc=%d, free=%d",
+		   alloc, free);
+}
+EXPORT_SYMBOL(cause_list_debug);
+
+#ifndef DISABLE_CAUSES_DEBUG
+// return 1 if magic is wrong or mem is bad
+int cause_list_check(struct cause_list *cl) {
+	char tmp;
+	if (!cl) {
+		WARN_ON("cannot check_magic on NULL causes");
+		return 1;
+	}
+	if (probe_kernel_address(cl, tmp)) {
+		WARN_ON("invalid causes address");
+		return 1;
+	}
+	if (cl->magic == CAUSES_MAGIC_BAD) {
+		WARN_ON("causes was kfreed'd");
+		return 1;
+	}
+	if (cl->magic != CAUSES_MAGIC_GOOD) {
+		WARN_ON("causes is corrupt");
+		return 1;
+	}
+	return 0;
+}
+EXPORT_SYMBOL(cause_list_check);
+#endif
+
+struct cause_list* __new_cause_list(int line) {
+	struct cause_list* cl = kmalloc(sizeof(*cl), GFP_ATOMIC);
+	if (cl != NULL) {
+		memset(cl, 0, sizeof(*cl));
+#ifndef DISABLE_CAUSES_ALLOC_COUNTERS
+		atomic_inc(&cause_list_alloc);
+#endif
+#ifndef DISABLE_CAUSES_DEBUG
+		cl->magic = CAUSES_MAGIC_GOOD;
+		cl->new_line = line;
+#endif
+		set_cause_list_type(cl, SPLIT_UNKNOWN);
+		INIT_LIST_HEAD(&cl->items);
+		cl->inode = NULL;
+		cl->private = -1;
+		kref_init(&cl->refcount);
+	}
+	return cl;
+}
+EXPORT_SYMBOL(__new_cause_list);
+
+static void free_cause_list(struct kref *ref) {
+	struct cause_list* cl;
+	struct list_head *cause_node;
+	struct list_head *tmp;
+	struct request_queue *q = NULL;
+	elevator_causes_free_fn *causes_free_fn = NULL;
+	cl = container_of(ref, struct cause_list, refcount);
+
+	if (cause_list_check(cl))
+		return;
+
+	// notify sched
+	q = cl->callback_q;
+	if (q) {
+		spin_lock_irq(q->queue_lock);
+		if(q->sched_uniq == cl->sched_uniq){
+
+			causes_free_fn = q->elevator->elevator_type->ops.elevator_causes_free_fn;
+			if (causes_free_fn) {
+				causes_free_fn(q, cl);
+			}
+		}
+		spin_unlock_irq(q->queue_lock);
+	}
+
+	// cleanup list
+	list_for_each_safe(cause_node, tmp, &cl->items) {
+		struct io_cause *cause;
+		cause = list_entry(cause_node, struct io_cause, list);
+		list_del(cause_node);
+		kfree(cause);
+		atomic_inc(&cause_item_free);
+	}
+
+#ifndef DISABLE_CAUSES_ALLOC_COUNTERS
+	atomic_inc(&cause_list_free);
+#endif
+#ifndef DISABLE_CAUSES_DEBUG
+	cl->magic = CAUSES_MAGIC_BAD;
+#endif
+	kfree(cl);
+}
+EXPORT_SYMBOL(free_cause_list);
+
+struct cause_list* get_cause_list(struct cause_list* cause_list) {
+	if (cause_list == NULL)
+		return NULL;
+	if (cause_list_check(cause_list))
+		return NULL;
+	kref_get(&cause_list->refcount);
+	return cause_list;
+}
+EXPORT_SYMBOL(get_cause_list);
+
+void put_cause_list(struct cause_list* cause_list) {
+	if (!cause_list)
+		return;
+	if (cause_list_check(cause_list))
+		return;
+	kref_put(&cause_list->refcount, &free_cause_list);
+}
+EXPORT_SYMBOL(put_cause_list);
+
+struct put_cause_list_work {
+	struct cause_list* cause_list;
+	struct work_struct work;
+};
+
+void do_put_cause_list_work(struct work_struct *work){
+	struct put_cause_list_work *put_cause_list_work = container_of(work, struct put_cause_list_work, work);
+	if (cause_list_check(put_cause_list_work->cause_list))
+		return;
+	put_cause_list(put_cause_list_work->cause_list);
+	kfree(put_cause_list_work);
+}
+
+void put_cause_list_safe(struct cause_list* cause_list){
+	struct put_cause_list_work *put_cause_list_work = kmalloc(sizeof(*put_cause_list_work), GFP_ATOMIC);
+	if (!put_cause_list_work) {
+		WARN_ON("could not kmalloc causes safe put struct");
+		return;
+	}
+	if (cause_list_check(cause_list))
+		return;
+
+	put_cause_list_work->cause_list = cause_list;
+	INIT_WORK(&put_cause_list_work->work, do_put_cause_list_work);
+	schedule_work(&put_cause_list_work->work);
+}
+EXPORT_SYMBOL(put_cause_list_safe);
+
+int __cause_list_add(struct cause_list* cause_list,
+					 int account_id) {
+	int already_exists = 0;
+	struct list_head *cause_node;
+    struct io_cause *cause;
+	int rv = 0;
+
+	if (cause_list == NULL)
+		return 0;
+
+	if (cause_list_check(cause_list))
+		return 0;
+
+	// have we already listed this cause?
+    list_for_each(cause_node, &cause_list->items) {
+        cause = list_entry(cause_node, struct io_cause, list);
+		if (cause->account_id == account_id) {
+			already_exists = 1;
+			break;
+		}
+	}
+	if (!already_exists) {
+		cause = kmalloc(sizeof(*cause), GFP_ATOMIC);
+		atomic_inc(&cause_item_alloc);
+		if (cause != NULL) {
+			INIT_LIST_HEAD(&cause->list);
+			cause->account_id = account_id;
+			list_add_tail(&cause->list, &cause_list->items);
+			cause_list->item_count++;
+			rv = 1;
+		}
+	}
+	return rv;
+}
+EXPORT_SYMBOL(__cause_list_add);
+
+// either adds task (normal case) or copies tasks (proxy case)
+//
+// return number of new items added
+int cause_list_add(struct cause_list** cause_list,
+				   struct task_struct* task) {
+	if (!*cause_list)
+		*cause_list = new_cause_list();
+
+	if (task->causes) {
+		// proxy for other causes
+		return cause_list_copy(task->causes, cause_list);
+	} else {
+		return __cause_list_add(*cause_list, atomic_read(&task->account_id));
+	}
+}
+EXPORT_SYMBOL(cause_list_add);
+
+int cause_list_copy(struct cause_list* from,
+					struct cause_list** to) {
+	struct list_head *cause_node;
+    struct io_cause *cause;
+	int rv = 0;
+
+	if (!*to)
+		*to = new_cause_list();
+
+	if (from == NULL || *to == NULL)
+		return rv;
+
+	if (cause_list_check(from))
+		return 0;
+	if (cause_list_check(*to))
+		return 0;
+
+    list_for_each(cause_node, &from->items) {
+        cause = list_entry(cause_node, struct io_cause, list);
+		rv += __cause_list_add(*to, cause->account_id);
+	}
+	return rv;
+}
+EXPORT_SYMBOL(cause_list_copy);
+
+void add_causes_zero_to_bio(struct bio *bio, int size) {
+	struct cause_list* causes;
+	int max_pages_per_bio = queue_max_sectors(bdev_get_queue(bio->bi_bdev)) >> 3;
+
+	if (!bio->cll)
+		bio->cll = new_cause_list_list(max_pages_per_bio);
+	if (!bio->cll)
+		return;
+
+	causes = new_cause_list();
+	if (!causes)
+		return;
+
+	set_cause_list_type(causes, SPLIT_ZERO);
+	causes->size = size;
+	cause_list_list_add(bio->cll, causes);
+	put_cause_list(causes);
+}
+EXPORT_SYMBOL(add_causes_zero_to_bio);
+
+void move_causes_bh_to_bio(struct buffer_head *bh,
+						   struct bio *bio) {
+	int max_pages_per_bio = queue_max_sectors(bdev_get_queue(bio->bi_bdev)) >> 3;
+
+	spin_lock(&bh->causes_lock);
+	if (bh->causes) {
+		if (cause_list_check(bh->causes)) {
+			spin_unlock(&bh->causes_lock);
+			return;
+		}
+
+		if (!bio->cll)
+			bio->cll = new_cause_list_list(max_pages_per_bio);
+
+		WARN_ON(bh->causes->item_count == 0 &&
+				bh->causes->type != SPLIT_JOURNAL);
+
+		bh->causes->size = bh->b_size;
+		cause_list_list_add(bio->cll, bh->causes);
+		put_cause_list(bh->causes);
+		bh->causes = NULL;
+	}
+	spin_unlock(&bh->causes_lock);
+}
+EXPORT_SYMBOL(move_causes_bh_to_bio);
+
+void set_cause_list_type(struct cause_list* cause_list, int type) {
+	if (!cause_list)
+		return;
+	if (cause_list_check(cause_list))
+		return;
+	cause_list->type = type;
+}
+EXPORT_SYMBOL(set_cause_list_type);
+
+///////////////////////////
+// cause list list stuff //
+///////////////////////////
+
+struct cause_list_list *new_cause_list_list(int item_capacity) {
+	struct cause_list_list *cll = NULL;
+	struct cause_list *uniq_causes = NULL;
+	int alloc_size = sizeof(*cll) + sizeof(cll->items[0])*item_capacity;
+	if (item_capacity < 0) {
+		WARN_ON("cannot create negative capacity cll");
+		return NULL;
+	}
+	cll = kmalloc(alloc_size, GFP_ATOMIC);
+	uniq_causes = new_cause_list();
+	if (!cll || !uniq_causes) {
+		kfree(cll);
+		kfree(uniq_causes);
+		return NULL;
+	}
+	memset(cll, 0, alloc_size);
+	// size and item_count are 0
+	cll->uniq_causes = uniq_causes;
+	cll->item_capacity = item_capacity;
+	return cll;
+}
+EXPORT_SYMBOL(new_cause_list_list);
+
+void del_cause_list_list(struct cause_list_list *cll) {
+	int i;
+	if (!cll)
+		return;
+	for(i=0; i<cll->item_count; i++)
+		put_cause_list(cll->items[i]);
+	put_cause_list(cll->uniq_causes);
+	kfree(cll);
+}
+EXPORT_SYMBOL(del_cause_list_list);
+
+void cause_list_list_add(struct cause_list_list *cll,
+						 struct cause_list *cl) {
+	if (!cll || !cl)
+		return;
+	if (cll->item_count == cll->item_capacity) {
+		WARN_ON("cll too small!");
+		return;
+	}
+
+	if (cll->item_count == 0 || cl != cll->items[cll->item_count-1])
+		cause_list_copy(cl, &cll->uniq_causes);
+
+	cll->items[cll->item_count] = get_cause_list(cl);
+	cll->item_count++;
+	cll->size += cl->size;
+	cll->uniq_causes->size = cll->size;
+}
+EXPORT_SYMBOL(cause_list_list_add);
+
+
+struct cause_list_mem_desc get_cause_list_mem(){
+	struct cause_list_mem_desc desc;
+	desc.cause_list_alloc = atomic_read(&cause_list_alloc);
+	desc.cause_list_free = atomic_read(&cause_list_free);
+	desc.cause_item_alloc = atomic_read(&cause_item_alloc);
+	desc.cause_item_free = atomic_read(&cause_item_free);
+	return desc;
+}
+EXPORT_SYMBOL(get_cause_list_mem);
+
+#endif
diff -rubBN linux-3.2.51/block/cfq-iosched.c ../split-release/release/linux-3.2.51/block/cfq-iosched.c
--- linux-3.2.51/block/cfq-iosched.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/block/cfq-iosched.c	2015-09-01 05:42:46.000000000 +0200
@@ -305,6 +305,11 @@
 
 	/* Number of groups which are on blkcg->blkg_list */
 	unsigned int nr_blkcg_linked_grps;
+
+	/*
+	 * yangsuli added tracing facility
+	 */
+	int prio_served_rqs[IOPRIO_BE_NR];
 };
 
 static struct cfq_group *cfq_get_next_cfqg(struct cfq_data *cfqd);
@@ -610,6 +615,7 @@
 cfq_scaled_cfqq_slice(struct cfq_data *cfqd, struct cfq_queue *cfqq)
 {
 	unsigned slice = cfq_prio_to_slice(cfqd, cfqq);
+	//printk("yangsuli cfq_prio_to_slice returned %d for cfq %p prio %d\n", slice, cfqq, cfqq->ioprio);
 	if (cfqd->cfq_latency) {
 		/*
 		 * interested queues (we consider only the ones with the same
@@ -644,6 +650,8 @@
 	cfqq->slice_start = jiffies;
 	cfqq->slice_end = jiffies + slice;
 	cfqq->allocated_slice = slice;
+	//printk("yangsuli cfqq:%p of prio %lu slice set to %d\n", cfqq,
+	//		IOPRIO_PRIO_DATA(cfqq->ioprio), cfqq->allocated_slice);
 	cfq_log_cfqq(cfqd, cfqq, "set_slice=%lu", cfqq->slice_end - jiffies);
 }
 
@@ -1720,6 +1728,8 @@
 	if (cfqq) {
 		cfq_log_cfqq(cfqd, cfqq, "set_active wl_prio:%d wl_type:%d",
 				cfqd->serving_prio, cfqd->serving_type);
+	//	printk("yangsuli set_active cfqq:%p prio:%lu at %lu\n",
+	//			cfqq, IOPRIO_PRIO_DATA(cfqq->ioprio), jiffies);
 		cfq_blkiocg_update_avg_queue_size_stats(&cfqq->cfqg->blkg);
 		cfqq->slice_start = 0;
 		cfqq->dispatch_start = jiffies;
@@ -2333,8 +2343,10 @@
 	/*
 	 * We were waiting for group to get backlogged. Expire the queue
 	 */
-	if (cfq_cfqq_wait_busy(cfqq) && !RB_EMPTY_ROOT(&cfqq->sort_list))
+	if (cfq_cfqq_wait_busy(cfqq) && !RB_EMPTY_ROOT(&cfqq->sort_list)){
+		//printk("yangsuli waiting for group to get backlooged cause queue expeire: cfqq: %p prio: %lu\n", cfqq, IOPRIO_PRIO_DATA(cfqq->ioprio));
 		goto expire;
+	}
 
 	/*
 	 * The active queue has run out of time, expire it and select new.
@@ -2374,6 +2386,7 @@
 	if (new_cfqq) {
 		if (!cfqq->new_cfqq)
 			cfq_setup_merge(cfqq, new_cfqq);
+		//printk("yangsuli another queue has request waiting within mean seek distance cause queue expeire: cfqq: %p prio: %lu\n", cfqq, IOPRIO_PRIO_DATA(cfqq->ioprio));
 		goto expire;
 	}
 
@@ -2416,6 +2429,7 @@
 	}
 
 expire:
+	//printk("yangsuli code path fall cause queue expeire: cfqq: %p prio: %lu\n", cfqq, IOPRIO_PRIO_DATA(cfqq->ioprio));
 	cfq_slice_expired(cfqd, 0);
 new_queue:
 	/*
@@ -2591,6 +2605,8 @@
 	 */
 	cfq_dispatch_insert(cfqd->queue, rq);
 
+	cfqd->prio_served_rqs[cfqq->ioprio]++;
+
 	if (!cfqd->active_cic) {
 		struct cfq_io_context *cic = RQ_CIC(rq);
 
@@ -2626,6 +2642,8 @@
 	if (!cfq_dispatch_request(cfqd, cfqq))
 		return 0;
 
+	//printk("yangsuli dispatched a request from cfqq %p prio %lu\n", cfqq, IOPRIO_PRIO_DATA(cfqq->ioprio));
+
 	cfqq->slice_dispatch++;
 	cfq_clear_cfqq_must_dispatch(cfqq);
 
@@ -3507,6 +3525,17 @@
 	struct cfq_data *cfqd = q->elevator->elevator_data;
 	struct cfq_queue *cfqq = RQ_CFQQ(rq);
 
+	/*
+	int raw_prio = ((struct cfq_io_context*)rq->elevator_private[0])->ioc->ioprio;
+	int prio;
+	if(ioprio_valid(raw_prio)){
+        	prio = IOPRIO_PRIO_DATA(raw_prio);
+	}else{
+		prio = IOPRIO_NORM;
+	}
+	//printk("cfq_insert_request called. execname %s data_size %d prio %d\n", current->comm, blk_rq_bytes(rq), prio);
+	*/
+
 	cfq_log_cfqq(cfqd, cfqq, "insert_request");
 	cfq_init_prio_data(cfqq, RQ_CIC(rq)->ioc);
 
@@ -3933,6 +3962,11 @@
 	struct cfq_data *cfqd = e->elevator_data;
 	struct request_queue *q = cfqd->queue;
 	bool wait = false;
+	int i;
+
+	for ( i = 0; i < IOPRIO_BE_NR; i++){
+		printk("cfq prio: %d req_num: %d\n", i, cfqd->prio_served_rqs[i]);
+	}
 
 	cfq_shutdown_timer_wq(cfqd);
 
@@ -4111,8 +4145,13 @@
 	cfqd->cfq_slice_async_rq = cfq_slice_async_rq;
 	cfqd->cfq_slice_idle = cfq_slice_idle;
 	cfqd->cfq_group_idle = cfq_group_idle;
-	cfqd->cfq_latency = 1;
+	cfqd->cfq_latency = 0;
 	cfqd->hw_tag = -1;
+	
+	for (i = 0; i < IOPRIO_BE_NR; i++){
+		cfqd->prio_served_rqs[i] = 0;
+	}
+
 	/*
 	 * we optimistically start assuming sync ops weren't delayed in last
 	 * second, in order to have larger depth for async operations.
diff -rubBN linux-3.2.51/block/elevator.c ../split-release/release/linux-3.2.51/block/elevator.c
--- linux-3.2.51/block/elevator.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/block/elevator.c	2015-09-01 05:42:46.000000000 +0200
@@ -177,6 +177,9 @@
 static void elevator_attach(struct request_queue *q, struct elevator_queue *eq,
 			   void *data)
 {
+	q->sched_uniq++;
+	if (q->sched_uniq < 0)
+		q->sched_uniq = 1;
 	q->elevator = eq;
 	eq->elevator_data = data;
 }
@@ -284,6 +287,7 @@
 		return -ENOMEM;
 	}
 
+	q->sched_uniq = 1000; // arbitrary start num
 	elevator_attach(q, eq, data);
 	return 0;
 }
@@ -466,6 +470,18 @@
 }
 EXPORT_SYMBOL(elv_dispatch_add_tail);
 
+void elv_dispatch_add_head(struct request_queue *q, struct request *rq)
+{
+	if (q->last_merge == rq)
+		q->last_merge = NULL;
+
+	elv_rqhash_del(q, rq);
+
+	q->nr_sorted--;
+	list_add(&rq->queuelist, &q->queue_head);
+}
+EXPORT_SYMBOL(elv_dispatch_add_head);
+
 int elv_merge(struct request_queue *q, struct request **req, struct bio *bio)
 {
 	struct elevator_queue *e = q->elevator;
@@ -1099,3 +1115,92 @@
 	return NULL;
 }
 EXPORT_SYMBOL(elv_rb_latter_request);
+
+
+struct block_device *btrfs_inode_to_bdev(struct inode*);
+struct request_queue *inode_to_request_queue(struct inode *inode) {
+	struct request_queue *rq = NULL;
+	struct block_device *bdev = NULL;
+	if (!inode)
+		return NULL;
+	if(!S_ISREG(inode->i_mode) && !S_ISDIR(inode->i_mode)){
+		return NULL;
+	}
+	if (!inode->i_sb)
+		return NULL;
+
+	if(inode->i_sb->s_type &&
+			inode->i_sb->s_type->name &&
+			!strcmp(inode->i_sb->s_type->name, "btrfs")){
+		bdev = btrfs_inode_to_bdev(inode);
+	}else{
+		bdev = inode->i_sb->s_bdev;
+	}
+
+	if (!bdev)
+		return NULL;
+
+	rq = bdev_get_queue(bdev);
+
+	if(!rq || !rq->elevator || !rq->elevator->elevator_type)
+		return NULL;
+
+	return rq;
+
+}
+
+
+void get_elevator_call_info_from_inode(struct inode* inode,
+										  struct request_queue **rq,
+										  struct module **module,
+										  struct elevator_syscall_ops *sops) {
+	   *rq = inode_to_request_queue(inode);
+
+	if(!(*rq))
+		goto skip;
+
+	spin_lock_irq((*rq)->queue_lock);
+	*module = (*rq)->elevator->elevator_type->elevator_owner;
+	if(*module && try_module_get(*module)){
+		sops->sched_uniq = (*rq)->sched_uniq;
+		sops->read_entry_fn = (*rq)->elevator->elevator_type->ops.elevator_read_entry_fn;
+		sops->write_entry_fn = (*rq)->elevator->elevator_type->ops.elevator_write_entry_fn;
+		sops->fsync_entry_fn = (*rq)->elevator->elevator_type->ops.elevator_fsync_entry_fn;
+		sops->mkdir_entry_fn = (*rq)->elevator->elevator_type->ops.elevator_mkdir_entry_fn;
+		sops->create_entry_fn = (*rq)->elevator->elevator_type->ops.elevator_create_entry_fn;
+		sops->read_return_fn = (*rq)->elevator->elevator_type->ops.elevator_read_return_fn;
+		sops->write_return_fn = (*rq)->elevator->elevator_type->ops.elevator_write_return_fn;
+		sops->fsync_return_fn = (*rq)->elevator->elevator_type->ops.elevator_fsync_return_fn;
+		sops->mkdir_return_fn = (*rq)->elevator->elevator_type->ops.elevator_mkdir_return_fn;
+		sops->create_return_fn = (*rq)->elevator->elevator_type->ops.elevator_create_return_fn;
+	}else{
+		spin_unlock_irq((*rq)->queue_lock);
+		goto skip;
+	}
+	spin_unlock_irq((*rq)->queue_lock);
+	return;
+
+ skip:
+	*rq = NULL;
+	*module = NULL;
+	memset(sops, 0, sizeof(*sops));
+}
+void get_elevator_call_info(struct file* filp,
+										  struct request_queue **rq,
+										  struct module **module,
+										  struct elevator_syscall_ops *sops) {
+	if (!filp)
+		goto skip;
+	if (!filp->f_mapping)
+		goto skip;
+	if (!filp->f_mapping->host)
+		goto skip;
+
+	get_elevator_call_info_from_inode(filp->f_mapping->host, rq, module, sops);
+	return;
+
+ skip:
+	*rq = NULL;
+	*module = NULL;
+	memset(sops, 0, sizeof(*sops));
+}
diff -rubBN linux-3.2.51/block/Makefile ../split-release/release/linux-3.2.51/block/Makefile
--- linux-3.2.51/block/Makefile	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/block/Makefile	2015-09-01 05:42:46.000000000 +0200
@@ -5,7 +5,8 @@
 obj-$(CONFIG_BLOCK) := elevator.o blk-core.o blk-tag.o blk-sysfs.o \
 			blk-flush.o blk-settings.o blk-ioc.o blk-map.o \
 			blk-exec.o blk-merge.o blk-softirq.o blk-timeout.o \
-			blk-iopoll.o blk-lib.o ioctl.o genhd.o scsi_ioctl.o
+			blk-iopoll.o blk-lib.o ioctl.o genhd.o scsi_ioctl.o \
+            cause_tags.o
 
 obj-$(CONFIG_BLK_DEV_BSG)	+= bsg.o
 obj-$(CONFIG_BLK_DEV_BSGLIB)	+= bsg-lib.o
Binary files linux-3.2.51/Documentation/DocBook/media/dvb/dvbstb.pdf and ../split-release/release/linux-3.2.51/Documentation/DocBook/media/dvb/dvbstb.pdf differ
Binary files linux-3.2.51/Documentation/DocBook/media/v4l/crop.pdf and ../split-release/release/linux-3.2.51/Documentation/DocBook/media/v4l/crop.pdf differ
Binary files linux-3.2.51/Documentation/DocBook/media/v4l/fieldseq_bt.pdf and ../split-release/release/linux-3.2.51/Documentation/DocBook/media/v4l/fieldseq_bt.pdf differ
diff -rubBN linux-3.2.51/Documentation/DocBook/media/v4l/fieldseq_tb.pdf ../split-release/release/linux-3.2.51/Documentation/DocBook/media/v4l/fieldseq_tb.pdf
--- linux-3.2.51/Documentation/DocBook/media/v4l/fieldseq_tb.pdf	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/Documentation/DocBook/media/v4l/fieldseq_tb.pdf	1970-01-01 01:00:00.000000000 +0100
@@ -1,151 +0,0 @@
-%PDF-1.4
-%Çì¢
-5 0 obj
-<</Length 6 0 R/Filter /FlateDecode>>
-stream
-xœ­œQoÜ¸…ßõ+ôÖ$¨Q"E©oIë.¶p7›Ì }èAº¶“v²q¼İ¿_jœ±ÎØ>ç¸@`ó‰¢./ïœ|i»ÛnùóíçÏ—Íó7¥}ÿµéÂÔwCìw/_½o†2Ï!¥6—Ø‡©´SîÂĞ·©9æ±½:kÎŸ5CÈihkvÊöÍwx×feÆ¾„)³…@”ëËS4"
-(åÊdDQÉ9³Qû¨Ô_®„}4åå—¦„¢®½»5
-mnbel5ĞRR(³î3mVFõ™‡@¤úÌC(}æ!©Áè!ì#1ö<„·&Æ‡@¤Æ‡6<u%‡8˜€¥Ğùi—´)=”AM—Cwøxæ|ÏÃ ÕûÉŸ‚!c	c6=D!uCF—d(„}”JÈ&Épûh¬/›[ãÏV)ån"º}ŠZã>¹·ÙŠC êÇz1çQœêÅŒ‡1xcs
-“ĞBQ7Û\Å!ì¡>Ù\Å!ì¡a¶¹ŠC÷sÕş=}œÒZ[Ø÷ãL“Œ‡xÜ÷õM!›på¸õ!§ÍÃáhñ#Í%-®Á¢{XR-GšF=$ZœklGS>pH´8M1Œ.*($Z<Öaíz‚1¢½y¬çb‚Bª½5Ÿõƒw-sÅöƒ 3”a…ÌˆQLuÈ˜y‚C¢K©ÏÓE)…6Í<a˜M="•M=tÿ©®ïÙÔC÷ûğö=*›zH´XeS‰«lê!Õb‘M=$Z¬²©‡D‹U6õh±È¦–íUÙÔCª½"›zH´XeSAfPÙÔC˜«D6õèC•M=Äg¤¡ÔÏâ®üáĞ:‘cémåÇ!å©Ø¢—C(êz[†sD%&[Éqû¨ŸmË!¼µ”èb(åz1wkâë5²Ï<´~ö—}æ!©>óˆTŸyDjìyûHŒ=a‰±ç!‰±ç!ó‡œÃÄ¢‡0ç/zW©PhÅ)×Abr>‡@Ôu¡¸‡C«hˆõ§¹3Ê`½<…¡s…àÆÒP§%3yqhå)ô©ïOaşQä!ü&¢ÈC«HF‘‡@¤¢ÈCĞG"Š,ƒkX"Š<7¦¢ÈC°G¡¢ÈC›&.Ÿúö·f®¿œÆÒ¦n.Ká59y¡sˆß®ÚO}­öî2ù€IãÜ‡~’=Ów5yvñ–)c®/Ï‡;„ò!´oµ4í¡!Å‚ĞC·vÈ<|kÊóÍS’Ì4¡>ÅÔ©rux<æZ©”²ÌÏêZŠÙ_K1wk£S»ÿ}Ôf¼‡7+£ö="µè!©}@Hí'{ûHì'{ûHì'{Eb?ÙC¼F’}æ!˜oUŸyDªÏ<„"Ñg‘ƒÂ>cÏCxkbìyDjìyHä-µï¡5òÕ¼eP#öà="µï!ÒjŞC R{ğÂ>{ğÂ>{ğâÙJîÁ{h{¹ï!©=xHìÁ[oLìÁ{EbŞCØCbŞCØCbŞC»¨
-yìr*7¹*Îõ£D1s ‡jØÆc?)tw­ÕïPùJ5VLFÚ¬Œš°=„"1«yDjæ÷ŠÄôè!‰ÂC Ró¬‡@¤fHMĞB‘˜~<´ŠÔLoÔˆYÌC(%ƒ‡@¤¦C¡HÔZEr5ÙC(K÷‘Z–öŠÄ€‡@¤Ö·=„"±™à!‰…rHíJxVµöK$±+S‡‡×‡ök$Ú/’Hh¿ÒT/[úqÔ[‡Ğ…­ıZ“4±ó*cBbÛßûEãáì)¤Á…¦œSœêÅ:³^É!XÑ§PÇ­YÒ¥Ğ*çfw–“C Š]˜İé#Ag÷õb½YõætvªâÎæsh­le<yDyjmG‚ı%G‚>Rqä!©8òt¶Š#Ag«8òĞıÏ
-û‡<èÇAmVÏÜì¡Ûˆx…ªõ¼o…@„'­¸‰S Âcf\Å)Pá;®â¨ğ€!ïtN
-=òVqjU2ª¨0§q§@…#–«8µª$ÿôÎ)Páğç*N­ªƒlKƒAP Â\B[%¨Uuº©JP®Àê…X=¡>JX¿’R¨‚¡¢¨pƒ«8…*XÅ*J¡
-Ö1„ŠR Â•®âÆ*¬æò°ç¨pÙ“‡=§PKÃBE)TÁªPQ
-T¸ÎÌUœB,È
-¥@«ÖÜD!ÁÚ®0Q
-Ò®mğ´Å)TÁê†PQ
-T¸¾ÁUœB¬p¥@…kt
-
-U°ÊÁ[Å)TÁ:‡PQ
-T¸ÒÁUœ‚2~‹Š›RX&ÃatQ'S
-e8‰/
-eJa¡_B…2¥°P†o`ˆB™R¨‚¯Ÿ¥°æ†ïŞp§°æ†Ãû¢æ¦F;œ/ÑN)PÁ9<n¢Æ:U±N)ìt8Ó':R«êàÜ#U	
-Tx@«8*<DÉUœ‚Ú
-OÒt,(¾·K}Û<öĞºÀÇ2ÔH4"
-(Oõbf¥šC(ê†e9Îˆ¢Z©Ô‡dDÑ0ôIQÍ¶ôËXQú-4ñÃ@r—ÈCëÁ¹¹ã!©=Hm¥xDjKÎCØGb'ÍCØGbÌC(ûVZE2?yD*?yD*?yE"?yD*?yD*?yD*?yD*?yhıÆ‘Œ#HÅ‘‡@¤âÈC(qä!©8òˆTyD*<"G_8S9ÜC°k¦r¸‡@¤r¸‡P$r¸‡@¤ja‰ZÀCxk¢ğˆT-à!(ÄJÃ`Îøqëğ!Ò/fyËç4²Ó©–ÁâyŒô«éÂâ¹ŒáÎ×äîÎáÏs½˜9wÄ!ucˆî³…°
-ï£=›Å!ØOYg2NÂéN„“‡p–âádœ£D8yç(NÂ9J„“‡P$ÂÉC8Ù‰pòß"Š<„Å·ˆ"aÍÌ£È2X1‹(òVÌ"Š<„³ˆ"¡HD‘‡°ôQä¡Mó¥Mc×uíå®zºhò8Â_v¿ºh>4©LãRQæi.KqsÙ¤iîÂ8İ¾r±¾2çZX//ìßÄ_˜JÜëšûRFäVû¡ùû³öÓz¶q(eZúeÿ¥)S­	A²i^×Ûyÿ`«éï5ò¶»ccÿÓ‡}•ºÌ_šxó÷o?~¾l_n›ço¦úÄÂ<Ïíö¼¹ùÇ¡c›ÃTŸårn©Î3%¶ÛËæÉßÒIÿöÏßŸüéíöÕO·ÿnÆúÄûœÚíÉÁo_¾Ún_ıõéï^|q²=~óÃ‹íq}}¹æ²09×¨^4…~!G·¾ÓæÉÒEÿö_¿ŸŸ]…óg§X®ûPrË·Ïşq·IGq.]¶s§¸œ‡~ø-iüŸßòàUş¹ıË=Úß\_ÇûğíŞ¶g—¿|¾zwÑ~¾:=»ú}{ıù—vwõ¿W_¯Ûë«wŸ¾^~¼¾>;mzrŞ‡öåwÏ|qòÓÓ¥rkÜÏw»}süúíöeíŞı%cĞı¼»"`ßÿPÄÉ‹?×Vo«şòã×wß:;½±­o¯Uü0ñ÷·ÏÛ‡½µUt¼­Ãâuó_ˆq”0endstream
-endobj
-6 0 obj
-2760
-endobj
-4 0 obj
-<</Type/Page/MediaBox [0 0 566 600]
-/Parent 3 0 R
-/Resources<</ProcSet[/PDF /Text]
-/ExtGState 10 0 R
-/Font 11 0 R
->>
-/Contents 5 0 R
->>
-endobj
-3 0 obj
-<< /Type /Pages /Kids [
-4 0 R
-] /Count 1
->>
-endobj
-1 0 obj
-<</Type /Catalog /Pages 3 0 R
-/Metadata 13 0 R
->>
-endobj
-7 0 obj
-<</Type/ExtGState
-/OPM 1>>endobj
-10 0 obj
-<</R7
-7 0 R>>
-endobj
-11 0 obj
-<</R8
-8 0 R>>
-endobj
-8 0 obj
-<</BaseFont/SEXXTC+Helvetica/FontDescriptor 9 0 R/Type/Font
-/FirstChar 32/LastChar 118/Widths[
-278 0 0 0 0 0 0 0 333 333 0 0 278 0 278 278
-0 0 556 0 556 0 0 0 0 0 278 0 0 0 0 0
-0 667 667 722 722 667 611 778 0 278 0 0 556 833 722 778
-667 778 722 667 611 0 667 0 0 0 0 0 0 0 0 556
-0 556 556 0 556 556 278 556 0 222 0 0 222 833 556 556
-556 0 333 500 278 556 500]
-/Encoding/WinAnsiEncoding/Subtype/Type1>>
-endobj
-9 0 obj
-<</Type/FontDescriptor/FontName/SEXXTC+Helvetica/FontBBox[-22 -218 762 741]/Flags 4
-/Ascent 741
-/CapHeight 741
-/Descent -218
-/ItalicAngle 0
-/StemV 114
-/MissingWidth 278
-/CharSet(/A/B/C/D/E/F/G/I/L/M/N/O/P/Q/R/S/T/V/a/b/colon/comma/d/e/f/four/g/i/l/m/n/o/p/parenleft/parenright/period/r/s/slash/space/t/two/u/underscore/v)/FontFile3 12 0 R>>
-endobj
-12 0 obj
-<</Filter/FlateDecode
-/Subtype/Type1C/Length 2959>>stream
-xœViTW®²éª”>µ› Q$²¸ € ‹€
-¢¢( l*İ Šq‰bŒúÜbL4n¸†Œ‚[4ˆ
-ˆF0¨€6`78mâ(	Œ÷õyÍ™yf&É¯9§Ï©~Uõnİ÷İïûîe³~Ë²æ!Éé¹ÉÙË–&šVx‹‡öÃÃ$9DeØmÈ“cÂ¼€å–²” K³Â¡†Ú@Û øÌædÌXÖÉgúª˜¨yÑ)Ù«³’G;ddæe-KMËvãããí°$Ï>\¦\’³Ê!:QµÊ!Ü!*95'=1ëO7ÿ·íÿÇ0ŒkøTUÄ´Œ¤È€Ìä™)³‚²RgO_œ½,:gNhnzâ\åOgïQ£İ&ºÇÓ÷Gf&ãÆgf1AŒ;ãÄÌf¦3ÌGL03†ÁD3!Œ'3’™Ãx1ÎLÆx3s™±Œ3‰`¦1ã™H&€‘1ı‘1gXf cÍa2S7
-%cÆÈéÖ½Ìv4»’­bßö‹éwQÒ_’$)”t™M0K0;,u’î’^à†rk¹—ü8>–×	rÁWˆrÔdÿ°ş'û¿3ŸcÌËÌ»ÌßYØY8[Z,¶È„"+\4^Í>ÓK`ªA)ºõ(q¬†9+ƒ;RÃÂ&ì¡f±
-2Ä'$CÚÅ‘ı¥”Xrä`Rú†k„)|Ù$’uXB›ÔO£lêı[aw‹¬±ˆÆ‚V¸Â¿»]ÿ¢æÊÒp9ùwvàµa·GÈem~(V¹hš ã­‘Ú¤fŸèğ>§ÙB"©t%y’ÖcMÒÖR"ö(u8Xj©)MìÑÀ:M³šõ‘z;Yw³-h¸:tóØÅ+ÅÔ·PÒ--w+døaÑ¹ò' 7á±—“2NOŠ°Bø7¥)¾¾q)Î
-B^ˆ°Vó²nMYœ¯ß‚8=Wv#İÈk¡\+“¸X$Î^$é†Ã(pnï†):¶‹ŒVìô_İö'¶dàlw·¨F¬ªQß¤	»4¿š=¥…[-œO!Ÿ@NpiåÑg‚@†z–L"A/?İ8]}M1ı1OÖâD±£Ê—Ø³'y¹G>+°º÷ü¥ÂÊÒ‡ÙS–ë$†h</£3ÂzéN:0ê¤İ´ )ÙüZÃ^×K.‰*ğvo"ŞhòM™:sD,Kœj«fÿ”ù3‚éèí/Ç!Fø˜Ë_¸yÅUzxdÚDšæGî @ÌÔUåŸd)ÎfZq Z g$*5˜ÎiSa"ÁM­ìuQ‰­pŠ»S¶ª	0T,L‚À±Ğ(²Î@µ")Bhæe¯ı9X.şãÇ‰d±ˆœèáõ¬ÁºêY{_ ¤¼YülÑ“­××—¤wøT Yp%2•Lípg°l©®@1Ë—‚æ£øBÕåÜ¢ü¢Â®Zqßë»Z‘Ğv7Ô•‚x¬™z|H/Áö†<‘$GâFÖ<ø¸Bj{ÓéwÍKAŠÀB€D®d¹ÜÇ<%$‚:KºÃÃ‰
-!B~£÷Şúhğ}Û®ÇCõ’v[8ÇA8HA«!—˜œD(È9Nß3DÄ÷a
-NO]H$‰˜ìBœVPDÕDÏ;¤Áæ(l!şTOğ5”Lï­áÎ;zQ.ëÜ‡v£c;„:~ÍOw®E‚Lï¿(~ŠÂ“—us2}½1®Çq²Ní{Q_ÑÀ~ûRí¯%4™û"äqUèÚ‘‹ç¯^:VŠ
-0lb#q’“Š¥ÃCÌ`?ÌåÛËNš4w¡§¢Or»5°^ÃvPÛ˜m"şPcüÖcquµÆ8³‚4ûÏÕ6u”v²ºº÷Fp’G?½^òíéã'¿¸·Shã×ïÊß¹‰æİƒ–¬Xè'üÌ÷²¨õJî¬Ê%ØƒnŸÌ'Ş=Lkm?†ğdòØÂ ïcão¯¬DÕ¨ôLiÉ£ÀÍ‰¹Êœô„¼ù(-ßŸ}4÷›üÃ[‹„qÜ~çÆY`šĞÃ“ç®\º~¨µ€Ãx²&Bì(ŸLìÈ ˜É^sL’µ©lêø+5+wˆ®uOÏéö°š8R¤TB9BòÈÊ7Hm¸{øü]y··”$ÂqQôãˆ„Üå[â‘=qÿ9<À½‹2&xtV@L$¥6ĞÊTÿ	TèÔN
-ªƒÆ¸ŞT	+¼­Ï„Jµ°Y+ÁaØJÜñ[ğ2€båîâs%	*Mkî¬/ÌE)ö±–OKH>P+_wè³C[/
-^Ü2 v6£ÜZİÙÿığ“ŠIÇ‚«£û²ï¿­}X¬š³KŞë+PjrVV¯Çf”(‘@r×R²tŒbGZüeüA¥=H¹´[Nm½,×ûÓ\ş[jÀÆ¯ÎÄ ö^9˜Q?Íáß\MëŸäÜ‡*lÓÀi{U7h$†X,Š¤Ê²VsÄöcWb?5äti‚|á-•uü+˜ƒHE¹ªå?s+Á™˜µ‘IÈdGÅŠ#RDì’ÃˆÄ¾i.i~$/t‘Ş¥,,ÊnÀ.µ6ßPùQµÁ>“€êÆ‚ÓâÄÉÚ©Hê¸˜bU5mHÇÑÑ]gö
-õüÆmŸî0é©.uÓW%
- ¼¬í—EYİÙmSÓ:F«m‚± …F2(ÍÆ;¶šTFIÓÙÄÖë1KıDˆytyã…Uç×ø#­—·³ìYıK–BêW]NAQö()wiz*wÑ¦YÈÍ;šz)£h}ÉÎRZ¸3$œL.x‘<Ò¢GêÒâ²‹EÕèêˆ®q>Mf”ö:±âº-<®.û'Õó>Ş)7f^ ¶©n=-SPûÀ¹¶¸`¹Á;Dy‡&^z Çü£3ï]ı«\v¯•.«¦ñ½„4ğ¾¡t|g"B¾ÆXç[z†è¹^g2õ·¯$0—6×å+A)6kÔ,\°…E¸@êÊg£qÀ~ô/„¤½ğ™6Ö¶J`4İ8¾GI£yÖ5`F›+Z8Hq½†¿+7oÿmE‚ê“C§p×ß ¶Ó"sSRå+37¥o‹Z¹}¿p¶		O/gÄ*rx”–».4ŸğëÖnKß8#ky
-dóG×ÌüWMyaÅ]¹ì`Ì§²+†E_ï:³>"# XDùk³²—¥/ùdÂ’‹Ê+‹ÏtT´8²çÌA¡×3Mã¡¶ğ~œ9²ØØC–ãéHÑ9¨wrùïÔ“iúÙÉÿ>øhø§•÷jïŸO‘“ß—÷Š“BMK<ŒskTPbî¬òôÛ‰'‚MF„âVÆ‡ù?-pK+© ˜õ(Ÿ<ä/E–«¨å´LÿqİD0?sV’Îò0‚ˆú¾ñ×Š´ø
-m >¼[+ûsûï|W¢½ù¶bğ›Š›H+€µ[ùÀ;T5{©<3iãâíS·û‡½g¾>~ü‡ÒÓ•HxöcÔäèåÃSîóˆóø„|âaÓMoˆU³Ï%ğ)Mt$±'aˆŒAÄó	{K%ZÉyÁğà‹ÀWøéÀQèÁ¥™…g:	<‘øëÀ¿»Ù'Pw~bŒ’ºñp«Y$YdÁéûï@Às‰¤‰oÁî„!ƒÀ3ÂF‚0‡Ó‘áeÄÑ	‡x- ~^„~‡N¼”¹]¬¶[¢í‡ÿ˜Ç£éVÇm’ùúó×^(À’ÿB—¢éÃ»z¹Şu],ô£7—÷İÄfïŸ­V÷f’GK6õl’Vr°É°É4	“À,~BlDãëXüš³Ê.ÄjˆP‡rMæmM{--Ûö[`˜ÿ ô6
-endstream
-endobj
-13 0 obj
-<</Type/Metadata
-/Subtype/XML/Length 1388>>stream
-<?xpacket begin='ï»¿' id='W5M0MpCehiHzreSzNTczkc9d'?>
-<?adobe-xap-filters esc="CRLF"?>
-<x:xmpmeta xmlns:x='adobe:ns:meta/' x:xmptk='XMP toolkit 2.9.1-13, framework 1.6'>
-<rdf:RDF xmlns:rdf='http://www.w3.org/1999/02/22-rdf-syntax-ns#' xmlns:iX='http://ns.adobe.com/iX/1.0/'>
-<rdf:Description rdf:about='b1aeaa8f-72ef-11e7-0000-51c8be12fc20' xmlns:pdf='http://ns.adobe.com/pdf/1.3/' pdf:Producer='GNU Ghostscript 8.56'/>
-<rdf:Description rdf:about='b1aeaa8f-72ef-11e7-0000-51c8be12fc20' xmlns:xap='http://ns.adobe.com/xap/1.0/' xap:ModifyDate='2007-07-25T17:19:05Z' xap:CreateDate='2007-07-25T17:19:05Z'><xap:CreatorTool>fig2dev Version 3.2 Patchlevel 5-alpha7</xap:CreatorTool></rdf:Description>
-<rdf:Description rdf:about='b1aeaa8f-72ef-11e7-0000-51c8be12fc20' xmlns:xapMM='http://ns.adobe.com/xap/1.0/mm/' xapMM:DocumentID='b1aeaa8f-72ef-11e7-0000-51c8be12fc20'/>
-<rdf:Description rdf:about='b1aeaa8f-72ef-11e7-0000-51c8be12fc20' xmlns:dc='http://purl.org/dc/elements/1.1/' dc:format='application/pdf'><dc:title><rdf:Alt><rdf:li xml:lang='x-default'>fieldseq_tb.fig</rdf:li></rdf:Alt></dc:title><dc:creator><rdf:Seq><rdf:li>michael@localhost \(\)</rdf:li></rdf:Seq></dc:creator></rdf:Description>
-</rdf:RDF>
-</x:xmpmeta>
-                                                                        
-                                                                        
-<?xpacket end='w'?>
-endstream
-endobj
-2 0 obj
-<</Producer(GNU Ghostscript 8.56)
-/CreationDate(D:20070725171905Z)
-/ModDate(D:20070725171905Z)
-/Title(fieldseq_tb.fig)
-/Creator(fig2dev Version 3.2 Patchlevel 5-alpha7)
-/Author(michael@localhost \(\))>>endobj
-xref
-0 14
-0000000000 65535 f 
-0000003075 00000 n 
-0000008523 00000 n 
-0000003016 00000 n 
-0000002865 00000 n 
-0000000015 00000 n 
-0000002845 00000 n 
-0000003140 00000 n 
-0000003241 00000 n 
-0000003658 00000 n 
-0000003181 00000 n 
-0000003211 00000 n 
-0000004014 00000 n 
-0000007058 00000 n 
-trailer
-<< /Size 14 /Root 1 0 R /Info 2 0 R
-/ID [<9348BD7CCE2B07175ADFEF4DF463CEA4><9348BD7CCE2B07175ADFEF4DF463CEA4>]
->>
-startxref
-8740
-%%EOF
Binary files linux-3.2.51/Documentation/DocBook/media/v4l/pipeline.pdf and ../split-release/release/linux-3.2.51/Documentation/DocBook/media/v4l/pipeline.pdf differ
Binary files linux-3.2.51/Documentation/DocBook/media/v4l/vbi_525.pdf and ../split-release/release/linux-3.2.51/Documentation/DocBook/media/v4l/vbi_525.pdf differ
Binary files linux-3.2.51/Documentation/DocBook/media/v4l/vbi_625.pdf and ../split-release/release/linux-3.2.51/Documentation/DocBook/media/v4l/vbi_625.pdf differ
Binary files linux-3.2.51/Documentation/DocBook/media/v4l/vbi_hsync.pdf and ../split-release/release/linux-3.2.51/Documentation/DocBook/media/v4l/vbi_hsync.pdf differ
diff -rubBN linux-3.2.51/fs/bio.c ../split-release/release/linux-3.2.51/fs/bio.c
--- linux-3.2.51/fs/bio.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/fs/bio.c	2015-09-01 05:42:48.000000000 +0200
@@ -25,6 +25,7 @@
 #include <linux/module.h>
 #include <linux/mempool.h>
 #include <linux/workqueue.h>
+#include <linux/cause_tags.h>
 #include <scsi/sg.h>		/* for struct sg_iovec */
 
 #include <trace/events/block.h>
@@ -234,6 +235,8 @@
 {
 	void *p;
 
+	del_cause_list_list(bio->cll);
+
 	if (bio_has_allocated_vec(bio))
 		bvec_free_bs(bs, bio->bi_io_vec, BIO_POOL_IDX(bio));
 
diff -rubBN linux-3.2.51/fs/btrfs/extent_io.c ../split-release/release/linux-3.2.51/fs/btrfs/extent_io.c
--- linux-3.2.51/fs/btrfs/extent_io.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/fs/btrfs/extent_io.c	2015-09-01 05:42:48.000000000 +0200
@@ -19,6 +19,8 @@
 #include "btrfs_inode.h"
 #include "volumes.h"
 
+#include <linux/cause_tags.h>
+
 static struct kmem_cache *extent_state_cache;
 static struct kmem_cache *extent_buffer_cache;
 
@@ -52,6 +54,46 @@
 	unsigned int sync_io:1;
 };
 
+/*
+For debuging: prints the page information 
+*/
+void print_page_info(struct page *page) {
+    struct inode *inode = page->mapping->host;
+    //u64 start = (u64) page->index << PAGE_CACHE_SHIFT;
+    //u64 page_end = start + PAGE_CACHE_SIZE - 1;
+    //u64 end;
+    //struct btrfs_ordered_extent *ordered;
+    struct extent_io_tree *io_tree = &BTRFS_I(inode)->io_tree;
+    struct extent_buffer *eb;
+    unsigned long len;
+    u64 bytenr = page_offset(page);
+
+    printk("### page %ld : ", page->index);
+    
+    if(page->locked_causes == NULL){
+	printk("..No causes ");
+    }
+    else{
+	printk("..With causes ");
+    }
+
+    if (page->private == EXTENT_PAGE_PRIVATE){
+        printk("..No extent on private .\n");
+        return;
+    }
+    else{
+	printk("..has extent on private ..");
+    }
+
+    len = page->private >> 2;
+    eb = find_extent_buffer(io_tree, bytenr, len);
+    if (!eb) {
+        printk("..No buffer.\n");
+    } else {
+        printk("..With Buffer.\n");
+    }
+}
+
 int __init extent_io_init(void)
 {
 	extent_state_cache = kmem_cache_create("extent_state",
@@ -2407,6 +2449,47 @@
 	return bio;
 }
 
+static void request_sanity_check(struct bio *bio) {
+	struct cause_list *causes;
+	//struct bio *bio = rq->bio;
+	WARN_ON(bio == NULL);
+	while (bio != NULL) {
+		// (cll set) iff (req is a write)
+		//WARN_ON((bio->cll != NULL) && (rq_data_dir(rq) == READ));
+		//WARN_ON((bio->cll == NULL) && (rq_data_dir(rq) == WRITE));
+		if (bio->cll ) {
+			int cl_count = 0;
+			int cl_bytes = 0;
+			int i;
+			for (i=0; i < bio->cll->item_count; i++) {
+				causes = bio->cll->items[i];
+				if (causes->type != SPLIT_ZERO) {
+					WARN_ON(causes->size != (4 * 1024));
+					// sometimes transactions are commited without any I/O
+					// for processes, so journal cause_list's may be empty
+					if (causes->type != SPLIT_JOURNAL)
+						WARN_ON(causes->item_count == 0);
+				}
+				cl_count++;
+				cl_bytes += causes->size;
+			}
+
+			if(bio->cll->size != bio->bi_size){
+				printk("size not match cll_>size = %d, bio->size = %d", bio->cll->size, bio->bi_size);
+			}
+/*			if(bio->cll->size != bio->bi_size) {
+				printk(KERN_INFO "mismatch %ld != %d (%d)",
+					   bio->cll->size, bio->bi_size,
+					   bio->cll->item_count);
+			}
+*/
+			WARN_ON(cl_count != bio->cll->item_count);
+			WARN_ON(cl_bytes != bio->cll->size);
+		}
+		bio = bio->bi_next;
+	}
+}
+
 static int submit_one_bio(int rw, struct bio *bio, int mirror_num,
 			  unsigned long bio_flags)
 {
@@ -2422,6 +2505,17 @@
 
 	bio_get(bio);
 
+	// SAMER
+    /*
+	printk("submit_one_bio : vcnt %d and size %d : ", bio->bi_vcnt, bio->bi_size);
+	print_page_info(page);
+	if(bio->cll)
+		printk("BIO with causes. page %ld\n", page->index);
+	else
+		printk("BIO with NO causes. page %ld\n", page->index);
+
+	WARN_ON(1);
+    */
 	if (tree->ops && tree->ops->submit_bio_hook)
 		ret = tree->ops->submit_bio_hook(page->mapping->host, rw, bio,
 					   mirror_num, bio_flags, start);
@@ -2434,6 +2528,42 @@
 	return ret;
 }
 
+void copy_causes_page_to_bio(struct page *page, struct bio *bio, size_t page_size) {
+#ifndef DISABLE_CAUSES  /*just to enable search in code for changes*/
+#endif
+	//yangsuli ? why >> 1
+	int max_pages_per_bio = queue_max_sectors(bdev_get_queue(bio->bi_bdev)) >> 1;
+	struct cause_list_locked* locked_causes = (struct cause_list_locked*) page->locked_causes;
+	if(!locked_causes){
+		return;
+	}
+	if(!(locked_causes->causes)){
+		return;
+	}
+	
+	if (!bio->cll)
+		bio->cll = new_cause_list_list(max_pages_per_bio);
+
+	BUG_ON(page->locked_causes == NULL);
+	spin_lock(&locked_causes->lock);
+	if(!(locked_causes->causes)){
+		spin_unlock(&locked_causes->lock);
+		return;
+	}
+
+	WARN_ON(locked_causes->causes->item_count == 0 &&
+				locked_causes->causes->type != SPLIT_JOURNAL);
+
+	locked_causes->causes->size = page_size;
+	cause_list_list_add(bio->cll, locked_causes->causes);
+
+	// free cause list
+	put_cause_list(locked_causes->causes);
+//yangsuli: why???
+	locked_causes->causes = NULL;
+	spin_unlock(&locked_causes->lock);
+}
+
 static int submit_extent_page(int rw, struct extent_io_tree *tree,
 			      struct page *page, sector_t sector,
 			      size_t size, unsigned long offset,
@@ -2461,6 +2591,9 @@
 			contig = bio->bi_sector + (bio->bi_size >> 9) ==
 				sector;
 
+#ifndef DISABLE_CAUSES
+		copy_causes_page_to_bio(page, bio, page_size);
+#endif
 		if (prev_bio_flags != bio_flags || !contig ||
 		    (tree->ops && tree->ops->merge_bio_hook &&
 		     tree->ops->merge_bio_hook(page, offset, page_size, bio,
@@ -2483,6 +2617,9 @@
 		return -ENOMEM;
 
 	bio_add_page(bio, page, page_size, offset);
+#ifndef DISABLE_CAUSES
+	copy_causes_page_to_bio(page, bio, page_size);
+#endif
 	bio->bi_end_io = end_io_func;
 	bio->bi_private = tree;
 
diff -rubBN linux-3.2.51/fs/btrfs/extent_io.h ../split-release/release/linux-3.2.51/fs/btrfs/extent_io.h
--- linux-3.2.51/fs/btrfs/extent_io.h	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/fs/btrfs/extent_io.h	2015-09-01 05:42:48.000000000 +0200
@@ -188,6 +188,7 @@
 			 struct extent_state **cached, gfp_t mask);
 int try_lock_extent(struct extent_io_tree *tree, u64 start, u64 end,
 		    gfp_t mask);
+void print_page_info(struct page *page);
 int extent_read_full_page(struct extent_io_tree *tree, struct page *page,
 			  get_extent_t *get_extent, int mirror_num);
 int __init extent_io_init(void);
diff -rubBN linux-3.2.51/fs/btrfs/file.c ../split-release/release/linux-3.2.51/fs/btrfs/file.c
--- linux-3.2.51/fs/btrfs/file.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/fs/btrfs/file.c	2015-09-01 05:42:48.000000000 +0200
@@ -40,6 +40,9 @@
 #include "locking.h"
 #include "compat.h"
 
+#include <linux/blkdev.h>
+#include <linux/cause_tags.h>
+
 /*
  * when auto defrag is enabled we
  * queue up these defrag structs to remember which
@@ -377,6 +380,57 @@
 	}
 }
 
+void add_to_causes(struct inode *inode, struct page *page, loff_t pos) {
+#ifndef DISABLE_CAUSES	
+
+    struct request_queue *q = NULL;
+    elevator_causes_dirty_fn *causes_dirty_fn = NULL;
+    int cause_added;
+    struct cause_list_locked * locked_causes = (struct cause_list_locked *) page->locked_causes;
+
+    if(!locked_causes){
+	//printk("### no causes.\n");
+	WARN_ON(1);
+	return;
+    }
+    else{
+	//printk("### causes.\n");
+    }
+
+    // add this cause (data write)
+    spin_lock(&locked_causes->lock);
+    if(!(locked_causes->causes)){
+	locked_causes->causes = new_cause_list(); 
+    }
+    cause_added = cause_list_add(&locked_causes->causes, current);
+    set_cause_list_type(locked_causes->causes, SPLIT_DATA);
+
+    // dirty hook
+    q = inode_to_request_queue(inode);
+    if (q && locked_causes->causes && cause_added) {
+        spin_lock_irq(q->queue_lock);
+
+	//yangsuli ????
+        if (locked_causes->causes->callback_q == NULL) {
+            // TODO(tyler): need refcount get on q?
+            locked_causes->causes->callback_q = q;
+            locked_causes->causes->sched_uniq = q->sched_uniq;
+            locked_causes->causes->size = PAGE_CACHE_SIZE;
+        }
+        // one buffer can't correspond to many disks, right? 
+        WARN_ON(locked_causes->causes->callback_q != q);
+
+        causes_dirty_fn = q->elevator->elevator_type->ops.elevator_causes_dirty_fn;
+        if (causes_dirty_fn && q->sched_uniq == locked_causes->causes->sched_uniq)
+            causes_dirty_fn(q, locked_causes->causes, current, inode, pos);
+
+        spin_unlock_irq(q->queue_lock);
+    }
+    spin_unlock(&locked_causes->lock);
+
+#endif
+}
+
 /*
  * after copy_from_user, pages need to be dirtied and we need to make
  * sure holes are created between the current EOF and the start of
@@ -413,6 +467,10 @@
 		SetPageUptodate(p);
 		ClearPageChecked(p);
 		set_page_dirty(p);
+
+#ifndef DISABLE_CAUSES
+		add_to_causes(inode, p, pos);
+#endif
 	}
 
 	/*
@@ -1074,6 +1132,7 @@
 	int faili = 0;
 	u64 start_pos;
 	u64 last_pos;
+	struct cause_list_locked * locked_causes;
 
 	start_pos = pos & ~((u64)root->sectorsize - 1);
 	last_pos = ((u64)index + num_pages) << PAGE_CACHE_SHIFT;
@@ -1088,6 +1147,27 @@
 			goto fail;
 		}
 
+#ifndef DISABLE_CAUSES
+		// allocate the cause list if this a new page
+		if (pages[i]->locked_causes == NULL){
+			//printk("### No Causes.\n");	
+			pages[i]->locked_causes = kmalloc(sizeof(struct cause_list_locked), GFP_KERNEL);
+			BUG_ON(!(pages[i]->locked_causes));
+			locked_causes = (struct cause_list_locked *) pages[i]->locked_causes;
+			spin_lock_init(&locked_causes->lock);
+			locked_causes->causes = new_cause_list();                    
+        	}
+		else{
+			locked_causes = (struct cause_list_locked *) pages[i]->locked_causes;
+			if (locked_causes->causes == NULL){
+				locked_causes->causes = new_cause_list(); 
+			}
+			else{
+		    		//printk("### has Causes.\n");	
+			}
+		}
+#endif
+
 		if (i == 0)
 			err = prepare_uptodate_page(pages[i], pos,
 						    force_uptodate);
diff -rubBN linux-3.2.51/fs/btrfs/super.c ../split-release/release/linux-3.2.51/fs/btrfs/super.c
--- linux-3.2.51/fs/btrfs/super.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/fs/btrfs/super.c	2015-09-01 05:42:48.000000000 +0200
@@ -62,6 +62,24 @@
 static const struct super_operations btrfs_super_ops;
 static struct file_system_type btrfs_fs_type;
 
+struct block_device *btrfs_inode_to_bdev(struct inode* inode){
+	struct btrfs_inode *btrfs_inode = BTRFS_I(inode);
+	struct btrfs_device *btrfs_device;
+
+
+	if(!btrfs_inode->root)
+		return NULL;
+	if(!btrfs_inode->root->fs_info)
+		return NULL;
+	if(!btrfs_inode->root->fs_info->fs_devices)
+		return NULL;
+	if(btrfs_inode->root->fs_info->fs_devices->num_devices != 1)
+		return NULL;
+
+	btrfs_device = list_first_entry(&btrfs_inode->root->fs_info->fs_devices->devices, struct btrfs_device, dev_list);
+	return btrfs_device->bdev;
+}
+
 static const char *btrfs_decode_error(struct btrfs_fs_info *fs_info, int errno,
 				      char nbuf[16])
 {
diff -rubBN linux-3.2.51/fs/buffer.c ../split-release/release/linux-3.2.51/fs/buffer.c
--- linux-3.2.51/fs/buffer.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/fs/buffer.c	2015-09-01 05:42:49.000000000 +0200
@@ -42,6 +42,7 @@
 #include <linux/mpage.h>
 #include <linux/bit_spinlock.h>
 #include <linux/cleancache.h>
+#include <linux/cause_tags.h>
 
 static int fsync_buffers_list(spinlock_t *lock, struct list_head *list);
 
@@ -1911,12 +1912,15 @@
 EXPORT_SYMBOL(__block_write_begin);
 
 static int __block_commit_write(struct inode *inode, struct page *page,
-		unsigned from, unsigned to)
+		unsigned from, unsigned to, long pos)
 {
 	unsigned block_start, block_end;
 	int partial = 0;
 	unsigned blocksize;
 	struct buffer_head *bh, *head;
+	struct request_queue *q = NULL;
+	elevator_causes_dirty_fn *causes_dirty_fn = NULL;
+	int cause_added;
 
 	blocksize = 1 << inode->i_blkbits;
 
@@ -1930,6 +1934,37 @@
 		} else {
 			set_buffer_uptodate(bh);
 			mark_buffer_dirty(bh);
+
+#ifndef DISABLE_CAUSES	
+
+			spin_lock(&bh->causes_lock);
+			// add this cause (data write)
+			cause_added = cause_list_add(&bh->causes, current);
+			set_cause_list_type(bh->causes, SPLIT_DATA);
+
+			// dirty hook
+			q = inode_to_request_queue(inode);
+			if (q && bh->causes && cause_added) {
+				spin_lock_irq(q->queue_lock);
+				
+				if (bh->causes->callback_q == NULL) {
+					// TODO(tyler): need refcount get on q?
+					bh->causes->callback_q = q;
+					bh->causes->sched_uniq = q->sched_uniq;
+					bh->causes->size = bh->b_size;
+				}
+				// one buffer can't correspond to many disks, right? 
+				WARN_ON(bh->causes->callback_q != q);
+
+				causes_dirty_fn = q->elevator->elevator_type->ops.elevator_causes_dirty_fn;
+				if (causes_dirty_fn && q->sched_uniq == bh->causes->sched_uniq)
+					causes_dirty_fn(q, bh->causes, current, inode, pos);
+
+				spin_unlock_irq(q->queue_lock);
+			}
+			spin_unlock(&bh->causes_lock);
+
+#endif
 		}
 		clear_buffer_new(bh);
 	}
@@ -2004,7 +2039,7 @@
 	flush_dcache_page(page);
 
 	/* This could be a short (even 0-length) commit */
-	__block_commit_write(inode, page, start, start+copied);
+	__block_commit_write(inode, page, start, start+copied, pos);
 
 	return copied;
 }
@@ -2317,7 +2352,7 @@
 int block_commit_write(struct page *page, unsigned from, unsigned to)
 {
 	struct inode *inode = page->mapping->host;
-	__block_commit_write(inode,page,from,to);
+	__block_commit_write(inode,page,from,to,-1);
 	return 0;
 }
 EXPORT_SYMBOL(block_commit_write);
@@ -2944,6 +2979,9 @@
 	bio->bi_end_io = end_bio_bh_io_sync;
 	bio->bi_private = bh;
 
+	if (rw & WRITE)
+		move_causes_bh_to_bio(bh, bio);
+
 	bio_get(bio);
 	submit_bio(rw, bio);
 
@@ -3221,6 +3259,12 @@
 {
 	struct buffer_head *ret = kmem_cache_zalloc(bh_cachep, gfp_flags);
 	if (ret) {
+
+#ifndef DISABLE_CAUSES
+		spin_lock_init(&ret->causes_lock);
+		BUG_ON(ret->causes);
+#endif
+
 		INIT_LIST_HEAD(&ret->b_assoc_buffers);
 		preempt_disable();
 		__this_cpu_inc(bh_accounting.nr);
@@ -3233,6 +3277,9 @@
 
 void free_buffer_head(struct buffer_head *bh)
 {
+	put_cause_list(bh->causes);
+	bh->causes = NULL;
+
 	BUG_ON(!list_empty(&bh->b_assoc_buffers));
 	kmem_cache_free(bh_cachep, bh);
 	preempt_disable();
diff -rubBN linux-3.2.51/fs/disk_sim.h ../split-release/release/linux-3.2.51/fs/disk_sim.h
--- linux-3.2.51/fs/disk_sim.h	1970-01-01 01:00:00.000000000 +0100
+++ ../split-release/release/linux-3.2.51/fs/disk_sim.h	2015-09-01 05:42:48.000000000 +0200
@@ -0,0 +1,46 @@
+/* 
+ * File:   disk_sim.h
+ * Author: samer
+ *
+ * Created on March 13, 2014, 5:09 AM
+ */
+
+#ifndef DISK_SIM_H
+#define	DISK_SIM_H
+
+#include <linux/blk_types.h>
+#include <linux/kernel.h>
+
+#define SEEK_CAP ((size_t)2*1024*1024) // 2MB
+
+typedef struct {
+    off_t offset;
+    int size;  /* The size in bytes */
+} IORequest;
+
+/**
+ * The Disk struct
+ */
+struct disk_sim {
+	off_t head_location;
+};
+
+/**
+ * Initialize the disk simulator
+ */
+void disk_init(struct disk_sim* disk);
+
+size_t seek_time(off_t o1, off_t o2);
+
+/**
+ * Simulate the processing of a single request
+ * 
+ * @param request The block I/O request
+ * @return the estimated disk time for the request 
+ */
+size_t disk_simulate_request(struct disk_sim* disk, IORequest* request);
+
+void disk_set_head(struct disk_sim* disk, off_t location);
+
+#endif	/* DISK_SIM_H */
+
diff -rubBN linux-3.2.51/fs/ext4/ext4_jbd2.c ../split-release/release/linux-3.2.51/fs/ext4/ext4_jbd2.c
--- linux-3.2.51/fs/ext4/ext4_jbd2.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/fs/ext4/ext4_jbd2.c	2015-09-01 05:42:48.000000000 +0200
@@ -5,6 +5,7 @@
 #include "ext4_jbd2.h"
 
 #include <trace/events/ext4.h>
+#include <linux/cause_tags.h>
 
 int __ext4_journal_get_write_access(const char *where, unsigned int line,
 				    handle_t *handle, struct buffer_head *bh)
@@ -107,6 +108,14 @@
 {
 	int err = 0;
 
+#ifndef DISABLE_CAUSES
+	// add this cause (meta write)
+	spin_lock(&bh->causes_lock);
+	cause_list_add(&bh->causes, current);
+	set_cause_list_type(bh->causes, SPLIT_CHECKPOINT);
+	spin_unlock(&bh->causes_lock);
+#endif
+
 	if (ext4_handle_valid(handle)) {
 		err = jbd2_journal_dirty_metadata(handle, bh);
 		/* Errors can only happen if there is a bug */
diff -rubBN linux-3.2.51/fs/ext4/fsync.c ../split-release/release/linux-3.2.51/fs/ext4/fsync.c
--- linux-3.2.51/fs/ext4/fsync.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/fs/ext4/fsync.c	2015-09-01 05:42:48.000000000 +0200
@@ -205,6 +205,26 @@
  * i_mutex lock is held when entering and exiting this function
  */
 
+#ifdef SPLIT_DEBUG
+static inline int get_time_diff(struct timeval *end, struct timeval *start){
+	return (end->tv_sec - start->tv_sec) * 1000 + (end->tv_usec - start->tv_usec) / 1000;
+}
+
+static int is_on_sdb(struct super_block *sb){
+	if(!sb)
+		return 0;
+	if(!sb->s_bdev)
+		return 0;
+	if(!sb->s_bdev->bd_disk)
+		return 0;
+	if(!sb->s_bdev->bd_disk->disk_name)
+		return 0;
+	if(strcmp(sb->s_bdev->bd_disk->disk_name, "sdf") == 0)
+		return 1;
+	return 0;
+}
+#endif
+
 int ext4_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 {
 	struct inode *inode = file->f_mapping->host;
@@ -213,12 +233,28 @@
 	int ret;
 	tid_t commit_tid;
 	bool needs_barrier = false;
+#ifdef SPLIT_DEBUG
+	struct timeval wait_start, wait_end;
+	struct timeval wait_start2, wait_end2;
+	struct super_block *sb = inode->i_sb;
 
+	do_gettimeofday(&wait_start2);
+#endif
 	J_ASSERT(ext4_journal_current_handle() == NULL);
 
 	trace_ext4_sync_file_enter(file, datasync);
 
+#ifdef SPLIT_DEBUG
+	do_gettimeofday(&wait_start);
+#endif
 	ret = filemap_write_and_wait_range(inode->i_mapping, start, end);
+#ifdef SPLIT_DEBUG
+	do_gettimeofday(&wait_end);
+	if(is_on_sdb(sb)){
+		printk("yangsuli: fsync on file %s take %d ms flushing its own file data\n", file->f_dentry->d_name.name, get_time_diff(&wait_end, &wait_start)); 
+	}
+#endif
+	
 	if (ret)
 		return ret;
 	mutex_lock(&inode->i_mutex);
@@ -260,11 +296,39 @@
 	if (journal->j_flags & JBD2_BARRIER &&
 	    !jbd2_trans_will_send_data_barrier(journal, commit_tid))
 		needs_barrier = true;
+#ifdef SPLIT_DEBUG
+	do_gettimeofday(&wait_start);
+#endif
 	ret = jbd2_complete_transaction(journal, commit_tid);
+#ifdef SPLIT_DEBUG
+	do_gettimeofday(&wait_end);
+	if(is_on_sdb(sb) && get_time_diff(&wait_end, &wait_start) >= 10){
+		printk("yangsuli: fsync on file %s take %d ms waiting on commit %d\n", file->f_dentry->d_name.name, get_time_diff(&wait_end, &wait_start), commit_tid); 
+	}
+
+	do_gettimeofday(&wait_start);
+#endif
 	if (needs_barrier)
 		blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+	
+#ifdef SPLIT_DEBUG
+	do_gettimeofday(&wait_end);
+	if(is_on_sdb(sb) && get_time_diff(&wait_end, &wait_start) >= 10){
+		printk("yangsuli: fsync on file %s take %d ms issue blk_flush\n", file->f_dentry->d_name.name, get_time_diff(&wait_end, &wait_start)); 
+	}
+#endif
+
  out:
 	mutex_unlock(&inode->i_mutex);
 	trace_ext4_sync_file_exit(inode, ret);
+
+	
+#ifdef SPLIT_DEBUG
+	do_gettimeofday(&wait_end2);
+	if(is_on_sdb(sb) && get_time_diff(&wait_end, &wait_start) >= 10){
+		printk("yangsuli: ext4_sync_file on file %s take %d ms\n", file->f_dentry->d_name.name, get_time_diff(&wait_end2, &wait_start2)); 
+	}
+#endif
+
 	return ret;
 }
diff -rubBN linux-3.2.51/fs/ext4/inode.c ../split-release/release/linux-3.2.51/fs/ext4/inode.c
--- linux-3.2.51/fs/ext4/inode.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/fs/ext4/inode.c	2015-09-01 05:42:48.000000000 +0200
@@ -38,6 +38,7 @@
 #include <linux/printk.h>
 #include <linux/slab.h>
 #include <linux/ratelimit.h>
+#include <linux/cause_tags.h>
 
 #include "ext4_jbd2.h"
 #include "xattr.h"
@@ -1473,7 +1474,7 @@
  * The function skips space we know is already mapped to disk blocks.
  *
  */
-static void mpage_da_map_and_submit(struct mpage_da_data *mpd)
+static void __mpage_da_map_and_submit(struct mpage_da_data *mpd)
 {
 	int err, blks, get_blocks_flags;
 	struct ext4_map_blocks map, *mapp = NULL;
@@ -1605,6 +1606,14 @@
 	mpd->io_done = 1;
 }
 
+static void mpage_da_map_and_submit(struct mpage_da_data *mpd)
+{
+	__mpage_da_map_and_submit(mpd);
+	// we are no longer acting as a proxy for dirty buffers
+	put_cause_list(current->causes);
+	current->causes = new_cause_list();
+}
+
 #define BH_FLAGS ((1 << BH_Uptodate) | (1 << BH_Mapped) | \
 		(1 << BH_Delay) | (1 << BH_Unwritten))
 
@@ -2124,6 +2134,10 @@
 					 * with the page in ext4_writepage
 					 */
 					if (ext4_bh_delay_or_unwritten(NULL, bh)) {
+						// are we a proxy?
+						if (current->causes)
+							cause_list_copy(bh->causes, &current->causes);
+
 						mpage_add_bh_to_extent(mpd, logical,
 								       bh->b_size,
 								       bh->b_state);
@@ -2220,6 +2234,10 @@
 	if (unlikely(sbi->s_mount_flags & EXT4_MF_FS_ABORTED))
 		return -EROFS;
 
+	// now acting as cause proxy
+	BUG_ON(current->causes);
+	current->causes = new_cause_list();
+
 	if (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)
 		range_whole = 1;
 
@@ -2360,6 +2380,10 @@
 		mapping->writeback_index = done_index;
 
 out_writepages:
+	// done acting as cause proxy
+	put_cause_list(current->causes);
+	current->causes = NULL;
+
 	wbc->nr_to_write -= nr_to_writebump;
 	wbc->range_start = range_start;
 	trace_ext4_da_writepages_result(inode, wbc, ret, pages_written);
diff -rubBN linux-3.2.51/fs/ext4/page-io.c ../split-release/release/linux-3.2.51/fs/ext4/page-io.c
--- linux-3.2.51/fs/ext4/page-io.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/fs/ext4/page-io.c	2015-09-01 05:42:48.000000000 +0200
@@ -24,6 +24,7 @@
 #include <linux/workqueue.h>
 #include <linux/kernel.h>
 #include <linux/slab.h>
+#include <linux/cause_tags.h>
 
 #include "ext4_jbd2.h"
 #include "xattr.h"
@@ -347,8 +349,15 @@
 	io->io_end->size += bh->b_size;
 	io->io_next_block++;
 	ret = bio_add_page(io->io_bio, bh->b_page, bh->b_size, bh_offset(bh));
+
+
+
 	if (ret != bh->b_size)
 		goto submit_and_retry;
+
+	// I think this path is only for writes
+	move_causes_bh_to_bio(bh, io->io_bio);
+
 	if ((io_end->num_io_pages == 0) ||
 	    (io_end->pages[io_end->num_io_pages-1] != io_page)) {
 		io_end->pages[io_end->num_io_pages++] = io_page;
diff -rubBN linux-3.2.51/fs/inode.c ../split-release/release/linux-3.2.51/fs/inode.c
--- linux-3.2.51/fs/inode.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/fs/inode.c	2015-09-01 05:42:49.000000000 +0200
@@ -227,6 +227,8 @@
 		return NULL;
 	}
 
+	inode->i_private1 = 0;
+
 	return inode;
 }
 
@@ -1456,7 +1458,10 @@
 		return;
 
 	inode->i_atime = now;
+
+#ifndef SPLIT_NODEP
 	mark_inode_dirty_sync(inode);
+#endif
 	mnt_drop_write(mnt);
 }
 EXPORT_SYMBOL(touch_atime);
@@ -1507,7 +1512,10 @@
 		inode->i_ctime = now;
 	if (sync_it & S_MTIME)
 		inode->i_mtime = now;
+
+#ifndef SPLIT_NODEP
 	mark_inode_dirty_sync(inode);
+#endif
 	mnt_drop_write(file->f_path.mnt);
 }
 EXPORT_SYMBOL(file_update_time);
diff -rubBN linux-3.2.51/fs/io_batch.h ../split-release/release/linux-3.2.51/fs/io_batch.h
--- linux-3.2.51/fs/io_batch.h	1970-01-01 01:00:00.000000000 +0100
+++ ../split-release/release/linux-3.2.51/fs/io_batch.h	2015-09-01 05:42:49.000000000 +0200
@@ -0,0 +1,53 @@
+/* 
+ * File:   io_batch.h
+ * Author: samer
+ *
+ * Created on March 13, 2014, 5:11 AM
+ */
+
+#ifndef IO_BATCH_H
+#define	IO_BATCH_H
+
+#include <linux/bio.h>
+#include <linux/blk_types.h>
+#include <linux/slab.h>
+#include <linux/sort.h>
+
+#include "disk_sim.h"
+
+struct io_batch {
+	struct disk_sim disk;
+    IORequest *reqs;
+    int reqs_size;
+    int reqs_capacity;
+	size_t bytes; // sum of bytes accross reqs
+	size_t cost;  // amount of seq I/O these reqs are equivalent to
+	size_t bytes_cap;
+};
+
+struct io_batch* iobatch_new(int capacity, size_t bytes_cap);
+
+int iobatch_full(struct io_batch* batch);
+
+/**
+ * Add an IO request to the batch
+ * 
+ * @param batch the batch
+ * @param request the io request
+ */
+int iobatch_add_request(struct io_batch* batch,
+						off_t offset, int size);
+
+/**
+ * Reset the batch internal structures, and free old causes.
+ * 
+ * @param batch the batch
+ */
+void iobatch_clear(struct io_batch* batch);
+
+void iobatch_free(struct io_batch* batch);
+
+size_t iobatch_get_cost(struct io_batch* batch);
+
+#endif	/* IO_REQUEST_BATCH_H */
+
diff -rubBN linux-3.2.51/fs/jbd2/checkpoint.c ../split-release/release/linux-3.2.51/fs/jbd2/checkpoint.c
--- linux-3.2.51/fs/jbd2/checkpoint.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/fs/jbd2/checkpoint.c	2015-09-01 05:42:48.000000000 +0200
@@ -348,6 +348,7 @@
 		(*batch_count)++;
 		if (*batch_count == JBD2_NR_BATCH) {
 			spin_unlock(&journal->j_list_lock);
+			printk("yangsuli: journal checkpoint process submit %d blocks\n", *batch_count);
 			__flush_batch(journal, batch_count);
 			ret = 1;
 		}
@@ -722,7 +723,7 @@
 				    transaction->t_tid, stats);
 
 	__jbd2_journal_drop_transaction(journal, transaction);
-	kfree(transaction);
+	jbd2_free_transaction(transaction);
 
 	/* Just in case anybody was waiting for more transactions to be
            checkpointed... */
diff -rubBN linux-3.2.51/fs/jbd2/commit.c ../split-release/release/linux-3.2.51/fs/jbd2/commit.c
--- linux-3.2.51/fs/jbd2/commit.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/fs/jbd2/commit.c	2015-09-01 05:42:48.000000000 +0200
@@ -29,6 +29,9 @@
 #include <linux/bitops.h>
 #include <trace/events/jbd2.h>
 #include <asm/system.h>
+#include <linux/cause_tags.h>
+
+#define SPLIT_DEBUG
 
 /*
  * Default IO end handler for temporary BJ_IO buffer_heads.
@@ -97,7 +100,8 @@
 static int journal_submit_commit_record(journal_t *journal,
 					transaction_t *commit_transaction,
 					struct buffer_head **cbh,
-					__u32 crc32_sum)
+					__u32 crc32_sum,
+					struct cause_list* cause_list)
 {
 	struct journal_head *descriptor;
 	struct commit_header *tmp;
@@ -115,6 +119,7 @@
 		return 1;
 
 	bh = jh2bh(descriptor);
+	bh->causes = get_cause_list(cause_list);
 
 	tmp = (struct commit_header *)bh->b_data;
 	tmp->h_magic = cpu_to_be32(JBD2_MAGIC_NUMBER);
@@ -173,6 +178,9 @@
  * use writepages() because with dealyed allocation we may be doing
  * block allocation in writepages().
  */
+#ifdef SPLIT_DEBUG
+static int is_on_sdb(struct super_block *sb);
+#endif
 static int journal_submit_inode_data_buffers(struct address_space *mapping)
 {
 	int ret;
@@ -232,6 +240,11 @@
  * transaction if needed.
  *
  */
+#ifdef SPLIT_DEBUG
+static inline int get_time_diff(struct timeval *end, struct timeval *start){
+	return (end->tv_sec - start->tv_sec) * 1000 + (end->tv_usec - start->tv_usec) / 1000;
+}
+#endif
 static int journal_finish_inode_data_buffers(journal_t *journal,
 		transaction_t *commit_transaction)
 {
@@ -302,6 +315,22 @@
 		tag->t_blocknr_high = cpu_to_be32((block >> 31) >> 1);
 }
 
+#ifdef SPLIT_DEBUG
+static int is_on_sdb(struct super_block *sb){
+	if(!sb)
+		return 0;
+	if(!sb->s_bdev)
+		return 0;
+	if(!sb->s_bdev->bd_disk)
+		return 0;
+	if(!sb->s_bdev->bd_disk->disk_name)
+		return 0;
+	if(strcmp(sb->s_bdev->bd_disk->disk_name, "sdf") == 0)
+		return 1;
+	return 0;
+}
+#endif
+
 /*
  * jbd2_journal_commit_transaction
  *
@@ -331,6 +360,11 @@
 	struct buffer_head *cbh = NULL; /* For transactional checksums */
 	__u32 crc32_sum = ~0;
 	struct blk_plug plug;
+	struct cause_list* cause_list = NULL;
+#ifdef SPLIT_DEBUG
+	struct super_block *sb = journal->j_private;
+	struct timeval wait_start, wait_end;
+#endif
 
 	/*
 	 * First job: lock down the current transaction and wait for
@@ -364,7 +398,14 @@
 	stats.run.rs_running = jbd2_time_diff(commit_transaction->t_start,
 					      stats.run.rs_locked);
 
+#ifdef SPLIT_DEBUG
+	do_gettimeofday(&wait_start);
+#endif
 	spin_lock(&commit_transaction->t_handle_lock);
+
+	cause_list = get_cause_list(commit_transaction->causes);
+	set_cause_list_type(cause_list, SPLIT_JOURNAL);
+
 	while (atomic_read(&commit_transaction->t_updates)) {
 		DEFINE_WAIT(wait);
 
@@ -380,6 +421,14 @@
 		finish_wait(&journal->j_wait_updates, &wait);
 	}
 	spin_unlock(&commit_transaction->t_handle_lock);
+#ifdef SPLIT_DEBUG
+	do_gettimeofday(&wait_end);
+	
+	if(is_on_sdb(sb) && get_time_diff(&wait_end, &wait_start) >= 10){
+		printk("yangsuli: j_wait_updates take %d ms\n",get_time_diff(&wait_end, &wait_start)); 
+	}
+#endif
+
 
 	J_ASSERT (atomic_read(&commit_transaction->t_outstanding_credits) <=
 			journal->j_max_transaction_buffers);
@@ -459,7 +508,7 @@
 
 	blk_start_plug(&plug);
 	jbd2_journal_write_revoke_records(journal, commit_transaction,
-					  WRITE_SYNC);
+					  WRITE_SYNC, cause_list);
 	blk_finish_plug(&plug);
 
 	jbd_debug(3, "JBD2: commit phase 2\n");
@@ -595,6 +645,9 @@
 		}
 		set_bit(BH_JWrite, &jh2bh(new_jh)->b_state);
 		wbuf[bufs++] = jh2bh(new_jh);
+		(jh2bh(new_jh))->causes = jh->causes;
+		jh->causes = NULL;
+		set_cause_list_type((jh2bh(new_jh))->causes, SPLIT_JOURNAL_META);
 
 		/* Record the new block's tag in the current descriptor
                    buffer */
@@ -636,6 +689,8 @@
 start_journal_io:
 			for (i = 0; i < bufs; i++) {
 				struct buffer_head *bh = wbuf[i];
+				//bh->causes = get_cause_list(cause_list); // new ref
+
 				/*
 				 * Compute checksum.
 				 */
@@ -651,6 +706,13 @@
 				bh->b_end_io = journal_end_buffer_io_sync;
 				submit_bh(WRITE_SYNC, bh);
 			}
+			
+#ifdef SPLIT_DEBUG
+			if(is_on_sdb(sb)){
+				//printk("yangsuli: submit %d journal metadata blocks + 1 descriptor block for transaction %d\n", bufs, commit_transaction->t_tid);
+			}
+#endif
+		
 			cond_resched();
 			stats.run.rs_blocks_logged += bufs;
 
@@ -689,7 +751,7 @@
 	if (JBD2_HAS_INCOMPAT_FEATURE(journal,
 				      JBD2_FEATURE_INCOMPAT_ASYNC_COMMIT)) {
 		err = journal_submit_commit_record(journal, commit_transaction,
-						 &cbh, crc32_sum);
+						 &cbh, crc32_sum, cause_list);
 		if (err)
 			__jbd2_journal_abort_hard(journal);
 	}
@@ -810,17 +872,37 @@
 	if (!JBD2_HAS_INCOMPAT_FEATURE(journal,
 				       JBD2_FEATURE_INCOMPAT_ASYNC_COMMIT)) {
 		err = journal_submit_commit_record(journal, commit_transaction,
-						&cbh, crc32_sum);
+						&cbh, crc32_sum, cause_list);
 		if (err)
 			__jbd2_journal_abort_hard(journal);
 	}
+
+
+#ifdef SPLIT_DEBUG
+	do_gettimeofday(&wait_start);
+#endif
 	if (cbh)
 		err = journal_wait_on_commit_record(journal, cbh);
+#ifdef SPLIT_DEBUG
+	do_gettimeofday(&wait_end);
+	if(is_on_sdb(sb) && get_time_diff(&wait_end, &wait_start) >= 10){
+		printk("yangsuli: journal_wait_on_commit_record take %d ms\n",get_time_diff(&wait_end, &wait_start)); 
+	}
+
+
+	do_gettimeofday(&wait_start);
+#endif
 	if (JBD2_HAS_INCOMPAT_FEATURE(journal,
 				      JBD2_FEATURE_INCOMPAT_ASYNC_COMMIT) &&
 	    journal->j_flags & JBD2_BARRIER) {
 		blkdev_issue_flush(journal->j_dev, GFP_NOFS, NULL);
 	}
+#ifdef SPLIT_DEBUG
+	do_gettimeofday(&wait_end);
+	if(is_on_sdb(sb) && get_time_diff(&wait_end, &wait_start) >= 10){
+		printk("yangsuli: journal_blkdev_issue_flush take %d ms\n",get_time_diff(&wait_end, &wait_start)); 
+	}
+#endif
 
 	if (err)
 		jbd2_journal_abort(journal, err);
@@ -1046,9 +1128,12 @@
 	if (commit_transaction->t_checkpoint_list == NULL &&
 	    commit_transaction->t_checkpoint_io_list == NULL) {
 		__jbd2_journal_drop_transaction(journal, commit_transaction);
-		kfree(commit_transaction);
+		jbd2_free_transaction(commit_transaction);
 	}
 	spin_unlock(&journal->j_list_lock);
 	write_unlock(&journal->j_state_lock);
 	wake_up(&journal->j_wait_done_commit);
+
+	put_cause_list(cause_list);
+
 }
diff -rubBN linux-3.2.51/fs/jbd2/journal.c ../split-release/release/linux-3.2.51/fs/jbd2/journal.c
--- linux-3.2.51/fs/jbd2/journal.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/fs/jbd2/journal.c	2015-09-01 05:42:48.000000000 +0200
@@ -51,6 +51,7 @@
 #include <asm/uaccess.h>
 #include <asm/page.h>
 #include <asm/system.h>
+#include <linux/cause_tags.h>
 
 EXPORT_SYMBOL(jbd2_journal_extend);
 EXPORT_SYMBOL(jbd2_journal_stop);
@@ -2096,6 +2097,8 @@
 
 static void journal_free_journal_head(struct journal_head *jh)
 {
+	put_cause_list(jh->causes);
+
 #ifdef CONFIG_JBD2_DEBUG
 	atomic_dec(&nr_journal_heads);
 	memset(jh, JBD2_POISON_FREE, sizeof(*jh));
@@ -2168,6 +2171,7 @@
 			goto repeat;
 		}
 
+		new_jh->causes = NULL;
 		jh = new_jh;
 		new_jh = NULL;		/* We consumed it */
 		set_buffer_jbd(bh);
diff -rubBN linux-3.2.51/fs/jbd2/revoke.c ../split-release/release/linux-3.2.51/fs/jbd2/revoke.c
--- linux-3.2.51/fs/jbd2/revoke.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/fs/jbd2/revoke.c	2015-09-01 05:42:48.000000000 +0200
@@ -89,6 +89,7 @@
 #include <linux/bio.h>
 #endif
 #include <linux/log2.h>
+#include <linux/cause_tags.h>
 
 static struct kmem_cache *jbd2_revoke_record_cache;
 static struct kmem_cache *jbd2_revoke_table_cache;
@@ -120,7 +121,8 @@
 static void write_one_revoke_record(journal_t *, transaction_t *,
 				    struct journal_head **, int *,
 				    struct jbd2_revoke_record_s *, int);
-static void flush_descriptor(journal_t *, struct journal_head *, int, int);
+static void flush_descriptor(journal_t *, struct journal_head *,
+							 int, int, struct cause_list* cause_list);
 #endif
 
 /* Utility functions to maintain the revoke table */
@@ -501,7 +503,8 @@
  */
 void jbd2_journal_write_revoke_records(journal_t *journal,
 				       transaction_t *transaction,
-				       int write_op)
+				       int write_op,
+				       struct cause_list* cause_list)
 {
 	struct journal_head *descriptor;
 	struct jbd2_revoke_record_s *record;
@@ -532,7 +535,7 @@
 		}
 	}
 	if (descriptor)
-		flush_descriptor(journal, descriptor, offset, write_op);
+		flush_descriptor(journal, descriptor, offset, write_op, cause_list);
 	jbd_debug(1, "Wrote %d revoke records\n", count);
 }
 
@@ -565,7 +568,7 @@
 	/* Make sure we have a descriptor with space left for the record */
 	if (descriptor) {
 		if (offset == journal->j_blocksize) {
-			flush_descriptor(journal, descriptor, offset, write_op);
+			flush_descriptor(journal, descriptor, offset, write_op, NULL);
 			descriptor = NULL;
 		}
 	}
@@ -610,7 +613,8 @@
 
 static void flush_descriptor(journal_t *journal,
 			     struct journal_head *descriptor,
-			     int offset, int write_op)
+			     int offset, int write_op, 
+			     struct cause_list* cause_list)
 {
 	jbd2_journal_revoke_header_t *header;
 	struct buffer_head *bh = jh2bh(descriptor);
@@ -625,6 +629,7 @@
 	set_buffer_jwrite(bh);
 	BUFFER_TRACE(bh, "write");
 	set_buffer_dirty(bh);
+	bh->causes = get_cause_list(cause_list); // new ref
 	write_dirty_buffer(bh, write_op);
 }
 #endif
diff -rubBN linux-3.2.51/fs/jbd2/transaction.c ../split-release/release/linux-3.2.51/fs/jbd2/transaction.c
--- linux-3.2.51/fs/jbd2/transaction.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/fs/jbd2/transaction.c	2015-09-01 05:42:48.000000000 +0200
@@ -29,10 +29,19 @@
 #include <linux/backing-dev.h>
 #include <linux/bug.h>
 #include <linux/module.h>
+#include <linux/kthread.h>
+#include <linux/cause_tags.h>
+
 
 static void __jbd2_journal_temp_unlink_buffer(struct journal_head *jh);
 static void __jbd2_journal_unfile_buffer(struct journal_head *jh);
 
+void jbd2_free_transaction(transaction_t *transaction) {
+	if (transaction)
+		put_cause_list(transaction->causes);
+	kfree(transaction);
+}
+
 /*
  * jbd2_get_transaction: obtain a new transaction_t object.
  *
@@ -72,6 +81,8 @@
 	transaction->t_max_wait = 0;
 	transaction->t_start = jiffies;
 
+	transaction->causes = new_cause_list();
+
 	return transaction;
 }
 
@@ -162,7 +173,7 @@
 	if (is_journal_aborted(journal) ||
 	    (journal->j_errno != 0 && !(journal->j_flags & JBD2_ACK_ERR))) {
 		read_unlock(&journal->j_state_lock);
-		kfree(new_transaction);
+		jbd2_free_transaction(new_transaction);
 		return -EROFS;
 	}
 
@@ -285,7 +296,7 @@
 	read_unlock(&journal->j_state_lock);
 
 	lock_map_acquire(&handle->h_lockdep_map);
-	kfree(new_transaction);
+	jbd2_free_transaction(new_transaction);
 	return 0;
 }
 
@@ -1099,6 +1110,21 @@
 		handle->h_buffer_credits--;
 	}
 
+	cause_list_add(&transaction->causes, current);
+	cause_list_add(&jh->causes, current);
+
+	/*
+	if(current->causes){
+		struct io_cause *cause;
+		list_for_each_entry(cause, &current->causes->items, list){
+			printk("bh: %p add cause %d\n", bh, cause->account_id);
+		}
+
+	}else{
+		printk("bh: %p add cause %d\n", current->account_id);
+	}
+	*/
+
 	/*
 	 * fastpath, to avoid expensive locking.  If this buffer is already
 	 * on the running transaction's metadata list there is nothing to do.
@@ -1390,6 +1417,8 @@
 	 * writes.  No point in waiting for joiners in that case.
 	 */
 	pid = current->pid;
+	
+#ifndef SPLIT_NODEP
 	if (handle->h_sync && journal->j_last_sync_writer != pid) {
 		u64 commit_time, trans_time;
 
@@ -1414,6 +1443,7 @@
 			schedule_hrtimeout(&expires, HRTIMER_MODE_ABS);
 		}
 	}
+#endif
 
 	if (handle->h_sync)
 		transaction->t_synchronous_commit = 1;
@@ -2056,6 +2087,21 @@
 		J_ASSERT_JH(jh, !jh->b_frozen_data);
 		return;
 	case BJ_Metadata:
+#ifndef DISABLE_CAUSES
+		// don't blame ourselves for things like commit records, even
+		// though we write them!  This should be blamed on the processes
+		// that added records to start with.  Do this by checking if the
+		// thread is associated with the jbd2 journal.
+		/*
+		if (current->flags&PF_KTHREAD &&
+			kthread_data(current) == transaction->t_journal) {
+			// skip
+		} else {
+			cause_list_add(&transaction->causes, current);
+		}
+		*/
+#endif
+
 		transaction->t_nr_buffers++;
 		list = &transaction->t_buffers;
 		break;
diff -rubBN linux-3.2.51/fs/namei.c ../split-release/release/linux-3.2.51/fs/namei.c
--- linux-3.2.51/fs/namei.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/fs/namei.c	2015-09-01 05:42:49.000000000 +0200
@@ -34,6 +34,7 @@
 #include <linux/fs_struct.h>
 #include <linux/posix_acl.h>
 #include <asm/uaccess.h>
+#include <linux/blkdev.h>
 
 #include "internal.h"
 
@@ -1981,21 +1982,47 @@
 int vfs_create(struct inode *dir, struct dentry *dentry, int mode,
 		struct nameidata *nd)
 {
-	int error = may_create(dir, dentry);
+	struct request_queue* rq;
+	struct elevator_syscall_ops sops;
+	struct module* module;
+	void* opaque = NULL;
 
-	if (error)
+	int error;
+
+	// intercept entry
+	get_elevator_call_info_from_inode(dir, &rq, &module, &sops);
+	if(sops.create_entry_fn){
+		error = sops.create_entry_fn(rq, dir, dentry, mode, &opaque, sops.sched_uniq);
+		if (error) {
+			BUG_ON(!module);
+			module_put(module);
 		return error;
+		}
+	}
 
-	if (!dir->i_op->create)
-		return -EACCES;	/* shouldn't it be ENOSYS? */
+	error = may_create(dir, dentry);
+
+	if (error)
+		goto out;
+
+	if (!dir->i_op->create) {
+		error = -EACCES;	/* shouldn't it be ENOSYS? */
+		goto out;
+	}
 	mode &= S_IALLUGO;
 	mode |= S_IFREG;
 	error = security_inode_create(dir, dentry, mode);
 	if (error)
-		return error;
+		goto out;
 	error = dir->i_op->create(dir, dentry, mode, nd);
 	if (!error)
 		fsnotify_create(dir, dentry);
+ out:
+	if(sops.create_return_fn)
+		sops.create_return_fn(rq, opaque, error, sops.sched_uniq);
+	if(module)
+		module_put(module);
+
 	return error;
 }
 
@@ -2545,6 +2572,12 @@
 
 int vfs_mkdir(struct inode *dir, struct dentry *dentry, int mode)
 {
+	struct request_queue* rq;
+	struct elevator_syscall_ops sops;
+	struct module* module;
+	void* opaque = NULL;
+
+	//it's ok to check permission upfront before queuing since no I/O is performed
 	int error = may_create(dir, dentry);
 
 	if (error)
@@ -2553,14 +2586,31 @@
 	if (!dir->i_op->mkdir)
 		return -EPERM;
 
+	// intercept entry
+	get_elevator_call_info_from_inode(dir, &rq, &module, &sops);
+	if(sops.mkdir_entry_fn){
+		error = sops.mkdir_entry_fn(rq, dir, dentry, mode, &opaque, sops.sched_uniq);
+		if (error) {
+			BUG_ON(!module);
+			module_put(module);
+			return error;
+		}
+	}
+
 	mode &= (S_IRWXUGO|S_ISVTX);
 	error = security_inode_mkdir(dir, dentry, mode);
 	if (error)
-		return error;
+		goto out;
 
 	error = dir->i_op->mkdir(dir, dentry, mode);
 	if (!error)
 		fsnotify_mkdir(dir, dentry);
+
+out:
+	if(sops.mkdir_return_fn)
+		sops.mkdir_return_fn(rq, opaque, error, sops.sched_uniq);
+	if(module)
+		module_put(module);
 	return error;
 }
 
diff -rubBN linux-3.2.51/fs/read_write.c ../split-release/release/linux-3.2.51/fs/read_write.c
--- linux-3.2.51/fs/read_write.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/fs/read_write.c	2015-09-01 05:42:49.000000000 +0200
@@ -9,12 +9,13 @@
 #include <linux/fcntl.h>
 #include <linux/file.h>
 #include <linux/uio.h>
+#include <linux/module.h>
 #include <linux/fsnotify.h>
 #include <linux/security.h>
-#include <linux/module.h>
 #include <linux/syscalls.h>
 #include <linux/pagemap.h>
 #include <linux/splice.h>
+#include <linux/blkdev.h>
 #include "read_write.h"
 
 #include <asm/uaccess.h>
@@ -419,6 +420,10 @@
 
 ssize_t vfs_write(struct file *file, const char __user *buf, size_t count, loff_t *pos)
 {
+	struct request_queue* rq;
+	struct elevator_syscall_ops sops;
+	struct module* module;
+	void* opaque = NULL;
 	ssize_t ret;
 
 	if (!(file->f_mode & FMODE_WRITE))
@@ -428,6 +433,17 @@
 	if (unlikely(!access_ok(VERIFY_READ, buf, count)))
 		return -EFAULT;
 
+	// intercept entry
+	get_elevator_call_info(file, &rq, &module, &sops);
+	if (sops.write_entry_fn) {
+		ret = sops.write_entry_fn(rq, file, count, pos, &opaque, sops.sched_uniq);
+		if (ret) {
+			BUG_ON(!module);
+			module_put(module);
+			return ret;
+		}
+	}
+
 	ret = rw_verify_area(WRITE, file, pos, count);
 	if (ret >= 0) {
 		count = ret;
@@ -442,6 +458,12 @@
 		inc_syscw(current);
 	}
 
+	// intercept exit
+	if (sops.write_return_fn)
+		sops.write_return_fn(rq, opaque, ret, sops.sched_uniq);
+	if (module)
+		module_put(module);
+
 	return ret;
 }
 
diff -rubBN linux-3.2.51/fs/split_account.h ../split-release/release/linux-3.2.51/fs/split_account.h
--- linux-3.2.51/fs/split_account.h	1970-01-01 01:00:00.000000000 +0100
+++ ../split-release/release/linux-3.2.51/fs/split_account.h	2015-09-01 05:42:49.000000000 +0200
@@ -0,0 +1,61 @@
+#include <linux/list.h>
+#include <linux/hashtable.h>
+#include "io_batch.h"
+
+#define KB (1024)
+#define MB (1024*KB)
+#define BATCH_SIZE (64)
+#define BATCH_BYTE_CAP (32*MB)
+
+struct account {
+	//general account stuff
+	int account_id;
+	struct hlist_node node;
+	struct list_head list;
+
+	//stats
+	int num_read_req;
+	int num_write_req;
+	int num_write_call;
+	int num_write_passed;
+	int num_write_blocked;
+	int num_fsync_call;
+	int set_vio_to_global_min;
+
+	//scheduler specific
+	int ioprio;
+	struct list_head io_work_list_head;
+	long vio_counter;
+	long estimate;
+	// batch_estimate describes how much we have charged for
+	// this batch prior the the batch being complete.  When
+	// we actually charge for the batch, we need to refund this.
+	size_t batch_estimate;
+	int vio_reset;
+	int seq_counter;
+	int last_sector;
+
+	unsigned long last_end_request;
+	unsigned long slice_begin;
+	unsigned long slice_end;
+
+	int read_expire;
+	int write_expire;
+	int fsync_expire;
+	int dirty_pages;
+
+	struct io_batch *sim_batch;
+};
+
+struct acct_hash {
+	DECLARE_HASHTABLE(accounts, 10);
+};
+
+void init_acct_hash(struct acct_hash *acct_hash);
+struct account *get_account(struct acct_hash *acct_hash, int account_id);
+int get_account_id(struct task_struct* tsk);
+void print_accounts_stat(struct acct_hash* acct_hash);
+void free_accounts(struct acct_hash* acct_hash);
+
+#define acct_hash_for_each(name, bkt, node, obj, member)	\
+	hash_for_each((name)->accounts, bkt, node, obj, member)
diff -rubBN linux-3.2.51/fs/split_sched.h ../split-release/release/linux-3.2.51/fs/split_sched.h
--- linux-3.2.51/fs/split_sched.h	1970-01-01 01:00:00.000000000 +0100
+++ ../split-release/release/linux-3.2.51/fs/split_sched.h	2015-09-01 05:42:48.000000000 +0200
@@ -0,0 +1,111 @@
+#ifndef __SPLIT_SCHED_H__
+#define __SPLIT_SCHED_H__
+#include <linux/fs.h>
+#include <linux/completion.h>
+#include <linux/ioprio.h>
+#include <linux/rbtree.h>
+#include <linux/cause_tags.h>
+
+#define RQ_CAUSES(rq) (rq)->elevator_private[1]
+#define RQ_CAUSES_CAST(rq) ((struct cause_list *)(rq)->elevator_private[1])
+
+/*
+ * supported I/O related syscall and block level requests for scheduling
+ */
+enum SPLIT_SCHED_IO_TYPE{
+	SCHED_SYS_WRITE,
+	SCHED_SYS_FSYNC,
+	SCHED_SYS_MKDIR,
+	SCHED_READ_REQ,
+	SCHED_WRITE_REQ,
+};
+
+struct io_work {
+	enum SPLIT_SCHED_IO_TYPE type;
+	struct list_head list;
+	struct rb_node rb_node;
+	unsigned long expire;
+};
+
+struct io_batch_desc {
+	struct list_head work_selected;
+	int one_loop_work;
+	spinlock_t lock;
+};
+
+struct wrdesc {
+	struct account *acct;
+	struct file *w_file;
+	size_t w_count;
+	loff_t w_pos;
+	struct io_work io_work;
+	struct completion w_completion;
+	struct timeval w_time;
+	struct io_batch_desc *owner_batch;
+	int w_ret;
+};
+
+struct fsync_desc {
+	struct account *acct;
+	struct split_io_context* sic;
+	struct file* f_file;
+	int f_datasync;
+	struct io_work io_work;
+	struct completion f_completion;
+	struct timeval f_time; //time when user called fsync
+	struct io_batch_desc* owner_batch;
+	int f_ret;
+};
+
+struct mkdir_desc {
+	struct account *acct;
+	struct inode *dir;
+	struct dentry *dentry;
+	int mode;
+	int ret;
+	struct io_work io_work;
+	struct completion completion;
+	struct timeval time;
+	struct io_batch_desc* owner_batch;
+};
+
+struct req_desc {
+	struct account *acct;
+	struct io_work io_work;
+	struct request *rq;
+	struct request_queue *q;
+	struct io_batch_desc *owner_batch;
+};
+
+static inline struct fsync_desc *FSYNC_IO(struct io_work *io_work){
+	return container_of(io_work, struct fsync_desc, io_work);
+}
+
+static inline struct wrdesc *WR_IO(struct io_work *io_work){
+	return container_of(io_work, struct wrdesc, io_work);
+}
+
+static inline struct mkdir_desc *MKDIR_IO(struct io_work *io_work){
+	return container_of(io_work, struct mkdir_desc, io_work);
+}
+
+static inline struct req_desc *REQ_IO(struct io_work *io_work){
+	return container_of(io_work, struct req_desc, io_work);
+}
+
+struct fsync_desc* get_fsync_desc(struct file* file, int datasync);
+void put_fsync_desc(struct fsync_desc* fsync_desc);
+
+struct wrdesc *get_wrdesc(struct file *file, size_t count, loff_t *pos);
+void put_wrdesc(struct wrdesc *wrdesc);
+
+struct mkdir_desc *get_mkdir_desc(struct inode *dir, struct dentry *dentry, int mode);
+void put_mkdir_desc(struct mkdir_desc *mkdir_desc);
+
+struct req_desc *get_req_desc(struct request_queue *q, struct request *rq, gfp_t gfp_mask);
+void put_req_desc(struct req_desc* req_desc);
+
+
+int split_set_request(struct request_queue *q, struct request *rq, gfp_t gfp_mask);
+void split_put_request(struct request *rq);
+#endif
diff -rubBN linux-3.2.51/fs/sync.c ../split-release/release/linux-3.2.51/fs/sync.c
--- linux-3.2.51/fs/sync.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/fs/sync.c	2015-09-01 05:42:49.000000000 +0200
@@ -6,8 +6,8 @@
 #include <linux/file.h>
 #include <linux/fs.h>
 #include <linux/slab.h>
-#include <linux/module.h>
 #include <linux/namei.h>
+#include <linux/module.h>
 #include <linux/sched.h>
 #include <linux/writeback.h>
 #include <linux/syscalls.h>
@@ -16,6 +16,7 @@
 #include <linux/quotaops.h>
 #include <linux/buffer_head.h>
 #include <linux/backing-dev.h>
+#include <linux/blkdev.h>
 #include "internal.h"
 
 #define VALID_FLAGS (SYNC_FILE_RANGE_WAIT_BEFORE|SYNC_FILE_RANGE_WRITE| \
@@ -165,9 +166,35 @@
  */
 int vfs_fsync_range(struct file *file, loff_t start, loff_t end, int datasync)
 {
+	struct request_queue* rq;
+	struct elevator_syscall_ops sops;
+	struct module *module;
+	void* opaque = NULL;
+	int rv;
+
 	if (!file->f_op || !file->f_op->fsync)
 		return -EINVAL;
-	return file->f_op->fsync(file, start, end, datasync);
+
+	// intercept entry
+	get_elevator_call_info(file, &rq, &module, &sops);
+	if (sops.fsync_entry_fn) {
+		rv = sops.fsync_entry_fn(rq, file, datasync, &opaque, sops.sched_uniq);
+		if (rv) {
+			BUG_ON(!module);
+			module_put(module);
+			return rv;
+		}
+	}
+
+	rv = file->f_op->fsync(file, start, end, datasync);
+
+	// intercept exit
+	if (sops.fsync_return_fn)
+		sops.fsync_return_fn(rq, opaque, rv, sops.sched_uniq);
+	if (module)
+		module_put(module);
+
+	return rv;
 }
 EXPORT_SYMBOL(vfs_fsync_range);
 
diff -rubBN linux-3.2.51/fs/tmp.c ../split-release/release/linux-3.2.51/fs/tmp.c
--- linux-3.2.51/fs/tmp.c	1970-01-01 01:00:00.000000000 +0100
+++ ../split-release/release/linux-3.2.51/fs/tmp.c	2015-09-01 05:42:49.000000000 +0200
@@ -0,0 +1,4831 @@
+/*
+ *  linux/fs/ext4/inode.c
+ *
+ * Copyright (C) 1992, 1993, 1994, 1995
+ * Remy Card (card@masi.ibp.fr)
+ * Laboratoire MASI - Institut Blaise Pascal
+ * Universite Pierre et Marie Curie (Paris VI)
+ *
+ *  from
+ *
+ *  linux/fs/minix/inode.c
+ *
+ *  Copyright (C) 1991, 1992  Linus Torvalds
+ *
+ *  64-bit file support on 64-bit platforms by Jakub Jelinek
+ *	(jj@sunsite.ms.mff.cuni.cz)
+ *
+ *  Assorted race fixes, rewrite of ext4_get_block() by Al Viro, 2000
+ */
+
+#include <linux/module.h>
+#include <linux/fs.h>
+#include <linux/time.h>
+#include <linux/jbd2.h>
+#include <linux/highuid.h>
+#include <linux/pagemap.h>
+#include <linux/quotaops.h>
+#include <linux/string.h>
+#include <linux/buffer_head.h>
+#include <linux/writeback.h>
+#include <linux/pagevec.h>
+#include <linux/mpage.h>
+#include <linux/namei.h>
+#include <linux/uio.h>
+#include <linux/bio.h>
+#include <linux/workqueue.h>
+#include <linux/kernel.h>
+#include <linux/printk.h>
+#include <linux/slab.h>
+#include <linux/ratelimit.h>
+#include <linux/cause_tags.h>
+
+#include "ext4_jbd2.h"
+#include "xattr.h"
+#include "acl.h"
+#include "truncate.h"
+
+#include <trace/events/ext4.h>
+
+#define MPAGE_DA_EXTENT_TAIL 0x01
+
+static inline int ext4_begin_ordered_truncate(struct inode *inode,
+					      loff_t new_size)
+{
+	trace_ext4_begin_ordered_truncate(inode, new_size);
+	/*
+	 * If jinode is zero, then we never opened the file for
+	 * writing, so there's no need to call
+	 * jbd2_journal_begin_ordered_truncate() since there's no
+	 * outstanding writes we need to flush.
+	 */
+	if (!EXT4_I(inode)->jinode)
+		return 0;
+	return jbd2_journal_begin_ordered_truncate(EXT4_JOURNAL(inode),
+						   EXT4_I(inode)->jinode,
+						   new_size);
+}
+
+static void ext4_invalidatepage(struct page *page, unsigned long offset);
+static int noalloc_get_block_write(struct inode *inode, sector_t iblock,
+				   struct buffer_head *bh_result, int create);
+static int ext4_set_bh_endio(struct buffer_head *bh, struct inode *inode);
+static void ext4_end_io_buffer_write(struct buffer_head *bh, int uptodate);
+static int __ext4_journalled_writepage(struct page *page, unsigned int len);
+static int ext4_bh_delay_or_unwritten(handle_t *handle, struct buffer_head *bh);
+
+/*
+ * Test whether an inode is a fast symlink.
+ */
+static int ext4_inode_is_fast_symlink(struct inode *inode)
+{
+	int ea_blocks = EXT4_I(inode)->i_file_acl ?
+		(inode->i_sb->s_blocksize >> 9) : 0;
+
+	return (S_ISLNK(inode->i_mode) && inode->i_blocks - ea_blocks == 0);
+}
+
+/*
+ * Restart the transaction associated with *handle.  This does a commit,
+ * so before we call here everything must be consistently dirtied against
+ * this transaction.
+ */
+int ext4_truncate_restart_trans(handle_t *handle, struct inode *inode,
+				 int nblocks)
+{
+	int ret;
+
+	/*
+	 * Drop i_data_sem to avoid deadlock with ext4_map_blocks.  At this
+	 * moment, get_block can be called only for blocks inside i_size since
+	 * page cache has been already dropped and writes are blocked by
+	 * i_mutex. So we can safely drop the i_data_sem here.
+	 */
+	BUG_ON(EXT4_JOURNAL(inode) == NULL);
+	jbd_debug(2, "restarting handle %p\n", handle);
+	up_write(&EXT4_I(inode)->i_data_sem);
+	ret = ext4_journal_restart(handle, nblocks);
+	down_write(&EXT4_I(inode)->i_data_sem);
+	ext4_discard_preallocations(inode);
+
+	return ret;
+}
+
+/*
+ * Called at the last iput() if i_nlink is zero.
+ */
+void ext4_evict_inode(struct inode *inode)
+{
+	handle_t *handle;
+	int err;
+
+	trace_ext4_evict_inode(inode);
+
+	ext4_ioend_wait(inode);
+
+	if (inode->i_nlink) {
+		/*
+		 * When journalling data dirty buffers are tracked only in the
+		 * journal. So although mm thinks everything is clean and
+		 * ready for reaping the inode might still have some pages to
+		 * write in the running transaction or waiting to be
+		 * checkpointed. Thus calling jbd2_journal_invalidatepage()
+		 * (via truncate_inode_pages()) to discard these buffers can
+		 * cause data loss. Also even if we did not discard these
+		 * buffers, we would have no way to find them after the inode
+		 * is reaped and thus user could see stale data if he tries to
+		 * read them before the transaction is checkpointed. So be
+		 * careful and force everything to disk here... We use
+		 * ei->i_datasync_tid to store the newest transaction
+		 * containing inode's data.
+		 *
+		 * Note that directories do not have this problem because they
+		 * don't use page cache.
+		 */
+		if (ext4_should_journal_data(inode) &&
+		    (S_ISLNK(inode->i_mode) || S_ISREG(inode->i_mode)) &&
+		    inode->i_ino != EXT4_JOURNAL_INO) {
+			journal_t *journal = EXT4_SB(inode->i_sb)->s_journal;
+			tid_t commit_tid = EXT4_I(inode)->i_datasync_tid;
+
+			jbd2_complete_transaction(journal, commit_tid);
+			filemap_write_and_wait(&inode->i_data);
+		}
+		truncate_inode_pages(&inode->i_data, 0);
+		goto no_delete;
+	}
+
+	if (!is_bad_inode(inode))
+		dquot_initialize(inode);
+
+	if (ext4_should_order_data(inode))
+		ext4_begin_ordered_truncate(inode, 0);
+	truncate_inode_pages(&inode->i_data, 0);
+
+	if (is_bad_inode(inode))
+		goto no_delete;
+
+	handle = ext4_journal_start(inode, ext4_blocks_for_truncate(inode)+3);
+	if (IS_ERR(handle)) {
+		ext4_std_error(inode->i_sb, PTR_ERR(handle));
+		/*
+		 * If we're going to skip the normal cleanup, we still need to
+		 * make sure that the in-core orphan linked list is properly
+		 * cleaned up.
+		 */
+		ext4_orphan_del(NULL, inode);
+		goto no_delete;
+	}
+
+	if (IS_SYNC(inode))
+		ext4_handle_sync(handle);
+	inode->i_size = 0;
+	err = ext4_mark_inode_dirty(handle, inode);
+	if (err) {
+		ext4_warning(inode->i_sb,
+			     "couldn't mark inode dirty (err %d)", err);
+		goto stop_handle;
+	}
+	if (inode->i_blocks)
+		ext4_truncate(inode);
+
+	/*
+	 * ext4_ext_truncate() doesn't reserve any slop when it
+	 * restarts journal transactions; therefore there may not be
+	 * enough credits left in the handle to remove the inode from
+	 * the orphan list and set the dtime field.
+	 */
+	if (!ext4_handle_has_enough_credits(handle, 3)) {
+		err = ext4_journal_extend(handle, 3);
+		if (err > 0)
+			err = ext4_journal_restart(handle, 3);
+		if (err != 0) {
+			ext4_warning(inode->i_sb,
+				     "couldn't extend journal (err %d)", err);
+		stop_handle:
+			ext4_journal_stop(handle);
+			ext4_orphan_del(NULL, inode);
+			goto no_delete;
+		}
+	}
+
+	/*
+	 * Kill off the orphan record which ext4_truncate created.
+	 * AKPM: I think this can be inside the above `if'.
+	 * Note that ext4_orphan_del() has to be able to cope with the
+	 * deletion of a non-existent orphan - this is because we don't
+	 * know if ext4_truncate() actually created an orphan record.
+	 * (Well, we could do this if we need to, but heck - it works)
+	 */
+	ext4_orphan_del(handle, inode);
+	EXT4_I(inode)->i_dtime	= get_seconds();
+
+	/*
+	 * One subtle ordering requirement: if anything has gone wrong
+	 * (transaction abort, IO errors, whatever), then we can still
+	 * do these next steps (the fs will already have been marked as
+	 * having errors), but we can't free the inode if the mark_dirty
+	 * fails.
+	 */
+	if (ext4_mark_inode_dirty(handle, inode))
+		/* If that failed, just do the required in-core inode clear. */
+		ext4_clear_inode(inode);
+	else
+		ext4_free_inode(handle, inode);
+	ext4_journal_stop(handle);
+	return;
+no_delete:
+	ext4_clear_inode(inode);	/* We must guarantee clearing of inode... */
+}
+
+#ifdef CONFIG_QUOTA
+qsize_t *ext4_get_reserved_space(struct inode *inode)
+{
+	return &EXT4_I(inode)->i_reserved_quota;
+}
+#endif
+
+/*
+ * Calculate the number of metadata blocks need to reserve
+ * to allocate a block located at @lblock
+ */
+static int ext4_calc_metadata_amount(struct inode *inode, ext4_lblk_t lblock)
+{
+	if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))
+		return ext4_ext_calc_metadata_amount(inode, lblock);
+
+	return ext4_ind_calc_metadata_amount(inode, lblock);
+}
+
+/*
+ * Called with i_data_sem down, which is important since we can call
+ * ext4_discard_preallocations() from here.
+ */
+void ext4_da_update_reserve_space(struct inode *inode,
+					int used, int quota_claim)
+{
+	struct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);
+	struct ext4_inode_info *ei = EXT4_I(inode);
+
+	spin_lock(&ei->i_block_reservation_lock);
+	trace_ext4_da_update_reserve_space(inode, used, quota_claim);
+	if (unlikely(used > ei->i_reserved_data_blocks)) {
+		ext4_msg(inode->i_sb, KERN_NOTICE, "%s: ino %lu, used %d "
+			 "with only %d reserved data blocks\n",
+			 __func__, inode->i_ino, used,
+			 ei->i_reserved_data_blocks);
+		WARN_ON(1);
+		used = ei->i_reserved_data_blocks;
+	}
+
+	if (unlikely(ei->i_allocated_meta_blocks > ei->i_reserved_meta_blocks)) {
+		ext4_msg(inode->i_sb, KERN_NOTICE, "%s: ino %lu, allocated %d "
+			 "with only %d reserved metadata blocks\n", __func__,
+			 inode->i_ino, ei->i_allocated_meta_blocks,
+			 ei->i_reserved_meta_blocks);
+		WARN_ON(1);
+		ei->i_allocated_meta_blocks = ei->i_reserved_meta_blocks;
+	}
+
+	/* Update per-inode reservations */
+	ei->i_reserved_data_blocks -= used;
+	ei->i_reserved_meta_blocks -= ei->i_allocated_meta_blocks;
+	percpu_counter_sub(&sbi->s_dirtyclusters_counter,
+			   used + ei->i_allocated_meta_blocks);
+	ei->i_allocated_meta_blocks = 0;
+
+	if (ei->i_reserved_data_blocks == 0) {
+		/*
+		 * We can release all of the reserved metadata blocks
+		 * only when we have written all of the delayed
+		 * allocation blocks.
+		 */
+		percpu_counter_sub(&sbi->s_dirtyclusters_counter,
+				   ei->i_reserved_meta_blocks);
+		ei->i_reserved_meta_blocks = 0;
+		ei->i_da_metadata_calc_len = 0;
+	}
+	spin_unlock(&EXT4_I(inode)->i_block_reservation_lock);
+
+	/* Update quota subsystem for data blocks */
+	if (quota_claim)
+		dquot_claim_block(inode, EXT4_C2B(sbi, used));
+	else {
+		/*
+		 * We did fallocate with an offset that is already delayed
+		 * allocated. So on delayed allocated writeback we should
+		 * not re-claim the quota for fallocated blocks.
+		 */
+		dquot_release_reservation_block(inode, EXT4_C2B(sbi, used));
+	}
+
+	/*
+	 * If we have done all the pending block allocations and if
+	 * there aren't any writers on the inode, we can discard the
+	 * inode's preallocations.
+	 */
+	if ((ei->i_reserved_data_blocks == 0) &&
+	    (atomic_read(&inode->i_writecount) == 0))
+		ext4_discard_preallocations(inode);
+}
+
+static int __check_block_validity(struct inode *inode, const char *func,
+				unsigned int line,
+				struct ext4_map_blocks *map)
+{
+	if (!ext4_data_block_valid(EXT4_SB(inode->i_sb), map->m_pblk,
+				   map->m_len)) {
+		ext4_error_inode(inode, func, line, map->m_pblk,
+				 "lblock %lu mapped to illegal pblock "
+				 "(length %d)", (unsigned long) map->m_lblk,
+				 map->m_len);
+		return -EIO;
+	}
+	return 0;
+}
+
+#define check_block_validity(inode, map)	\
+	__check_block_validity((inode), __func__, __LINE__, (map))
+
+/*
+ * Return the number of contiguous dirty pages in a given inode
+ * starting at page frame idx.
+ */
+static pgoff_t ext4_num_dirty_pages(struct inode *inode, pgoff_t idx,
+				    unsigned int max_pages)
+{
+	struct address_space *mapping = inode->i_mapping;
+	pgoff_t	index;
+	struct pagevec pvec;
+	pgoff_t num = 0;
+	int i, nr_pages, done = 0;
+
+	if (max_pages == 0)
+		return 0;
+	pagevec_init(&pvec, 0);
+	while (!done) {
+		index = idx;
+		nr_pages = pagevec_lookup_tag(&pvec, mapping, &index,
+					      PAGECACHE_TAG_DIRTY,
+					      (pgoff_t)PAGEVEC_SIZE);
+		if (nr_pages == 0)
+			break;
+		for (i = 0; i < nr_pages; i++) {
+			struct page *page = pvec.pages[i];
+			struct buffer_head *bh, *head;
+
+			lock_page(page);
+			if (unlikely(page->mapping != mapping) ||
+			    !PageDirty(page) ||
+			    PageWriteback(page) ||
+			    page->index != idx) {
+				done = 1;
+				unlock_page(page);
+				break;
+			}
+			if (page_has_buffers(page)) {
+				bh = head = page_buffers(page);
+				do {
+					if (!buffer_delay(bh) &&
+					    !buffer_unwritten(bh))
+						done = 1;
+					bh = bh->b_this_page;
+				} while (!done && (bh != head));
+			}
+			unlock_page(page);
+			if (done)
+				break;
+			idx++;
+			num++;
+			if (num >= max_pages) {
+				done = 1;
+				break;
+			}
+		}
+		pagevec_release(&pvec);
+	}
+	return num;
+}
+
+/*
+ * Sets the BH_Da_Mapped bit on the buffer heads corresponding to the given map.
+ */
+static void set_buffers_da_mapped(struct inode *inode,
+				   struct ext4_map_blocks *map)
+{
+	struct address_space *mapping = inode->i_mapping;
+	struct pagevec pvec;
+	int i, nr_pages;
+	pgoff_t index, end;
+
+	index = map->m_lblk >> (PAGE_CACHE_SHIFT - inode->i_blkbits);
+	end = (map->m_lblk + map->m_len - 1) >>
+		(PAGE_CACHE_SHIFT - inode->i_blkbits);
+
+	pagevec_init(&pvec, 0);
+	while (index <= end) {
+		nr_pages = pagevec_lookup(&pvec, mapping, index,
+					  min(end - index + 1,
+					      (pgoff_t)PAGEVEC_SIZE));
+		if (nr_pages == 0)
+			break;
+		for (i = 0; i < nr_pages; i++) {
+			struct page *page = pvec.pages[i];
+			struct buffer_head *bh, *head;
+
+			if (unlikely(page->mapping != mapping) ||
+			    !PageDirty(page))
+				break;
+
+			if (page_has_buffers(page)) {
+				bh = head = page_buffers(page);
+				do {
+					set_buffer_da_mapped(bh);
+					bh = bh->b_this_page;
+				} while (bh != head);
+			}
+			index++;
+		}
+		pagevec_release(&pvec);
+	}
+}
+
+/*
+ * The ext4_map_blocks() function tries to look up the requested blocks,
+ * and returns if the blocks are already mapped.
+ *
+ * Otherwise it takes the write lock of the i_data_sem and allocate blocks
+ * and store the allocated blocks in the result buffer head and mark it
+ * mapped.
+ *
+ * If file type is extents based, it will call ext4_ext_map_blocks(),
+ * Otherwise, call with ext4_ind_map_blocks() to handle indirect mapping
+ * based files
+ *
+ * On success, it returns the number of blocks being mapped or allocate.
+ * if create==0 and the blocks are pre-allocated and uninitialized block,
+ * the result buffer head is unmapped. If the create ==1, it will make sure
+ * the buffer head is mapped.
+ *
+ * It returns 0 if plain look up failed (blocks have not been allocated), in
+ * that case, buffer head is unmapped
+ *
+ * It returns the error in case of allocation failure.
+ */
+int ext4_map_blocks(handle_t *handle, struct inode *inode,
+		    struct ext4_map_blocks *map, int flags)
+{
+	int retval;
+
+	map->m_flags = 0;
+	ext_debug("ext4_map_blocks(): inode %lu, flag %d, max_blocks %u,"
+		  "logical block %lu\n", inode->i_ino, flags, map->m_len,
+		  (unsigned long) map->m_lblk);
+	/*
+	 * Try to see if we can get the block without requesting a new
+	 * file system block.
+	 */
+	down_read((&EXT4_I(inode)->i_data_sem));
+	if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {
+		retval = ext4_ext_map_blocks(handle, inode, map, flags &
+					     EXT4_GET_BLOCKS_KEEP_SIZE);
+	} else {
+		retval = ext4_ind_map_blocks(handle, inode, map, flags &
+					     EXT4_GET_BLOCKS_KEEP_SIZE);
+	}
+	up_read((&EXT4_I(inode)->i_data_sem));
+
+	if (retval > 0 && map->m_flags & EXT4_MAP_MAPPED) {
+		int ret = check_block_validity(inode, map);
+		if (ret != 0)
+			return ret;
+	}
+
+	/* If it is only a block(s) look up */
+	if ((flags & EXT4_GET_BLOCKS_CREATE) == 0)
+		return retval;
+
+	/*
+	 * Returns if the blocks have already allocated
+	 *
+	 * Note that if blocks have been preallocated
+	 * ext4_ext_get_block() returns the create = 0
+	 * with buffer head unmapped.
+	 */
+	if (retval > 0 && map->m_flags & EXT4_MAP_MAPPED)
+		return retval;
+
+	/*
+	 * When we call get_blocks without the create flag, the
+	 * BH_Unwritten flag could have gotten set if the blocks
+	 * requested were part of a uninitialized extent.  We need to
+	 * clear this flag now that we are committed to convert all or
+	 * part of the uninitialized extent to be an initialized
+	 * extent.  This is because we need to avoid the combination
+	 * of BH_Unwritten and BH_Mapped flags being simultaneously
+	 * set on the buffer_head.
+	 */
+	map->m_flags &= ~EXT4_MAP_UNWRITTEN;
+
+	/*
+	 * New blocks allocate and/or writing to uninitialized extent
+	 * will possibly result in updating i_data, so we take
+	 * the write lock of i_data_sem, and call get_blocks()
+	 * with create == 1 flag.
+	 */
+	down_write((&EXT4_I(inode)->i_data_sem));
+
+	/*
+	 * if the caller is from delayed allocation writeout path
+	 * we have already reserved fs blocks for allocation
+	 * let the underlying get_block() function know to
+	 * avoid double accounting
+	 */
+	if (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE)
+		ext4_set_inode_state(inode, EXT4_STATE_DELALLOC_RESERVED);
+	/*
+	 * We need to check for EXT4 here because migrate
+	 * could have changed the inode type in between
+	 */
+	if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {
+		retval = ext4_ext_map_blocks(handle, inode, map, flags);
+	} else {
+		retval = ext4_ind_map_blocks(handle, inode, map, flags);
+
+		if (retval > 0 && map->m_flags & EXT4_MAP_NEW) {
+			/*
+			 * We allocated new blocks which will result in
+			 * i_data's format changing.  Force the migrate
+			 * to fail by clearing migrate flags
+			 */
+			ext4_clear_inode_state(inode, EXT4_STATE_EXT_MIGRATE);
+		}
+
+		/*
+		 * Update reserved blocks/metadata blocks after successful
+		 * block allocation which had been deferred till now. We don't
+		 * support fallocate for non extent files. So we can update
+		 * reserve space here.
+		 */
+		if ((retval > 0) &&
+			(flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE))
+			ext4_da_update_reserve_space(inode, retval, 1);
+	}
+	if (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) {
+		ext4_clear_inode_state(inode, EXT4_STATE_DELALLOC_RESERVED);
+
+		/* If we have successfully mapped the delayed allocated blocks,
+		 * set the BH_Da_Mapped bit on them. Its important to do this
+		 * under the protection of i_data_sem.
+		 */
+		if (retval > 0 && map->m_flags & EXT4_MAP_MAPPED)
+			set_buffers_da_mapped(inode, map);
+	}
+
+	up_write((&EXT4_I(inode)->i_data_sem));
+	if (retval > 0 && map->m_flags & EXT4_MAP_MAPPED) {
+		int ret = check_block_validity(inode, map);
+		if (ret != 0)
+			return ret;
+	}
+	return retval;
+}
+
+/* Maximum number of blocks we map for direct IO at once. */
+#define DIO_MAX_BLOCKS 4096
+
+static int _ext4_get_block(struct inode *inode, sector_t iblock,
+			   struct buffer_head *bh, int flags)
+{
+	handle_t *handle = ext4_journal_current_handle();
+	struct ext4_map_blocks map;
+	int ret = 0, started = 0;
+	int dio_credits;
+
+	map.m_lblk = iblock;
+	map.m_len = bh->b_size >> inode->i_blkbits;
+
+	if (flags && !handle) {
+		/* Direct IO write... */
+		if (map.m_len > DIO_MAX_BLOCKS)
+			map.m_len = DIO_MAX_BLOCKS;
+		dio_credits = ext4_chunk_trans_blocks(inode, map.m_len);
+		handle = ext4_journal_start(inode, dio_credits);
+		if (IS_ERR(handle)) {
+			ret = PTR_ERR(handle);
+			return ret;
+		}
+		started = 1;
+	}
+
+	ret = ext4_map_blocks(handle, inode, &map, flags);
+	if (ret > 0) {
+		map_bh(bh, inode->i_sb, map.m_pblk);
+		bh->b_state = (bh->b_state & ~EXT4_MAP_FLAGS) | map.m_flags;
+		bh->b_size = inode->i_sb->s_blocksize * map.m_len;
+		ret = 0;
+	}
+	if (started)
+		ext4_journal_stop(handle);
+	return ret;
+}
+
+int ext4_get_block(struct inode *inode, sector_t iblock,
+		   struct buffer_head *bh, int create)
+{
+	return _ext4_get_block(inode, iblock, bh,
+			       create ? EXT4_GET_BLOCKS_CREATE : 0);
+}
+
+/*
+ * `handle' can be NULL if create is zero
+ */
+struct buffer_head *ext4_getblk(handle_t *handle, struct inode *inode,
+				ext4_lblk_t block, int create, int *errp)
+{
+	struct ext4_map_blocks map;
+	struct buffer_head *bh;
+	int fatal = 0, err;
+
+	J_ASSERT(handle != NULL || create == 0);
+
+	map.m_lblk = block;
+	map.m_len = 1;
+	err = ext4_map_blocks(handle, inode, &map,
+			      create ? EXT4_GET_BLOCKS_CREATE : 0);
+
+	if (err < 0)
+		*errp = err;
+	if (err <= 0)
+		return NULL;
+	*errp = 0;
+
+	bh = sb_getblk(inode->i_sb, map.m_pblk);
+	if (!bh) {
+		*errp = -ENOMEM;
+		return NULL;
+	}
+	if (map.m_flags & EXT4_MAP_NEW) {
+		J_ASSERT(create != 0);
+		J_ASSERT(handle != NULL);
+
+		/*
+		 * Now that we do not always journal data, we should
+		 * keep in mind whether this should always journal the
+		 * new buffer as metadata.  For now, regular file
+		 * writes use ext4_get_block instead, so it's not a
+		 * problem.
+		 */
+		lock_buffer(bh);
+		BUFFER_TRACE(bh, "call get_create_access");
+		fatal = ext4_journal_get_create_access(handle, bh);
+		if (!fatal && !buffer_uptodate(bh)) {
+			memset(bh->b_data, 0, inode->i_sb->s_blocksize);
+			set_buffer_uptodate(bh);
+		}
+		unlock_buffer(bh);
+		BUFFER_TRACE(bh, "call ext4_handle_dirty_metadata");
+		err = ext4_handle_dirty_metadata(handle, inode, bh);
+		if (!fatal)
+			fatal = err;
+	} else {
+		BUFFER_TRACE(bh, "not a new buffer");
+	}
+	if (fatal) {
+		*errp = fatal;
+		brelse(bh);
+		bh = NULL;
+	}
+	return bh;
+}
+
+struct buffer_head *ext4_bread(handle_t *handle, struct inode *inode,
+			       ext4_lblk_t block, int create, int *err)
+{
+	struct buffer_head *bh;
+
+	bh = ext4_getblk(handle, inode, block, create, err);
+	if (!bh)
+		return bh;
+	if (buffer_uptodate(bh))
+		return bh;
+	ll_rw_block(READ | REQ_META | REQ_PRIO, 1, &bh);
+	wait_on_buffer(bh);
+	if (buffer_uptodate(bh))
+		return bh;
+	put_bh(bh);
+	*err = -EIO;
+	return NULL;
+}
+
+static int walk_page_buffers(handle_t *handle,
+			     struct buffer_head *head,
+			     unsigned from,
+			     unsigned to,
+			     int *partial,
+			     int (*fn)(handle_t *handle,
+				       struct buffer_head *bh))
+{
+	struct buffer_head *bh;
+	unsigned block_start, block_end;
+	unsigned blocksize = head->b_size;
+	int err, ret = 0;
+	struct buffer_head *next;
+
+	for (bh = head, block_start = 0;
+	     ret == 0 && (bh != head || !block_start);
+	     block_start = block_end, bh = next) {
+		next = bh->b_this_page;
+		block_end = block_start + blocksize;
+		if (block_end <= from || block_start >= to) {
+			if (partial && !buffer_uptodate(bh))
+				*partial = 1;
+			continue;
+		}
+		err = (*fn)(handle, bh);
+		if (!ret)
+			ret = err;
+	}
+	return ret;
+}
+
+/*
+ * To preserve ordering, it is essential that the hole instantiation and
+ * the data write be encapsulated in a single transaction.  We cannot
+ * close off a transaction and start a new one between the ext4_get_block()
+ * and the commit_write().  So doing the jbd2_journal_start at the start of
+ * prepare_write() is the right place.
+ *
+ * Also, this function can nest inside ext4_writepage() ->
+ * block_write_full_page(). In that case, we *know* that ext4_writepage()
+ * has generated enough buffer credits to do the whole page.  So we won't
+ * block on the journal in that case, which is good, because the caller may
+ * be PF_MEMALLOC.
+ *
+ * By accident, ext4 can be reentered when a transaction is open via
+ * quota file writes.  If we were to commit the transaction while thus
+ * reentered, there can be a deadlock - we would be holding a quota
+ * lock, and the commit would never complete if another thread had a
+ * transaction open and was blocking on the quota lock - a ranking
+ * violation.
+ *
+ * So what we do is to rely on the fact that jbd2_journal_stop/journal_start
+ * will _not_ run commit under these circumstances because handle->h_ref
+ * is elevated.  We'll still have enough credits for the tiny quotafile
+ * write.
+ */
+static int do_journal_get_write_access(handle_t *handle,
+				       struct buffer_head *bh)
+{
+	int dirty = buffer_dirty(bh);
+	int ret;
+
+	if (!buffer_mapped(bh) || buffer_freed(bh))
+		return 0;
+	/*
+	 * __block_write_begin() could have dirtied some buffers. Clean
+	 * the dirty bit as jbd2_journal_get_write_access() could complain
+	 * otherwise about fs integrity issues. Setting of the dirty bit
+	 * by __block_write_begin() isn't a real problem here as we clear
+	 * the bit before releasing a page lock and thus writeback cannot
+	 * ever write the buffer.
+	 */
+	if (dirty)
+		clear_buffer_dirty(bh);
+	ret = ext4_journal_get_write_access(handle, bh);
+	if (!ret && dirty)
+		ret = ext4_handle_dirty_metadata(handle, NULL, bh);
+	return ret;
+}
+
+static int ext4_get_block_write(struct inode *inode, sector_t iblock,
+		   struct buffer_head *bh_result, int create);
+static int ext4_write_begin(struct file *file, struct address_space *mapping,
+			    loff_t pos, unsigned len, unsigned flags,
+			    struct page **pagep, void **fsdata)
+{
+	struct inode *inode = mapping->host;
+	int ret, needed_blocks;
+	handle_t *handle;
+	int retries = 0;
+	struct page *page;
+	pgoff_t index;
+	unsigned from, to;
+
+	trace_ext4_write_begin(inode, pos, len, flags);
+	/*
+	 * Reserve one block more for addition to orphan list in case
+	 * we allocate blocks but write fails for some reason
+	 */
+	needed_blocks = ext4_writepage_trans_blocks(inode) + 1;
+	index = pos >> PAGE_CACHE_SHIFT;
+	from = pos & (PAGE_CACHE_SIZE - 1);
+	to = from + len;
+
+retry:
+	handle = ext4_journal_start(inode, needed_blocks);
+	if (IS_ERR(handle)) {
+		ret = PTR_ERR(handle);
+		goto out;
+	}
+
+	/* We cannot recurse into the filesystem as the transaction is already
+	 * started */
+	flags |= AOP_FLAG_NOFS;
+
+	page = grab_cache_page_write_begin(mapping, index, flags);
+	if (!page) {
+		ext4_journal_stop(handle);
+		ret = -ENOMEM;
+		goto out;
+	}
+	*pagep = page;
+
+	if (ext4_should_dioread_nolock(inode))
+		ret = __block_write_begin(page, pos, len, ext4_get_block_write);
+	else
+		ret = __block_write_begin(page, pos, len, ext4_get_block);
+
+	if (!ret && ext4_should_journal_data(inode)) {
+		ret = walk_page_buffers(handle, page_buffers(page),
+				from, to, NULL, do_journal_get_write_access);
+	}
+
+	if (ret) {
+		unlock_page(page);
+		page_cache_release(page);
+		/*
+		 * __block_write_begin may have instantiated a few blocks
+		 * outside i_size.  Trim these off again. Don't need
+		 * i_size_read because we hold i_mutex.
+		 *
+		 * Add inode to orphan list in case we crash before
+		 * truncate finishes
+		 */
+		if (pos + len > inode->i_size && ext4_can_truncate(inode))
+			ext4_orphan_add(handle, inode);
+
+		ext4_journal_stop(handle);
+		if (pos + len > inode->i_size) {
+			ext4_truncate_failed_write(inode);
+			/*
+			 * If truncate failed early the inode might
+			 * still be on the orphan list; we need to
+			 * make sure the inode is removed from the
+			 * orphan list in that case.
+			 */
+			if (inode->i_nlink)
+				ext4_orphan_del(NULL, inode);
+		}
+	}
+
+	if (ret == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries))
+		goto retry;
+out:
+	return ret;
+}
+
+/* For write_end() in data=journal mode */
+static int write_end_fn(handle_t *handle, struct buffer_head *bh)
+{
+	if (!buffer_mapped(bh) || buffer_freed(bh))
+		return 0;
+	set_buffer_uptodate(bh);
+	return ext4_handle_dirty_metadata(handle, NULL, bh);
+}
+
+static int ext4_generic_write_end(struct file *file,
+				  struct address_space *mapping,
+				  loff_t pos, unsigned len, unsigned copied,
+				  struct page *page, void *fsdata)
+{
+	int i_size_changed = 0;
+	struct inode *inode = mapping->host;
+	handle_t *handle = ext4_journal_current_handle();
+
+	copied = block_write_end(file, mapping, pos, len, copied, page, fsdata);
+
+	/*
+	 * No need to use i_size_read() here, the i_size
+	 * cannot change under us because we hold i_mutex.
+	 *
+	 * But it's important to update i_size while still holding page lock:
+	 * page writeout could otherwise come in and zero beyond i_size.
+	 */
+	if (pos + copied > inode->i_size) {
+		i_size_write(inode, pos + copied);
+		i_size_changed = 1;
+	}
+
+	if (pos + copied >  EXT4_I(inode)->i_disksize) {
+		/* We need to mark inode dirty even if
+		 * new_i_size is less that inode->i_size
+		 * bu greater than i_disksize.(hint delalloc)
+		 */
+		ext4_update_i_disksize(inode, (pos + copied));
+		i_size_changed = 1;
+	}
+	unlock_page(page);
+	page_cache_release(page);
+
+	/*
+	 * Don't mark the inode dirty under page lock. First, it unnecessarily
+	 * makes the holding time of page lock longer. Second, it forces lock
+	 * ordering of page lock and transaction start for journaling
+	 * filesystems.
+	 */
+	if (i_size_changed)
+		ext4_mark_inode_dirty(handle, inode);
+
+	return copied;
+}
+
+/*
+ * We need to pick up the new inode size which generic_commit_write gave us
+ * `file' can be NULL - eg, when called from page_symlink().
+ *
+ * ext4 never places buffers on inode->i_mapping->private_list.  metadata
+ * buffers are managed internally.
+ */
+static int ext4_ordered_write_end(struct file *file,
+				  struct address_space *mapping,
+				  loff_t pos, unsigned len, unsigned copied,
+				  struct page *page, void *fsdata)
+{
+	handle_t *handle = ext4_journal_current_handle();
+	struct inode *inode = mapping->host;
+	int ret = 0, ret2;
+
+	trace_ext4_ordered_write_end(inode, pos, len, copied);
+	ret = ext4_jbd2_file_inode(handle, inode);
+
+	if (ret == 0) {
+		ret2 = ext4_generic_write_end(file, mapping, pos, len, copied,
+							page, fsdata);
+		copied = ret2;
+		if (pos + len > inode->i_size && ext4_can_truncate(inode))
+			/* if we have allocated more blocks and copied
+			 * less. We will have blocks allocated outside
+			 * inode->i_size. So truncate them
+			 */
+			ext4_orphan_add(handle, inode);
+		if (ret2 < 0)
+			ret = ret2;
+	} else {
+		unlock_page(page);
+		page_cache_release(page);
+	}
+
+	ret2 = ext4_journal_stop(handle);
+	if (!ret)
+		ret = ret2;
+
+	if (pos + len > inode->i_size) {
+		ext4_truncate_failed_write(inode);
+		/*
+		 * If truncate failed early the inode might still be
+		 * on the orphan list; we need to make sure the inode
+		 * is removed from the orphan list in that case.
+		 */
+		if (inode->i_nlink)
+			ext4_orphan_del(NULL, inode);
+	}
+
+
+	return ret ? ret : copied;
+}
+
+static int ext4_writeback_write_end(struct file *file,
+				    struct address_space *mapping,
+				    loff_t pos, unsigned len, unsigned copied,
+				    struct page *page, void *fsdata)
+{
+	handle_t *handle = ext4_journal_current_handle();
+	struct inode *inode = mapping->host;
+	int ret = 0, ret2;
+
+	trace_ext4_writeback_write_end(inode, pos, len, copied);
+	ret2 = ext4_generic_write_end(file, mapping, pos, len, copied,
+							page, fsdata);
+	copied = ret2;
+	if (pos + len > inode->i_size && ext4_can_truncate(inode))
+		/* if we have allocated more blocks and copied
+		 * less. We will have blocks allocated outside
+		 * inode->i_size. So truncate them
+		 */
+		ext4_orphan_add(handle, inode);
+
+	if (ret2 < 0)
+		ret = ret2;
+
+	ret2 = ext4_journal_stop(handle);
+	if (!ret)
+		ret = ret2;
+
+	if (pos + len > inode->i_size) {
+		ext4_truncate_failed_write(inode);
+		/*
+		 * If truncate failed early the inode might still be
+		 * on the orphan list; we need to make sure the inode
+		 * is removed from the orphan list in that case.
+		 */
+		if (inode->i_nlink)
+			ext4_orphan_del(NULL, inode);
+	}
+
+	return ret ? ret : copied;
+}
+
+static int ext4_journalled_write_end(struct file *file,
+				     struct address_space *mapping,
+				     loff_t pos, unsigned len, unsigned copied,
+				     struct page *page, void *fsdata)
+{
+	handle_t *handle = ext4_journal_current_handle();
+	struct inode *inode = mapping->host;
+	int ret = 0, ret2;
+	int partial = 0;
+	unsigned from, to;
+	loff_t new_i_size;
+
+	trace_ext4_journalled_write_end(inode, pos, len, copied);
+	from = pos & (PAGE_CACHE_SIZE - 1);
+	to = from + len;
+
+	BUG_ON(!ext4_handle_valid(handle));
+
+	if (copied < len) {
+		if (!PageUptodate(page))
+			copied = 0;
+		page_zero_new_buffers(page, from+copied, to);
+	}
+
+	ret = walk_page_buffers(handle, page_buffers(page), from,
+				to, &partial, write_end_fn);
+	if (!partial)
+		SetPageUptodate(page);
+	new_i_size = pos + copied;
+	if (new_i_size > inode->i_size)
+		i_size_write(inode, pos+copied);
+	ext4_set_inode_state(inode, EXT4_STATE_JDATA);
+	EXT4_I(inode)->i_datasync_tid = handle->h_transaction->t_tid;
+	if (new_i_size > EXT4_I(inode)->i_disksize) {
+		ext4_update_i_disksize(inode, new_i_size);
+		ret2 = ext4_mark_inode_dirty(handle, inode);
+		if (!ret)
+			ret = ret2;
+	}
+
+	unlock_page(page);
+	page_cache_release(page);
+	if (pos + len > inode->i_size && ext4_can_truncate(inode))
+		/* if we have allocated more blocks and copied
+		 * less. We will have blocks allocated outside
+		 * inode->i_size. So truncate them
+		 */
+		ext4_orphan_add(handle, inode);
+
+	ret2 = ext4_journal_stop(handle);
+	if (!ret)
+		ret = ret2;
+	if (pos + len > inode->i_size) {
+		ext4_truncate_failed_write(inode);
+		/*
+		 * If truncate failed early the inode might still be
+		 * on the orphan list; we need to make sure the inode
+		 * is removed from the orphan list in that case.
+		 */
+		if (inode->i_nlink)
+			ext4_orphan_del(NULL, inode);
+	}
+
+	return ret ? ret : copied;
+}
+
+/*
+ * Reserve a single cluster located at lblock
+ */
+static int ext4_da_reserve_space(struct inode *inode, ext4_lblk_t lblock)
+{
+	int retries = 0;
+	struct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);
+	struct ext4_inode_info *ei = EXT4_I(inode);
+	unsigned int md_needed;
+	int ret;
+	ext4_lblk_t save_last_lblock;
+	int save_len;
+
+	/*
+	 * We will charge metadata quota at writeout time; this saves
+	 * us from metadata over-estimation, though we may go over by
+	 * a small amount in the end.  Here we just reserve for data.
+	 */
+	ret = dquot_reserve_block(inode, EXT4_C2B(sbi, 1));
+	if (ret)
+		return ret;
+
+	/*
+	 * recalculate the amount of metadata blocks to reserve
+	 * in order to allocate nrblocks
+	 * worse case is one extent per block
+	 */
+repeat:
+	spin_lock(&ei->i_block_reservation_lock);
+	/*
+	 * ext4_calc_metadata_amount() has side effects, which we have
+	 * to be prepared undo if we fail to claim space.
+	 */
+	save_len = ei->i_da_metadata_calc_len;
+	save_last_lblock = ei->i_da_metadata_calc_last_lblock;
+	md_needed = EXT4_NUM_B2C(sbi,
+				 ext4_calc_metadata_amount(inode, lblock));
+	trace_ext4_da_reserve_space(inode, md_needed);
+
+	/*
+	 * We do still charge estimated metadata to the sb though;
+	 * we cannot afford to run out of free blocks.
+	 */
+	if (ext4_claim_free_clusters(sbi, md_needed + 1, 0)) {
+		ei->i_da_metadata_calc_len = save_len;
+		ei->i_da_metadata_calc_last_lblock = save_last_lblock;
+		spin_unlock(&ei->i_block_reservation_lock);
+		if (ext4_should_retry_alloc(inode->i_sb, &retries)) {
+			yield();
+			goto repeat;
+		}
+		dquot_release_reservation_block(inode, EXT4_C2B(sbi, 1));
+		return -ENOSPC;
+	}
+	ei->i_reserved_data_blocks++;
+	ei->i_reserved_meta_blocks += md_needed;
+	spin_unlock(&ei->i_block_reservation_lock);
+
+	return 0;       /* success */
+}
+
+static void ext4_da_release_space(struct inode *inode, int to_free)
+{
+	struct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);
+	struct ext4_inode_info *ei = EXT4_I(inode);
+
+	if (!to_free)
+		return;		/* Nothing to release, exit */
+
+	spin_lock(&EXT4_I(inode)->i_block_reservation_lock);
+
+	trace_ext4_da_release_space(inode, to_free);
+	if (unlikely(to_free > ei->i_reserved_data_blocks)) {
+		/*
+		 * if there aren't enough reserved blocks, then the
+		 * counter is messed up somewhere.  Since this
+		 * function is called from invalidate page, it's
+		 * harmless to return without any action.
+		 */
+		ext4_msg(inode->i_sb, KERN_NOTICE, "ext4_da_release_space: "
+			 "ino %lu, to_free %d with only %d reserved "
+			 "data blocks\n", inode->i_ino, to_free,
+			 ei->i_reserved_data_blocks);
+		WARN_ON(1);
+		to_free = ei->i_reserved_data_blocks;
+	}
+	ei->i_reserved_data_blocks -= to_free;
+
+	if (ei->i_reserved_data_blocks == 0) {
+		/*
+		 * We can release all of the reserved metadata blocks
+		 * only when we have written all of the delayed
+		 * allocation blocks.
+		 * Note that in case of bigalloc, i_reserved_meta_blocks,
+		 * i_reserved_data_blocks, etc. refer to number of clusters.
+		 */
+		percpu_counter_sub(&sbi->s_dirtyclusters_counter,
+				   ei->i_reserved_meta_blocks);
+		ei->i_reserved_meta_blocks = 0;
+		ei->i_da_metadata_calc_len = 0;
+	}
+
+	/* update fs dirty data blocks counter */
+	percpu_counter_sub(&sbi->s_dirtyclusters_counter, to_free);
+
+	spin_unlock(&EXT4_I(inode)->i_block_reservation_lock);
+
+	dquot_release_reservation_block(inode, EXT4_C2B(sbi, to_free));
+}
+
+static void ext4_da_page_release_reservation(struct page *page,
+					     unsigned long offset)
+{
+	int to_release = 0;
+	struct buffer_head *head, *bh;
+	unsigned int curr_off = 0;
+	struct inode *inode = page->mapping->host;
+	struct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);
+	int num_clusters;
+
+	head = page_buffers(page);
+	bh = head;
+	do {
+		unsigned int next_off = curr_off + bh->b_size;
+
+		if ((offset <= curr_off) && (buffer_delay(bh))) {
+			to_release++;
+			clear_buffer_delay(bh);
+			clear_buffer_da_mapped(bh);
+		}
+		curr_off = next_off;
+	} while ((bh = bh->b_this_page) != head);
+
+	/* If we have released all the blocks belonging to a cluster, then we
+	 * need to release the reserved space for that cluster. */
+	num_clusters = EXT4_NUM_B2C(sbi, to_release);
+	while (num_clusters > 0) {
+		ext4_fsblk_t lblk;
+		lblk = (page->index << (PAGE_CACHE_SHIFT - inode->i_blkbits)) +
+			((num_clusters - 1) << sbi->s_cluster_bits);
+		if (sbi->s_cluster_ratio == 1 ||
+		    !ext4_find_delalloc_cluster(inode, lblk, 1))
+			ext4_da_release_space(inode, 1);
+
+		num_clusters--;
+	}
+}
+
+/*
+ * Delayed allocation stuff
+ */
+
+/*
+ * mpage_da_submit_io - walks through extent of pages and try to write
+ * them with writepage() call back
+ *
+ * @mpd->inode: inode
+ * @mpd->first_page: first page of the extent
+ * @mpd->next_page: page after the last page of the extent
+ *
+ * By the time mpage_da_submit_io() is called we expect all blocks
+ * to be allocated. this may be wrong if allocation failed.
+ *
+ * As pages are already locked by write_cache_pages(), we can't use it
+ */
+static int mpage_da_submit_io(struct mpage_da_data *mpd,
+			      struct ext4_map_blocks *map)
+{
+	struct pagevec pvec;
+	unsigned long index, end;
+	int ret = 0, err, nr_pages, i;
+	struct inode *inode = mpd->inode;
+	struct address_space *mapping = inode->i_mapping;
+	loff_t size = i_size_read(inode);
+	unsigned int len, block_start;
+	struct buffer_head *bh, *page_bufs = NULL;
+	int journal_data = ext4_should_journal_data(inode);
+	sector_t pblock = 0, cur_logical = 0;
+	struct ext4_io_submit io_submit;
+
+	BUG_ON(mpd->next_page <= mpd->first_page);
+	memset(&io_submit, 0, sizeof(io_submit));
+	/*
+	 * We need to start from the first_page to the next_page - 1
+	 * to make sure we also write the mapped dirty buffer_heads.
+	 * If we look at mpd->b_blocknr we would only be looking
+	 * at the currently mapped buffer_heads.
+	 */
+	index = mpd->first_page;
+	end = mpd->next_page - 1;
+
+	pagevec_init(&pvec, 0);
+	while (index <= end) {
+		nr_pages = pagevec_lookup(&pvec, mapping, index, PAGEVEC_SIZE);
+		if (nr_pages == 0)
+			break;
+		for (i = 0; i < nr_pages; i++) {
+			int commit_write = 0, skip_page = 0;
+			struct page *page = pvec.pages[i];
+
+			index = page->index;
+			if (index > end)
+				break;
+
+			if (index == size >> PAGE_CACHE_SHIFT)
+				len = size & ~PAGE_CACHE_MASK;
+			else
+				len = PAGE_CACHE_SIZE;
+			if (map) {
+				cur_logical = index << (PAGE_CACHE_SHIFT -
+							inode->i_blkbits);
+				pblock = map->m_pblk + (cur_logical -
+							map->m_lblk);
+			}
+			index++;
+
+			BUG_ON(!PageLocked(page));
+			BUG_ON(PageWriteback(page));
+
+			/*
+			 * If the page does not have buffers (for
+			 * whatever reason), try to create them using
+			 * __block_write_begin.  If this fails,
+			 * skip the page and move on.
+			 */
+			if (!page_has_buffers(page)) {
+				if (__block_write_begin(page, 0, len,
+						noalloc_get_block_write)) {
+				skip_page:
+					unlock_page(page);
+					continue;
+				}
+				commit_write = 1;
+			}
+
+			bh = page_bufs = page_buffers(page);
+			block_start = 0;
+			do {
+				if (!bh)
+					goto skip_page;
+				if (map && (cur_logical >= map->m_lblk) &&
+				    (cur_logical <= (map->m_lblk +
+						     (map->m_len - 1)))) {
+					if (buffer_delay(bh)) {
+						clear_buffer_delay(bh);
+						bh->b_blocknr = pblock;
+					}
+					if (buffer_da_mapped(bh))
+						clear_buffer_da_mapped(bh);
+					if (buffer_unwritten(bh) ||
+					    buffer_mapped(bh))
+						BUG_ON(bh->b_blocknr != pblock);
+					if (map->m_flags & EXT4_MAP_UNINIT)
+						set_buffer_uninit(bh);
+					clear_buffer_unwritten(bh);
+				}
+
+				/*
+				 * skip page if block allocation undone and
+				 * block is dirty
+				 */
+				if (ext4_bh_delay_or_unwritten(NULL, bh))
+					skip_page = 1;
+				bh = bh->b_this_page;
+				block_start += bh->b_size;
+				cur_logical++;
+				pblock++;
+			} while (bh != page_bufs);
+
+			if (skip_page)
+				goto skip_page;
+
+			if (commit_write)
+				/* mark the buffer_heads as dirty & uptodate */
+				block_commit_write(page, 0, len);
+
+			clear_page_dirty_for_io(page);
+			/*
+			 * Delalloc doesn't support data journalling,
+			 * but eventually maybe we'll lift this
+			 * restriction.
+			 */
+			if (unlikely(journal_data && PageChecked(page)))
+				err = __ext4_journalled_writepage(page, len);
+			else if (test_opt(inode->i_sb, MBLK_IO_SUBMIT))
+				err = ext4_bio_write_page(&io_submit, page,
+							  len, mpd->wbc);
+			else if (buffer_uninit(page_bufs)) {
+				ext4_set_bh_endio(page_bufs, inode);
+				err = block_write_full_page_endio(page,
+					noalloc_get_block_write,
+					mpd->wbc, ext4_end_io_buffer_write);
+			} else
+				err = block_write_full_page(page,
+					noalloc_get_block_write, mpd->wbc);
+
+			if (!err)
+				mpd->pages_written++;
+			/*
+			 * In error case, we have to continue because
+			 * remaining pages are still locked
+			 */
+			if (ret == 0)
+				ret = err;
+		}
+		pagevec_release(&pvec);
+	}
+	ext4_io_submit(&io_submit);
+	return ret;
+}
+
+static void ext4_da_block_invalidatepages(struct mpage_da_data *mpd)
+{
+	int nr_pages, i;
+	pgoff_t index, end;
+	struct pagevec pvec;
+	struct inode *inode = mpd->inode;
+	struct address_space *mapping = inode->i_mapping;
+
+	index = mpd->first_page;
+	end   = mpd->next_page - 1;
+	pagevec_init(&pvec, 0);
+	while (index <= end) {
+		nr_pages = pagevec_lookup(&pvec, mapping, index, PAGEVEC_SIZE);
+		if (nr_pages == 0)
+			break;
+		for (i = 0; i < nr_pages; i++) {
+			struct page *page = pvec.pages[i];
+			if (page->index > end)
+				break;
+			BUG_ON(!PageLocked(page));
+			BUG_ON(PageWriteback(page));
+			block_invalidatepage(page, 0);
+			ClearPageUptodate(page);
+			unlock_page(page);
+		}
+		index = pvec.pages[nr_pages - 1]->index + 1;
+		pagevec_release(&pvec);
+	}
+	return;
+}
+
+static void ext4_print_free_blocks(struct inode *inode)
+{
+	struct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);
+	printk(KERN_CRIT "Total free blocks count %lld\n",
+	       EXT4_C2B(EXT4_SB(inode->i_sb),
+			ext4_count_free_clusters(inode->i_sb)));
+	printk(KERN_CRIT "Free/Dirty block details\n");
+	printk(KERN_CRIT "free_blocks=%lld\n",
+	       (long long) EXT4_C2B(EXT4_SB(inode->i_sb),
+		percpu_counter_sum(&sbi->s_freeclusters_counter)));
+	printk(KERN_CRIT "dirty_blocks=%lld\n",
+	       (long long) EXT4_C2B(EXT4_SB(inode->i_sb),
+		percpu_counter_sum(&sbi->s_dirtyclusters_counter)));
+	printk(KERN_CRIT "Block reservation details\n");
+	printk(KERN_CRIT "i_reserved_data_blocks=%u\n",
+	       EXT4_I(inode)->i_reserved_data_blocks);
+	printk(KERN_CRIT "i_reserved_meta_blocks=%u\n",
+	       EXT4_I(inode)->i_reserved_meta_blocks);
+	return;
+}
+
+/*
+ * mpage_da_map_and_submit - go through given space, map them
+ *       if necessary, and then submit them for I/O
+ *
+ * @mpd - bh describing space
+ *
+ * The function skips space we know is already mapped to disk blocks.
+ *
+ */
+static void __mpage_da_map_and_submit(struct mpage_da_data *mpd)
+{
+	int err, blks, get_blocks_flags;
+	struct ext4_map_blocks map, *mapp = NULL;
+	sector_t next = mpd->b_blocknr;
+	unsigned max_blocks = mpd->b_size >> mpd->inode->i_blkbits;
+	loff_t disksize = EXT4_I(mpd->inode)->i_disksize;
+	handle_t *handle = NULL;
+
+	/*
+	 * If the blocks are mapped already, or we couldn't accumulate
+	 * any blocks, then proceed immediately to the submission stage.
+	 */
+	if ((mpd->b_size == 0) ||
+	    ((mpd->b_state  & (1 << BH_Mapped)) &&
+	     !(mpd->b_state & (1 << BH_Delay)) &&
+	     !(mpd->b_state & (1 << BH_Unwritten))))
+		goto submit_io;
+
+	handle = ext4_journal_current_handle();
+	BUG_ON(!handle);
+
+	/*
+	 * Call ext4_map_blocks() to allocate any delayed allocation
+	 * blocks, or to convert an uninitialized extent to be
+	 * initialized (in the case where we have written into
+	 * one or more preallocated blocks).
+	 *
+	 * We pass in the magic EXT4_GET_BLOCKS_DELALLOC_RESERVE to
+	 * indicate that we are on the delayed allocation path.  This
+	 * affects functions in many different parts of the allocation
+	 * call path.  This flag exists primarily because we don't
+	 * want to change *many* call functions, so ext4_map_blocks()
+	 * will set the EXT4_STATE_DELALLOC_RESERVED flag once the
+	 * inode's allocation semaphore is taken.
+	 *
+	 * If the blocks in questions were delalloc blocks, set
+	 * EXT4_GET_BLOCKS_DELALLOC_RESERVE so the delalloc accounting
+	 * variables are updated after the blocks have been allocated.
+	 */
+	map.m_lblk = next;
+	map.m_len = max_blocks;
+	get_blocks_flags = EXT4_GET_BLOCKS_CREATE;
+	if (ext4_should_dioread_nolock(mpd->inode))
+		get_blocks_flags |= EXT4_GET_BLOCKS_IO_CREATE_EXT;
+	if (mpd->b_state & (1 << BH_Delay))
+		get_blocks_flags |= EXT4_GET_BLOCKS_DELALLOC_RESERVE;
+
+	blks = ext4_map_blocks(handle, mpd->inode, &map, get_blocks_flags);
+	if (blks < 0) {
+		struct super_block *sb = mpd->inode->i_sb;
+
+		err = blks;
+		/*
+		 * If get block returns EAGAIN or ENOSPC and there
+		 * appears to be free blocks we will just let
+		 * mpage_da_submit_io() unlock all of the pages.
+		 */
+		if (err == -EAGAIN)
+			goto submit_io;
+
+		if (err == -ENOSPC && ext4_count_free_clusters(sb)) {
+			mpd->retval = err;
+			goto submit_io;
+		}
+
+		/*
+		 * get block failure will cause us to loop in
+		 * writepages, because a_ops->writepage won't be able
+		 * to make progress. The page will be redirtied by
+		 * writepage and writepages will again try to write
+		 * the same.
+		 */
+		if (!(EXT4_SB(sb)->s_mount_flags & EXT4_MF_FS_ABORTED)) {
+			ext4_msg(sb, KERN_CRIT,
+				 "delayed block allocation failed for inode %lu "
+				 "at logical offset %llu with max blocks %zd "
+				 "with error %d", mpd->inode->i_ino,
+				 (unsigned long long) next,
+				 mpd->b_size >> mpd->inode->i_blkbits, err);
+			ext4_msg(sb, KERN_CRIT,
+				"This should not happen!! Data will be lost\n");
+			if (err == -ENOSPC)
+				ext4_print_free_blocks(mpd->inode);
+		}
+		/* invalidate all the pages */
+		ext4_da_block_invalidatepages(mpd);
+
+		/* Mark this page range as having been completed */
+		mpd->io_done = 1;
+		return;
+	}
+	BUG_ON(blks == 0);
+
+	mapp = &map;
+	if (map.m_flags & EXT4_MAP_NEW) {
+		struct block_device *bdev = mpd->inode->i_sb->s_bdev;
+		int i;
+
+		for (i = 0; i < map.m_len; i++)
+			unmap_underlying_metadata(bdev, map.m_pblk + i);
+
+		if (ext4_should_order_data(mpd->inode)) {
+			err = ext4_jbd2_file_inode(handle, mpd->inode);
+			if (err) {
+				/* Only if the journal is aborted */
+				mpd->retval = err;
+				goto submit_io;
+			}
+		}
+	}
+
+	/*
+	 * Update on-disk size along with block allocation.
+	 */
+	disksize = ((loff_t) next + blks) << mpd->inode->i_blkbits;
+	if (disksize > i_size_read(mpd->inode))
+		disksize = i_size_read(mpd->inode);
+	if (disksize > EXT4_I(mpd->inode)->i_disksize) {
+		ext4_update_i_disksize(mpd->inode, disksize);
+		err = ext4_mark_inode_dirty(handle, mpd->inode);
+		if (err)
+			ext4_error(mpd->inode->i_sb,
+				   "Failed to mark inode %lu dirty",
+				   mpd->inode->i_ino);
+	}
+
+submit_io:
+	mpage_da_submit_io(mpd, mapp);
+	mpd->io_done = 1;
+}
+
+static void mpage_da_map_and_submit(struct mpage_da_data *mpd)
+{
+	__mpage_da_map_and_submit(mpd);
+	// we are no longer acting as a proxy for dirty buffers
+	put_cause_list(current->causes);
+	current->causes = new_cause_list();
+}
+
+#define BH_FLAGS ((1 << BH_Uptodate) | (1 << BH_Mapped) | \
+		(1 << BH_Delay) | (1 << BH_Unwritten))
+
+/*
+ * mpage_add_bh_to_extent - try to add one more block to extent of blocks
+ *
+ * @mpd->lbh - extent of blocks
+ * @logical - logical number of the block in the file
+ * @bh - bh of the block (used to access block's state)
+ *
+ * the function is used to collect contig. blocks in same state
+ */
+static void mpage_add_bh_to_extent(struct mpage_da_data *mpd,
+				   sector_t logical, size_t b_size,
+				   unsigned long b_state)
+{
+	sector_t next;
+	int nrblocks = mpd->b_size >> mpd->inode->i_blkbits;
+
+	/*
+	 * XXX Don't go larger than mballoc is willing to allocate
+	 * This is a stopgap solution.  We eventually need to fold
+	 * mpage_da_submit_io() into this function and then call
+	 * ext4_map_blocks() multiple times in a loop
+	 */
+	if (nrblocks >= 8*1024*1024/mpd->inode->i_sb->s_blocksize)
+		goto flush_it;
+
+	/* check if thereserved journal credits might overflow */
+	if (!(ext4_test_inode_flag(mpd->inode, EXT4_INODE_EXTENTS))) {
+		if (nrblocks >= EXT4_MAX_TRANS_DATA) {
+			/*
+			 * With non-extent format we are limited by the journal
+			 * credit available.  Total credit needed to insert
+			 * nrblocks contiguous blocks is dependent on the
+			 * nrblocks.  So limit nrblocks.
+			 */
+			goto flush_it;
+		} else if ((nrblocks + (b_size >> mpd->inode->i_blkbits)) >
+				EXT4_MAX_TRANS_DATA) {
+			/*
+			 * Adding the new buffer_head would make it cross the
+			 * allowed limit for which we have journal credit
+			 * reserved. So limit the new bh->b_size
+			 */
+			b_size = (EXT4_MAX_TRANS_DATA - nrblocks) <<
+						mpd->inode->i_blkbits;
+			/* we will do mpage_da_submit_io in the next loop */
+		}
+	}
+	/*
+	 * First block in the extent
+	 */
+	if (mpd->b_size == 0) {
+		mpd->b_blocknr = logical;
+		mpd->b_size = b_size;
+		mpd->b_state = b_state & BH_FLAGS;
+		return;
+	}
+
+	next = mpd->b_blocknr + nrblocks;
+	/*
+	 * Can we merge the block to our big extent?
+	 */
+	if (logical == next && (b_state & BH_FLAGS) == mpd->b_state) {
+		mpd->b_size += b_size;
+		return;
+	}
+
+flush_it:
+	/*
+	 * We couldn't merge the block to our extent, so we
+	 * need to flush current  extent and start new one
+	 */
+	mpage_da_map_and_submit(mpd);
+	return;
+}
+
+static int ext4_bh_delay_or_unwritten(handle_t *handle, struct buffer_head *bh)
+{
+	return (buffer_delay(bh) || buffer_unwritten(bh)) && buffer_dirty(bh);
+}
+
+/*
+ * This function is grabs code from the very beginning of
+ * ext4_map_blocks, but assumes that the caller is from delayed write
+ * time. This function looks up the requested blocks and sets the
+ * buffer delay bit under the protection of i_data_sem.
+ */
+static int ext4_da_map_blocks(struct inode *inode, sector_t iblock,
+			      struct ext4_map_blocks *map,
+			      struct buffer_head *bh)
+{
+	int retval;
+	sector_t invalid_block = ~((sector_t) 0xffff);
+
+	if (invalid_block < ext4_blocks_count(EXT4_SB(inode->i_sb)->s_es))
+		invalid_block = ~0;
+
+	map->m_flags = 0;
+	ext_debug("ext4_da_map_blocks(): inode %lu, max_blocks %u,"
+		  "logical block %lu\n", inode->i_ino, map->m_len,
+		  (unsigned long) map->m_lblk);
+	/*
+	 * Try to see if we can get the block without requesting a new
+	 * file system block.
+	 */
+	down_read((&EXT4_I(inode)->i_data_sem));
+	if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))
+		retval = ext4_ext_map_blocks(NULL, inode, map, 0);
+	else
+		retval = ext4_ind_map_blocks(NULL, inode, map, 0);
+
+	if (retval == 0) {
+		/*
+		 * XXX: __block_prepare_write() unmaps passed block,
+		 * is it OK?
+		 */
+		/* If the block was allocated from previously allocated cluster,
+		 * then we dont need to reserve it again. */
+		if (!(map->m_flags & EXT4_MAP_FROM_CLUSTER)) {
+			retval = ext4_da_reserve_space(inode, iblock);
+			if (retval)
+				/* not enough space to reserve */
+				goto out_unlock;
+		}
+
+		/* Clear EXT4_MAP_FROM_CLUSTER flag since its purpose is served
+		 * and it should not appear on the bh->b_state.
+		 */
+		map->m_flags &= ~EXT4_MAP_FROM_CLUSTER;
+
+		map_bh(bh, inode->i_sb, invalid_block);
+		set_buffer_new(bh);
+		set_buffer_delay(bh);
+	}
+
+out_unlock:
+	up_read((&EXT4_I(inode)->i_data_sem));
+
+	return retval;
+}
+
+/*
+ * This is a special get_blocks_t callback which is used by
+ * ext4_da_write_begin().  It will either return mapped block or
+ * reserve space for a single block.
+ *
+ * For delayed buffer_head we have BH_Mapped, BH_New, BH_Delay set.
+ * We also have b_blocknr = -1 and b_bdev initialized properly
+ *
+ * For unwritten buffer_head we have BH_Mapped, BH_New, BH_Unwritten set.
+ * We also have b_blocknr = physicalblock mapping unwritten extent and b_bdev
+ * initialized properly.
+ */
+static int ext4_da_get_block_prep(struct inode *inode, sector_t iblock,
+				  struct buffer_head *bh, int create)
+{
+	struct ext4_map_blocks map;
+	int ret = 0;
+
+	BUG_ON(create == 0);
+	BUG_ON(bh->b_size != inode->i_sb->s_blocksize);
+
+	map.m_lblk = iblock;
+	map.m_len = 1;
+
+	/*
+	 * first, we need to know whether the block is allocated already
+	 * preallocated blocks are unmapped but should treated
+	 * the same as allocated blocks.
+	 */
+	ret = ext4_da_map_blocks(inode, iblock, &map, bh);
+	if (ret <= 0)
+		return ret;
+
+	map_bh(bh, inode->i_sb, map.m_pblk);
+	bh->b_state = (bh->b_state & ~EXT4_MAP_FLAGS) | map.m_flags;
+
+	if (buffer_unwritten(bh)) {
+		/* A delayed write to unwritten bh should be marked
+		 * new and mapped.  Mapped ensures that we don't do
+		 * get_block multiple times when we write to the same
+		 * offset and new ensures that we do proper zero out
+		 * for partial write.
+		 */
+		set_buffer_new(bh);
+		set_buffer_mapped(bh);
+	}
+	return 0;
+}
+
+/*
+ * This function is used as a standard get_block_t calback function
+ * when there is no desire to allocate any blocks.  It is used as a
+ * callback function for block_write_begin() and block_write_full_page().
+ * These functions should only try to map a single block at a time.
+ *
+ * Since this function doesn't do block allocations even if the caller
+ * requests it by passing in create=1, it is critically important that
+ * any caller checks to make sure that any buffer heads are returned
+ * by this function are either all already mapped or marked for
+ * delayed allocation before calling  block_write_full_page().  Otherwise,
+ * b_blocknr could be left unitialized, and the page write functions will
+ * be taken by surprise.
+ */
+static int noalloc_get_block_write(struct inode *inode, sector_t iblock,
+				   struct buffer_head *bh_result, int create)
+{
+	BUG_ON(bh_result->b_size != inode->i_sb->s_blocksize);
+	return _ext4_get_block(inode, iblock, bh_result, 0);
+}
+
+static int bget_one(handle_t *handle, struct buffer_head *bh)
+{
+	get_bh(bh);
+	return 0;
+}
+
+static int bput_one(handle_t *handle, struct buffer_head *bh)
+{
+	put_bh(bh);
+	return 0;
+}
+
+static int __ext4_journalled_writepage(struct page *page,
+				       unsigned int len)
+{
+	struct address_space *mapping = page->mapping;
+	struct inode *inode = mapping->host;
+	struct buffer_head *page_bufs;
+	handle_t *handle = NULL;
+	int ret = 0;
+	int err;
+
+	ClearPageChecked(page);
+	page_bufs = page_buffers(page);
+	BUG_ON(!page_bufs);
+	walk_page_buffers(handle, page_bufs, 0, len, NULL, bget_one);
+	/* As soon as we unlock the page, it can go away, but we have
+	 * references to buffers so we are safe */
+	unlock_page(page);
+
+	handle = ext4_journal_start(inode, ext4_writepage_trans_blocks(inode));
+	if (IS_ERR(handle)) {
+		ret = PTR_ERR(handle);
+		goto out;
+	}
+
+	BUG_ON(!ext4_handle_valid(handle));
+
+	ret = walk_page_buffers(handle, page_bufs, 0, len, NULL,
+				do_journal_get_write_access);
+
+	err = walk_page_buffers(handle, page_bufs, 0, len, NULL,
+				write_end_fn);
+	if (ret == 0)
+		ret = err;
+	EXT4_I(inode)->i_datasync_tid = handle->h_transaction->t_tid;
+	err = ext4_journal_stop(handle);
+	if (!ret)
+		ret = err;
+
+	walk_page_buffers(handle, page_bufs, 0, len, NULL, bput_one);
+	ext4_set_inode_state(inode, EXT4_STATE_JDATA);
+out:
+	return ret;
+}
+
+static int ext4_set_bh_endio(struct buffer_head *bh, struct inode *inode);
+static void ext4_end_io_buffer_write(struct buffer_head *bh, int uptodate);
+
+/*
+ * Note that we don't need to start a transaction unless we're journaling data
+ * because we should have holes filled from ext4_page_mkwrite(). We even don't
+ * need to file the inode to the transaction's list in ordered mode because if
+ * we are writing back data added by write(), the inode is already there and if
+ * we are writing back data modified via mmap(), no one guarantees in which
+ * transaction the data will hit the disk. In case we are journaling data, we
+ * cannot start transaction directly because transaction start ranks above page
+ * lock so we have to do some magic.
+ *
+ * This function can get called via...
+ *   - ext4_da_writepages after taking page lock (have journal handle)
+ *   - journal_submit_inode_data_buffers (no journal handle)
+ *   - shrink_page_list via pdflush (no journal handle)
+ *   - grab_page_cache when doing write_begin (have journal handle)
+ *
+ * We don't do any block allocation in this function. If we have page with
+ * multiple blocks we need to write those buffer_heads that are mapped. This
+ * is important for mmaped based write. So if we do with blocksize 1K
+ * truncate(f, 1024);
+ * a = mmap(f, 0, 4096);
+ * a[0] = 'a';
+ * truncate(f, 4096);
+ * we have in the page first buffer_head mapped via page_mkwrite call back
+ * but other bufer_heads would be unmapped but dirty(dirty done via the
+ * do_wp_page). So writepage should write the first block. If we modify
+ * the mmap area beyond 1024 we will again get a page_fault and the
+ * page_mkwrite callback will do the block allocation and mark the
+ * buffer_heads mapped.
+ *
+ * We redirty the page if we have any buffer_heads that is either delay or
+ * unwritten in the page.
+ *
+ * We can get recursively called as show below.
+ *
+ *	ext4_writepage() -> kmalloc() -> __alloc_pages() -> page_launder() ->
+ *		ext4_writepage()
+ *
+ * But since we don't do any block allocation we should not deadlock.
+ * Page also have the dirty flag cleared so we don't get recurive page_lock.
+ */
+static int ext4_writepage(struct page *page,
+			  struct writeback_control *wbc)
+{
+	int ret = 0, commit_write = 0;
+	loff_t size;
+	unsigned int len;
+	struct buffer_head *page_bufs = NULL;
+	struct inode *inode = page->mapping->host;
+
+	trace_ext4_writepage(page);
+	size = i_size_read(inode);
+	if (page->index == size >> PAGE_CACHE_SHIFT)
+		len = size & ~PAGE_CACHE_MASK;
+	else
+		len = PAGE_CACHE_SIZE;
+
+	/*
+	 * If the page does not have buffers (for whatever reason),
+	 * try to create them using __block_write_begin.  If this
+	 * fails, redirty the page and move on.
+	 */
+	if (!page_has_buffers(page)) {
+		if (__block_write_begin(page, 0, len,
+					noalloc_get_block_write)) {
+		redirty_page:
+			redirty_page_for_writepage(wbc, page);
+			unlock_page(page);
+			return 0;
+		}
+		commit_write = 1;
+	}
+	page_bufs = page_buffers(page);
+	if (walk_page_buffers(NULL, page_bufs, 0, len, NULL,
+			      ext4_bh_delay_or_unwritten)) {
+		/*
+		 * We don't want to do block allocation, so redirty
+		 * the page and return.  We may reach here when we do
+		 * a journal commit via journal_submit_inode_data_buffers.
+		 * We can also reach here via shrink_page_list but it
+		 * should never be for direct reclaim so warn if that
+		 * happens
+		 */
+		WARN_ON_ONCE((current->flags & (PF_MEMALLOC|PF_KSWAPD)) ==
+								PF_MEMALLOC);
+		goto redirty_page;
+	}
+	if (commit_write)
+		/* now mark the buffer_heads as dirty and uptodate */
+		block_commit_write(page, 0, len);
+
+	if (PageChecked(page) && ext4_should_journal_data(inode))
+		/*
+		 * It's mmapped pagecache.  Add buffers and journal it.  There
+		 * doesn't seem much point in redirtying the page here.
+		 */
+		return __ext4_journalled_writepage(page, len);
+
+	if (buffer_uninit(page_bufs)) {
+		ext4_set_bh_endio(page_bufs, inode);
+		ret = block_write_full_page_endio(page, noalloc_get_block_write,
+					    wbc, ext4_end_io_buffer_write);
+	} else
+		ret = block_write_full_page(page, noalloc_get_block_write,
+					    wbc);
+
+	return ret;
+}
+
+/*
+ * This is called via ext4_da_writepages() to
+ * calculate the total number of credits to reserve to fit
+ * a single extent allocation into a single transaction,
+ * ext4_da_writpeages() will loop calling this before
+ * the block allocation.
+ */
+
+static int ext4_da_writepages_trans_blocks(struct inode *inode)
+{
+	int max_blocks = EXT4_I(inode)->i_reserved_data_blocks;
+
+	/*
+	 * With non-extent format the journal credit needed to
+	 * insert nrblocks contiguous block is dependent on
+	 * number of contiguous block. So we will limit
+	 * number of contiguous block to a sane value
+	 */
+	if (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) &&
+	    (max_blocks > EXT4_MAX_TRANS_DATA))
+		max_blocks = EXT4_MAX_TRANS_DATA;
+
+	return ext4_chunk_trans_blocks(inode, max_blocks);
+}
+
+/*
+ * write_cache_pages_da - walk the list of dirty pages of the given
+ * address space and accumulate pages that need writing, and call
+ * mpage_da_map_and_submit to map a single contiguous memory region
+ * and then write them.
+ */
+static int write_cache_pages_da(struct address_space *mapping,
+				struct writeback_control *wbc,
+				struct mpage_da_data *mpd,
+				pgoff_t *done_index)
+{
+	struct buffer_head	*bh, *head;
+	struct inode		*inode = mapping->host;
+	struct pagevec		pvec;
+	unsigned int		nr_pages;
+	sector_t		logical;
+	pgoff_t			index, end;
+	long			nr_to_write = wbc->nr_to_write;
+	int			i, tag, ret = 0;
+
+	memset(mpd, 0, sizeof(struct mpage_da_data));
+	mpd->wbc = wbc;
+	mpd->inode = inode;
+	pagevec_init(&pvec, 0);
+	index = wbc->range_start >> PAGE_CACHE_SHIFT;
+	end = wbc->range_end >> PAGE_CACHE_SHIFT;
+
+	if (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)
+		tag = PAGECACHE_TAG_TOWRITE;
+	else
+		tag = PAGECACHE_TAG_DIRTY;
+
+	*done_index = index;
+	while (index <= end) {
+		nr_pages = pagevec_lookup_tag(&pvec, mapping, &index, tag,
+			      min(end - index, (pgoff_t)PAGEVEC_SIZE-1) + 1);
+		if (nr_pages == 0)
+			return 0;
+
+		for (i = 0; i < nr_pages; i++) {
+			struct page *page = pvec.pages[i];
+
+			/*
+			 * At this point, the page may be truncated or
+			 * invalidated (changing page->mapping to NULL), or
+			 * even swizzled back from swapper_space to tmpfs file
+			 * mapping. However, page->index will not change
+			 * because we have a reference on the page.
+			 */
+			if (page->index > end)
+				goto out;
+
+			*done_index = page->index + 1;
+
+			/*
+			 * If we can't merge this page, and we have
+			 * accumulated an contiguous region, write it
+			 */
+			if ((mpd->next_page != page->index) &&
+			    (mpd->next_page != mpd->first_page)) {
+				mpage_da_map_and_submit(mpd);
+				goto ret_extent_tail;
+			}
+
+			lock_page(page);
+
+			/*
+			 * If the page is no longer dirty, or its
+			 * mapping no longer corresponds to inode we
+			 * are writing (which means it has been
+			 * truncated or invalidated), or the page is
+			 * already under writeback and we are not
+			 * doing a data integrity writeback, skip the page
+			 */
+			if (!PageDirty(page) ||
+			    (PageWriteback(page) &&
+			     (wbc->sync_mode == WB_SYNC_NONE)) ||
+			    unlikely(page->mapping != mapping)) {
+				unlock_page(page);
+				continue;
+			}
+
+			wait_on_page_writeback(page);
+			BUG_ON(PageWriteback(page));
+
+			if (mpd->next_page != page->index)
+				mpd->first_page = page->index;
+			mpd->next_page = page->index + 1;
+			logical = (sector_t) page->index <<
+				(PAGE_CACHE_SHIFT - inode->i_blkbits);
+
+			if (!page_has_buffers(page)) {
+				mpage_add_bh_to_extent(mpd, logical,
+						PAGE_CACHE_SIZE,
+						(1 << BH_Dirty) | (1 << BH_Uptodate));
+
+				if (mpd->io_done)
+					goto ret_extent_tail;
+			} else {
+				/*
+				 * Page with regular buffer heads,
+				 * just add all dirty ones
+				 */
+				head = page_buffers(page);
+				bh = head;
+				do {
+					BUG_ON(buffer_locked(bh));
+					/*
+					 * We need to try to allocate
+					 * unmapped blocks in the same page.
+					 * Otherwise we won't make progress
+					 * with the page in ext4_writepage
+					 */
+					if (ext4_bh_delay_or_unwritten(NULL, bh)) {
+						// are we a proxy?
+						if (current->causes)
+							cause_list_copy(bh->causes, &current->causes);
+
+						mpage_add_bh_to_extent(mpd, logical,
+								       bh->b_size,
+								       bh->b_state);
+						if (mpd->io_done)
+							goto ret_extent_tail;
+					} else if (buffer_dirty(bh) && (buffer_mapped(bh))) {
+						/*
+						 * mapped dirty buffer. We need
+						 * to update the b_state
+						 * because we look at b_state
+						 * in mpage_da_map_blocks.  We
+						 * don't update b_size because
+						 * if we find an unmapped
+						 * buffer_head later we need to
+						 * use the b_state flag of that
+						 * buffer_head.
+						 */
+						if (mpd->b_size == 0)
+							mpd->b_state = bh->b_state & BH_FLAGS;
+					}
+					logical++;
+				} while ((bh = bh->b_this_page) != head);
+			}
+
+			if (nr_to_write > 0) {
+				nr_to_write--;
+				if (nr_to_write == 0 &&
+				    wbc->sync_mode == WB_SYNC_NONE)
+					/*
+					 * We stop writing back only if we are
+					 * not doing integrity sync. In case of
+					 * integrity sync we have to keep going
+					 * because someone may be concurrently
+					 * dirtying pages, and we might have
+					 * synced a lot of newly appeared dirty
+					 * pages, but have not synced all of the
+					 * old dirty pages.
+					 */
+					goto out;
+			}
+		}
+		pagevec_release(&pvec);
+		cond_resched();
+	}
+	return 0;
+ret_extent_tail:
+	ret = MPAGE_DA_EXTENT_TAIL;
+out:
+	pagevec_release(&pvec);
+	cond_resched();
+	return ret;
+}
+
+
+static int ext4_da_writepages(struct address_space *mapping,
+			      struct writeback_control *wbc)
+{
+	pgoff_t	index;
+	int range_whole = 0;
+	handle_t *handle = NULL;
+	struct mpage_da_data mpd;
+	struct inode *inode = mapping->host;
+	int pages_written = 0;
+	unsigned int max_pages;
+	int range_cyclic, cycled = 1, io_done = 0;
+	int needed_blocks, ret = 0;
+	long desired_nr_to_write, nr_to_writebump = 0;
+	loff_t range_start = wbc->range_start;
+	struct ext4_sb_info *sbi = EXT4_SB(mapping->host->i_sb);
+	pgoff_t done_index = 0;
+	pgoff_t end;
+	struct blk_plug plug;
+
+	trace_ext4_da_writepages(inode, wbc);
+
+	/*
+	 * No pages to write? This is mainly a kludge to avoid starting
+	 * a transaction for special inodes like journal inode on last iput()
+	 * because that could violate lock ordering on umount
+	 */
+	if (!mapping->nrpages || !mapping_tagged(mapping, PAGECACHE_TAG_DIRTY))
+		return 0;
+
+	/*
+	 * If the filesystem has aborted, it is read-only, so return
+	 * right away instead of dumping stack traces later on that
+	 * will obscure the real source of the problem.  We test
+	 * EXT4_MF_FS_ABORTED instead of sb->s_flag's MS_RDONLY because
+	 * the latter could be true if the filesystem is mounted
+	 * read-only, and in that case, ext4_da_writepages should
+	 * *never* be called, so if that ever happens, we would want
+	 * the stack trace.
+	 */
+	if (unlikely(sbi->s_mount_flags & EXT4_MF_FS_ABORTED))
+		return -EROFS;
+
+	// now acting as cause proxy
+	BUG_ON(current->causes);
+	current->causes = new_cause_list();
+
+	if (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)
+		range_whole = 1;
+
+	range_cyclic = wbc->range_cyclic;
+	if (wbc->range_cyclic) {
+		index = mapping->writeback_index;
+		if (index)
+			cycled = 0;
+		wbc->range_start = index << PAGE_CACHE_SHIFT;
+		wbc->range_end  = LLONG_MAX;
+		wbc->range_cyclic = 0;
+		end = -1;
+	} else {
+		index = wbc->range_start >> PAGE_CACHE_SHIFT;
+		end = wbc->range_end >> PAGE_CACHE_SHIFT;
+	}
+
+	/*
+	 * This works around two forms of stupidity.  The first is in
+	 * the writeback code, which caps the maximum number of pages
+	 * written to be 1024 pages.  This is wrong on multiple
+	 * levels; different architectues have a different page size,
+	 * which changes the maximum amount of data which gets
+	 * written.  Secondly, 4 megabytes is way too small.  XFS
+	 * forces this value to be 16 megabytes by multiplying
+	 * nr_to_write parameter by four, and then relies on its
+	 * allocator to allocate larger extents to make them
+	 * contiguous.  Unfortunately this brings us to the second
+	 * stupidity, which is that ext4's mballoc code only allocates
+	 * at most 2048 blocks.  So we force contiguous writes up to
+	 * the number of dirty blocks in the inode, or
+	 * sbi->max_writeback_mb_bump whichever is smaller.
+	 */
+	max_pages = sbi->s_max_writeback_mb_bump << (20 - PAGE_CACHE_SHIFT);
+	if (!range_cyclic && range_whole) {
+		if (wbc->nr_to_write == LONG_MAX)
+			desired_nr_to_write = wbc->nr_to_write;
+		else
+			desired_nr_to_write = wbc->nr_to_write * 8;
+	} else
+		desired_nr_to_write = ext4_num_dirty_pages(inode, index,
+							   max_pages);
+	if (desired_nr_to_write > max_pages)
+		desired_nr_to_write = max_pages;
+
+	if (wbc->nr_to_write < desired_nr_to_write) {
+		nr_to_writebump = desired_nr_to_write - wbc->nr_to_write;
+		wbc->nr_to_write = desired_nr_to_write;
+	}
+
+retry:
+	if (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)
+		tag_pages_for_writeback(mapping, index, end);
+
+	blk_start_plug(&plug);
+	while (!ret && wbc->nr_to_write > 0) {
+
+		/*
+		 * we  insert one extent at a time. So we need
+		 * credit needed for single extent allocation.
+		 * journalled mode is currently not supported
+		 * by delalloc
+		 */
+		BUG_ON(ext4_should_journal_data(inode));
+		needed_blocks = ext4_da_writepages_trans_blocks(inode);
+
+		/* start a new transaction*/
+		handle = ext4_journal_start(inode, needed_blocks);
+		if (IS_ERR(handle)) {
+			ret = PTR_ERR(handle);
+			ext4_msg(inode->i_sb, KERN_CRIT, "%s: jbd2_start: "
+			       "%ld pages, ino %lu; err %d", __func__,
+				wbc->nr_to_write, inode->i_ino, ret);
+			blk_finish_plug(&plug);
+			goto out_writepages;
+		}
+
+		/*
+		 * Now call write_cache_pages_da() to find the next
+		 * contiguous region of logical blocks that need
+		 * blocks to be allocated by ext4 and submit them.
+		 */
+		ret = write_cache_pages_da(mapping, wbc, &mpd, &done_index);
+		/*
+		 * If we have a contiguous extent of pages and we
+		 * haven't done the I/O yet, map the blocks and submit
+		 * them for I/O.
+		 */
+
+		if (!mpd.io_done && mpd.next_page != mpd.first_page) {
+			mpage_da_map_and_submit(&mpd);
+			ret = MPAGE_DA_EXTENT_TAIL;
+		}
+
+		trace_ext4_da_write_pages(inode, &mpd);
+		wbc->nr_to_write -= mpd.pages_written;
+
+		ext4_journal_stop(handle);
+
+		if ((mpd.retval == -ENOSPC) && sbi->s_journal) {
+			/* commit the transaction which would
+			 * free blocks released in the transaction
+			 * and try again
+			 */
+			jbd2_journal_force_commit_nested(sbi->s_journal);
+			ret = 0;
+		} else if (ret == MPAGE_DA_EXTENT_TAIL) {
+			/*
+			 * Got one extent now try with rest of the pages.
+			 * If mpd.retval is set -EIO, journal is aborted.
+			 * So we don't need to write any more.
+			 */
+			pages_written += mpd.pages_written;
+			ret = mpd.retval;
+			io_done = 1;
+		} else if (wbc->nr_to_write)
+			/*
+			 * There is no more writeout needed
+			 * or we requested for a noblocking writeout
+			 * and we found the device congested
+			 */
+			break;
+	}
+	blk_finish_plug(&plug);
+	if (!io_done && !cycled) {
+		cycled = 1;
+		index = 0;
+		wbc->range_start = index << PAGE_CACHE_SHIFT;
+		wbc->range_end  = mapping->writeback_index - 1;
+		goto retry;
+	}
+
+	/* Update index */
+	wbc->range_cyclic = range_cyclic;
+	if (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0))
+		/*
+		 * set the writeback_index so that range_cyclic
+		 * mode will write it back later
+		 */
+		mapping->writeback_index = done_index;
+
+out_writepages:
+	// done acting as cause proxy
+	put_cause_list(current->causes);
+	current->causes = NULL;
+
+	wbc->nr_to_write -= nr_to_writebump;
+	wbc->range_start = range_start;
+	trace_ext4_da_writepages_result(inode, wbc, ret, pages_written);
+	return ret;
+}
+
+#define FALL_BACK_TO_NONDELALLOC 1
+static int ext4_nonda_switch(struct super_block *sb)
+{
+	s64 free_blocks, dirty_blocks;
+	struct ext4_sb_info *sbi = EXT4_SB(sb);
+
+	/*
+	 * switch to non delalloc mode if we are running low
+	 * on free block. The free block accounting via percpu
+	 * counters can get slightly wrong with percpu_counter_batch getting
+	 * accumulated on each CPU without updating global counters
+	 * Delalloc need an accurate free block accounting. So switch
+	 * to non delalloc when we are near to error range.
+	 */
+	free_blocks  = EXT4_C2B(sbi,
+		percpu_counter_read_positive(&sbi->s_freeclusters_counter));
+	dirty_blocks = percpu_counter_read_positive(&sbi->s_dirtyclusters_counter);
+	/*
+	 * Start pushing delalloc when 1/2 of free blocks are dirty.
+	 */
+	if (dirty_blocks && (free_blocks < 2 * dirty_blocks) &&
+	    !writeback_in_progress(sb->s_bdi) &&
+	    down_read_trylock(&sb->s_umount)) {
+		writeback_inodes_sb(sb, WB_REASON_FS_FREE_SPACE);
+		up_read(&sb->s_umount);
+	}
+
+	if (2 * free_blocks < 3 * dirty_blocks ||
+		free_blocks < (dirty_blocks + EXT4_FREECLUSTERS_WATERMARK)) {
+		/*
+		 * free block count is less than 150% of dirty blocks
+		 * or free blocks is less than watermark
+		 */
+		return 1;
+	}
+	return 0;
+}
+
+static int ext4_da_write_begin(struct file *file, struct address_space *mapping,
+			       loff_t pos, unsigned len, unsigned flags,
+			       struct page **pagep, void **fsdata)
+{
+	int ret, retries = 0;
+	struct page *page;
+	pgoff_t index;
+	struct inode *inode = mapping->host;
+	handle_t *handle;
+
+	index = pos >> PAGE_CACHE_SHIFT;
+
+	if (ext4_nonda_switch(inode->i_sb)) {
+		*fsdata = (void *)FALL_BACK_TO_NONDELALLOC;
+		return ext4_write_begin(file, mapping, pos,
+					len, flags, pagep, fsdata);
+	}
+	*fsdata = (void *)0;
+	trace_ext4_da_write_begin(inode, pos, len, flags);
+retry:
+	/*
+	 * With delayed allocation, we don't log the i_disksize update
+	 * if there is delayed block allocation. But we still need
+	 * to journalling the i_disksize update if writes to the end
+	 * of file which has an already mapped buffer.
+	 */
+	handle = ext4_journal_start(inode, 1);
+	if (IS_ERR(handle)) {
+		ret = PTR_ERR(handle);
+		goto out;
+	}
+	/* We cannot recurse into the filesystem as the transaction is already
+	 * started */
+	flags |= AOP_FLAG_NOFS;
+
+	page = grab_cache_page_write_begin(mapping, index, flags);
+	if (!page) {
+		ext4_journal_stop(handle);
+		ret = -ENOMEM;
+		goto out;
+	}
+	*pagep = page;
+
+	ret = __block_write_begin(page, pos, len, ext4_da_get_block_prep);
+	if (ret < 0) {
+		unlock_page(page);
+		ext4_journal_stop(handle);
+		page_cache_release(page);
+		/*
+		 * block_write_begin may have instantiated a few blocks
+		 * outside i_size.  Trim these off again. Don't need
+		 * i_size_read because we hold i_mutex.
+		 */
+		if (pos + len > inode->i_size)
+			ext4_truncate_failed_write(inode);
+	}
+
+	if (ret == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries))
+		goto retry;
+out:
+	return ret;
+}
+
+/*
+ * Check if we should update i_disksize
+ * when write to the end of file but not require block allocation
+ */
+static int ext4_da_should_update_i_disksize(struct page *page,
+					    unsigned long offset)
+{
+	struct buffer_head *bh;
+	struct inode *inode = page->mapping->host;
+	unsigned int idx;
+	int i;
+
+	bh = page_buffers(page);
+	idx = offset >> inode->i_blkbits;
+
+	for (i = 0; i < idx; i++)
+		bh = bh->b_this_page;
+
+	if (!buffer_mapped(bh) || (buffer_delay(bh)) || buffer_unwritten(bh))
+		return 0;
+	return 1;
+}
+
+static int ext4_da_write_end(struct file *file,
+			     struct address_space *mapping,
+			     loff_t pos, unsigned len, unsigned copied,
+			     struct page *page, void *fsdata)
+{
+	struct inode *inode = mapping->host;
+	int ret = 0, ret2;
+	handle_t *handle = ext4_journal_current_handle();
+	loff_t new_i_size;
+	unsigned long start, end;
+	int write_mode = (int)(unsigned long)fsdata;
+
+	if (write_mode == FALL_BACK_TO_NONDELALLOC) {
+		switch (ext4_inode_journal_mode(inode)) {
+		case EXT4_INODE_ORDERED_DATA_MODE:
+			return ext4_ordered_write_end(file, mapping, pos,
+					len, copied, page, fsdata);
+		case EXT4_INODE_WRITEBACK_DATA_MODE:
+			return ext4_writeback_write_end(file, mapping, pos,
+					len, copied, page, fsdata);
+		default:
+			BUG();
+		}
+	}
+
+	trace_ext4_da_write_end(inode, pos, len, copied);
+	start = pos & (PAGE_CACHE_SIZE - 1);
+	end = start + copied - 1;
+
+	/*
+	 * generic_write_end() will run mark_inode_dirty() if i_size
+	 * changes.  So let's piggyback the i_disksize mark_inode_dirty
+	 * into that.
+	 */
+
+	new_i_size = pos + copied;
+	if (copied && new_i_size > EXT4_I(inode)->i_disksize) {
+		if (ext4_da_should_update_i_disksize(page, end)) {
+			down_write(&EXT4_I(inode)->i_data_sem);
+			if (new_i_size > EXT4_I(inode)->i_disksize) {
+				/*
+				 * Updating i_disksize when extending file
+				 * without needing block allocation
+				 */
+				if (ext4_should_order_data(inode))
+					ret = ext4_jbd2_file_inode(handle,
+								   inode);
+
+				EXT4_I(inode)->i_disksize = new_i_size;
+			}
+			up_write(&EXT4_I(inode)->i_data_sem);
+			/* We need to mark inode dirty even if
+			 * new_i_size is less that inode->i_size
+			 * bu greater than i_disksize.(hint delalloc)
+			 */
+			ext4_mark_inode_dirty(handle, inode);
+		}
+	}
+	ret2 = generic_write_end(file, mapping, pos, len, copied,
+							page, fsdata);
+	copied = ret2;
+	if (ret2 < 0)
+		ret = ret2;
+	ret2 = ext4_journal_stop(handle);
+	if (!ret)
+		ret = ret2;
+
+	return ret ? ret : copied;
+}
+
+static void ext4_da_invalidatepage(struct page *page, unsigned long offset)
+{
+	/*
+	 * Drop reserved blocks
+	 */
+	BUG_ON(!PageLocked(page));
+	if (!page_has_buffers(page))
+		goto out;
+
+	ext4_da_page_release_reservation(page, offset);
+
+out:
+	ext4_invalidatepage(page, offset);
+
+	return;
+}
+
+/*
+ * Force all delayed allocation blocks to be allocated for a given inode.
+ */
+int ext4_alloc_da_blocks(struct inode *inode)
+{
+	trace_ext4_alloc_da_blocks(inode);
+
+	if (!EXT4_I(inode)->i_reserved_data_blocks &&
+	    !EXT4_I(inode)->i_reserved_meta_blocks)
+		return 0;
+
+	/*
+	 * We do something simple for now.  The filemap_flush() will
+	 * also start triggering a write of the data blocks, which is
+	 * not strictly speaking necessary (and for users of
+	 * laptop_mode, not even desirable).  However, to do otherwise
+	 * would require replicating code paths in:
+	 *
+	 * ext4_da_writepages() ->
+	 *    write_cache_pages() ---> (via passed in callback function)
+	 *        __mpage_da_writepage() -->
+	 *           mpage_add_bh_to_extent()
+	 *           mpage_da_map_blocks()
+	 *
+	 * The problem is that write_cache_pages(), located in
+	 * mm/page-writeback.c, marks pages clean in preparation for
+	 * doing I/O, which is not desirable if we're not planning on
+	 * doing I/O at all.
+	 *
+	 * We could call write_cache_pages(), and then redirty all of
+	 * the pages by calling redirty_page_for_writepage() but that
+	 * would be ugly in the extreme.  So instead we would need to
+	 * replicate parts of the code in the above functions,
+	 * simplifying them because we wouldn't actually intend to
+	 * write out the pages, but rather only collect contiguous
+	 * logical block extents, call the multi-block allocator, and
+	 * then update the buffer heads with the block allocations.
+	 *
+	 * For now, though, we'll cheat by calling filemap_flush(),
+	 * which will map the blocks, and start the I/O, but not
+	 * actually wait for the I/O to complete.
+	 */
+	return filemap_flush(inode->i_mapping);
+}
+
+/*
+ * bmap() is special.  It gets used by applications such as lilo and by
+ * the swapper to find the on-disk block of a specific piece of data.
+ *
+ * Naturally, this is dangerous if the block concerned is still in the
+ * journal.  If somebody makes a swapfile on an ext4 data-journaling
+ * filesystem and enables swap, then they may get a nasty shock when the
+ * data getting swapped to that swapfile suddenly gets overwritten by
+ * the original zero's written out previously to the journal and
+ * awaiting writeback in the kernel's buffer cache.
+ *
+ * So, if we see any bmap calls here on a modified, data-journaled file,
+ * take extra steps to flush any blocks which might be in the cache.
+ */
+static sector_t ext4_bmap(struct address_space *mapping, sector_t block)
+{
+	struct inode *inode = mapping->host;
+	journal_t *journal;
+	int err;
+
+	if (mapping_tagged(mapping, PAGECACHE_TAG_DIRTY) &&
+			test_opt(inode->i_sb, DELALLOC)) {
+		/*
+		 * With delalloc we want to sync the file
+		 * so that we can make sure we allocate
+		 * blocks for file
+		 */
+		filemap_write_and_wait(mapping);
+	}
+
+	if (EXT4_JOURNAL(inode) &&
+	    ext4_test_inode_state(inode, EXT4_STATE_JDATA)) {
+		/*
+		 * This is a REALLY heavyweight approach, but the use of
+		 * bmap on dirty files is expected to be extremely rare:
+		 * only if we run lilo or swapon on a freshly made file
+		 * do we expect this to happen.
+		 *
+		 * (bmap requires CAP_SYS_RAWIO so this does not
+		 * represent an unprivileged user DOS attack --- we'd be
+		 * in trouble if mortal users could trigger this path at
+		 * will.)
+		 *
+		 * NB. EXT4_STATE_JDATA is not set on files other than
+		 * regular files.  If somebody wants to bmap a directory
+		 * or symlink and gets confused because the buffer
+		 * hasn't yet been flushed to disk, they deserve
+		 * everything they get.
+		 */
+
+		ext4_clear_inode_state(inode, EXT4_STATE_JDATA);
+		journal = EXT4_JOURNAL(inode);
+		jbd2_journal_lock_updates(journal);
+		err = jbd2_journal_flush(journal);
+		jbd2_journal_unlock_updates(journal);
+
+		if (err)
+			return 0;
+	}
+
+	return generic_block_bmap(mapping, block, ext4_get_block);
+}
+
+static int ext4_readpage(struct file *file, struct page *page)
+{
+	trace_ext4_readpage(page);
+	return mpage_readpage(page, ext4_get_block);
+}
+
+static int
+ext4_readpages(struct file *file, struct address_space *mapping,
+		struct list_head *pages, unsigned nr_pages)
+{
+	return mpage_readpages(mapping, pages, nr_pages, ext4_get_block);
+}
+
+static void ext4_invalidatepage_free_endio(struct page *page, unsigned long offset)
+{
+	struct buffer_head *head, *bh;
+	unsigned int curr_off = 0;
+
+	if (!page_has_buffers(page))
+		return;
+	head = bh = page_buffers(page);
+	do {
+		if (offset <= curr_off && test_clear_buffer_uninit(bh)
+					&& bh->b_private) {
+			ext4_free_io_end(bh->b_private);
+			bh->b_private = NULL;
+			bh->b_end_io = NULL;
+		}
+		curr_off = curr_off + bh->b_size;
+		bh = bh->b_this_page;
+	} while (bh != head);
+}
+
+static void ext4_invalidatepage(struct page *page, unsigned long offset)
+{
+	journal_t *journal = EXT4_JOURNAL(page->mapping->host);
+
+	trace_ext4_invalidatepage(page, offset);
+
+	/*
+	 * free any io_end structure allocated for buffers to be discarded
+	 */
+	if (ext4_should_dioread_nolock(page->mapping->host))
+		ext4_invalidatepage_free_endio(page, offset);
+	/*
+	 * If it's a full truncate we just forget about the pending dirtying
+	 */
+	if (offset == 0)
+		ClearPageChecked(page);
+
+	if (journal)
+		jbd2_journal_invalidatepage(journal, page, offset);
+	else
+		block_invalidatepage(page, offset);
+}
+
+static int ext4_releasepage(struct page *page, gfp_t wait)
+{
+	journal_t *journal = EXT4_JOURNAL(page->mapping->host);
+
+	trace_ext4_releasepage(page);
+
+	WARN_ON(PageChecked(page));
+	if (!page_has_buffers(page))
+		return 0;
+	if (journal)
+		return jbd2_journal_try_to_free_buffers(journal, page, wait);
+	else
+		return try_to_free_buffers(page);
+}
+
+/*
+ * ext4_get_block used when preparing for a DIO write or buffer write.
+ * We allocate an uinitialized extent if blocks haven't been allocated.
+ * The extent will be converted to initialized after the IO is complete.
+ */
+static int ext4_get_block_write(struct inode *inode, sector_t iblock,
+		   struct buffer_head *bh_result, int create)
+{
+	ext4_debug("ext4_get_block_write: inode %lu, create flag %d\n",
+		   inode->i_ino, create);
+	return _ext4_get_block(inode, iblock, bh_result,
+			       EXT4_GET_BLOCKS_IO_CREATE_EXT);
+}
+
+static void ext4_end_io_dio(struct kiocb *iocb, loff_t offset,
+			    ssize_t size, void *private, int ret,
+			    bool is_async)
+{
+	struct inode *inode = iocb->ki_filp->f_path.dentry->d_inode;
+        ext4_io_end_t *io_end = iocb->private;
+	struct workqueue_struct *wq;
+	unsigned long flags;
+	struct ext4_inode_info *ei;
+
+	/* if not async direct IO or dio with 0 bytes write, just return */
+	if (!io_end || !size)
+		goto out;
+
+	ext_debug("ext4_end_io_dio(): io_end 0x%p"
+		  "for inode %lu, iocb 0x%p, offset %llu, size %llu\n",
+ 		  iocb->private, io_end->inode->i_ino, iocb, offset,
+		  size);
+
+	iocb->private = NULL;
+
+	/* if not aio dio with unwritten extents, just free io and return */
+	if (!(io_end->flag & EXT4_IO_END_UNWRITTEN)) {
+		ext4_free_io_end(io_end);
+out:
+		inode_dio_done(inode);
+		if (is_async)
+			aio_complete(iocb, ret, 0);
+		return;
+	}
+
+	io_end->offset = offset;
+	io_end->size = size;
+	if (is_async) {
+		io_end->iocb = iocb;
+		io_end->result = ret;
+	}
+	wq = EXT4_SB(io_end->inode->i_sb)->dio_unwritten_wq;
+
+	/* Add the io_end to per-inode completed aio dio list*/
+	ei = EXT4_I(io_end->inode);
+	spin_lock_irqsave(&ei->i_completed_io_lock, flags);
+	list_add_tail(&io_end->list, &ei->i_completed_io_list);
+	spin_unlock_irqrestore(&ei->i_completed_io_lock, flags);
+
+	/* queue the work to convert unwritten extents to written */
+	queue_work(wq, &io_end->work);
+}
+
+static void ext4_end_io_buffer_write(struct buffer_head *bh, int uptodate)
+{
+	ext4_io_end_t *io_end = bh->b_private;
+	struct workqueue_struct *wq;
+	struct inode *inode;
+	unsigned long flags;
+
+	if (!test_clear_buffer_uninit(bh) || !io_end)
+		goto out;
+
+	if (!(io_end->inode->i_sb->s_flags & MS_ACTIVE)) {
+		printk("sb umounted, discard end_io request for inode %lu\n",
+			io_end->inode->i_ino);
+		ext4_free_io_end(io_end);
+		goto out;
+	}
+
+	/*
+	 * It may be over-defensive here to check EXT4_IO_END_UNWRITTEN now,
+	 * but being more careful is always safe for the future change.
+	 */
+	inode = io_end->inode;
+	ext4_set_io_unwritten_flag(inode, io_end);
+
+	/* Add the io_end to per-inode completed io list*/
+	spin_lock_irqsave(&EXT4_I(inode)->i_completed_io_lock, flags);
+	list_add_tail(&io_end->list, &EXT4_I(inode)->i_completed_io_list);
+	spin_unlock_irqrestore(&EXT4_I(inode)->i_completed_io_lock, flags);
+
+	wq = EXT4_SB(inode->i_sb)->dio_unwritten_wq;
+	/* queue the work to convert unwritten extents to written */
+	queue_work(wq, &io_end->work);
+out:
+	bh->b_private = NULL;
+	bh->b_end_io = NULL;
+	clear_buffer_uninit(bh);
+	end_buffer_async_write(bh, uptodate);
+}
+
+static int ext4_set_bh_endio(struct buffer_head *bh, struct inode *inode)
+{
+	ext4_io_end_t *io_end;
+	struct page *page = bh->b_page;
+	loff_t offset = (sector_t)page->index << PAGE_CACHE_SHIFT;
+	size_t size = bh->b_size;
+
+retry:
+	io_end = ext4_init_io_end(inode, GFP_ATOMIC);
+	if (!io_end) {
+		pr_warn_ratelimited("%s: allocation fail\n", __func__);
+		schedule();
+		goto retry;
+	}
+	io_end->offset = offset;
+	io_end->size = size;
+	/*
+	 * We need to hold a reference to the page to make sure it
+	 * doesn't get evicted before ext4_end_io_work() has a chance
+	 * to convert the extent from written to unwritten.
+	 */
+	io_end->page = page;
+	get_page(io_end->page);
+
+	bh->b_private = io_end;
+	bh->b_end_io = ext4_end_io_buffer_write;
+	return 0;
+}
+
+/*
+ * For ext4 extent files, ext4 will do direct-io write to holes,
+ * preallocated extents, and those write extend the file, no need to
+ * fall back to buffered IO.
+ *
+ * For holes, we fallocate those blocks, mark them as uninitialized
+ * If those blocks were preallocated, we mark sure they are splited, but
+ * still keep the range to write as uninitialized.
+ *
+ * The unwrritten extents will be converted to written when DIO is completed.
+ * For async direct IO, since the IO may still pending when return, we
+ * set up an end_io call back function, which will do the conversion
+ * when async direct IO completed.
+ *
+ * If the O_DIRECT write will extend the file then add this inode to the
+ * orphan list.  So recovery will truncate it back to the original size
+ * if the machine crashes during the write.
+ *
+ */
+static ssize_t ext4_ext_direct_IO(int rw, struct kiocb *iocb,
+			      const struct iovec *iov, loff_t offset,
+			      unsigned long nr_segs)
+{
+	struct file *file = iocb->ki_filp;
+	struct inode *inode = file->f_mapping->host;
+	ssize_t ret;
+	size_t count = iov_length(iov, nr_segs);
+
+	loff_t final_size = offset + count;
+	if (rw == WRITE && final_size <= inode->i_size) {
+		/*
+ 		 * We could direct write to holes and fallocate.
+		 *
+ 		 * Allocated blocks to fill the hole are marked as uninitialized
+ 		 * to prevent parallel buffered read to expose the stale data
+ 		 * before DIO complete the data IO.
+		 *
+ 		 * As to previously fallocated extents, ext4 get_block
+ 		 * will just simply mark the buffer mapped but still
+ 		 * keep the extents uninitialized.
+ 		 *
+		 * for non AIO case, we will convert those unwritten extents
+		 * to written after return back from blockdev_direct_IO.
+		 *
+		 * for async DIO, the conversion needs to be defered when
+		 * the IO is completed. The ext4 end_io callback function
+		 * will be called to take care of the conversion work.
+		 * Here for async case, we allocate an io_end structure to
+		 * hook to the iocb.
+ 		 */
+		iocb->private = NULL;
+		EXT4_I(inode)->cur_aio_dio = NULL;
+		if (!is_sync_kiocb(iocb)) {
+			ext4_io_end_t *io_end =
+				ext4_init_io_end(inode, GFP_NOFS);
+			if (!io_end)
+				return -ENOMEM;
+			io_end->flag |= EXT4_IO_END_DIRECT;
+			iocb->private = io_end;
+			/*
+			 * we save the io structure for current async
+			 * direct IO, so that later ext4_map_blocks()
+			 * could flag the io structure whether there
+			 * is a unwritten extents needs to be converted
+			 * when IO is completed.
+			 */
+			EXT4_I(inode)->cur_aio_dio = iocb->private;
+		}
+
+		ret = __blockdev_direct_IO(rw, iocb, inode,
+					 inode->i_sb->s_bdev, iov,
+					 offset, nr_segs,
+					 ext4_get_block_write,
+					 ext4_end_io_dio,
+					 NULL,
+					 DIO_LOCKING | DIO_SKIP_HOLES);
+		if (iocb->private)
+			EXT4_I(inode)->cur_aio_dio = NULL;
+		/*
+		 * The io_end structure takes a reference to the inode,
+		 * that structure needs to be destroyed and the
+		 * reference to the inode need to be dropped, when IO is
+		 * complete, even with 0 byte write, or failed.
+		 *
+		 * In the successful AIO DIO case, the io_end structure will be
+		 * desctroyed and the reference to the inode will be dropped
+		 * after the end_io call back function is called.
+		 *
+		 * In the case there is 0 byte write, or error case, since
+		 * VFS direct IO won't invoke the end_io call back function,
+		 * we need to free the end_io structure here.
+		 */
+		if (ret != -EIOCBQUEUED && ret <= 0 && iocb->private) {
+			ext4_free_io_end(iocb->private);
+			iocb->private = NULL;
+		} else if (ret > 0 && ext4_test_inode_state(inode,
+						EXT4_STATE_DIO_UNWRITTEN)) {
+			int err;
+			/*
+			 * for non AIO case, since the IO is already
+			 * completed, we could do the conversion right here
+			 */
+			err = ext4_convert_unwritten_extents(inode,
+							     offset, ret);
+			if (err < 0)
+				ret = err;
+			ext4_clear_inode_state(inode, EXT4_STATE_DIO_UNWRITTEN);
+		}
+		return ret;
+	}
+
+	/* for write the the end of file case, we fall back to old way */
+	return ext4_ind_direct_IO(rw, iocb, iov, offset, nr_segs);
+}
+
+static ssize_t ext4_direct_IO(int rw, struct kiocb *iocb,
+			      const struct iovec *iov, loff_t offset,
+			      unsigned long nr_segs)
+{
+	struct file *file = iocb->ki_filp;
+	struct inode *inode = file->f_mapping->host;
+	ssize_t ret;
+
+	/*
+	 * If we are doing data journalling we don't support O_DIRECT
+	 */
+	if (ext4_should_journal_data(inode))
+		return 0;
+
+	trace_ext4_direct_IO_enter(inode, offset, iov_length(iov, nr_segs), rw);
+	if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))
+		ret = ext4_ext_direct_IO(rw, iocb, iov, offset, nr_segs);
+	else
+		ret = ext4_ind_direct_IO(rw, iocb, iov, offset, nr_segs);
+	trace_ext4_direct_IO_exit(inode, offset,
+				iov_length(iov, nr_segs), rw, ret);
+	return ret;
+}
+
+/*
+ * Pages can be marked dirty completely asynchronously from ext4's journalling
+ * activity.  By filemap_sync_pte(), try_to_unmap_one(), etc.  We cannot do
+ * much here because ->set_page_dirty is called under VFS locks.  The page is
+ * not necessarily locked.
+ *
+ * We cannot just dirty the page and leave attached buffers clean, because the
+ * buffers' dirty state is "definitive".  We cannot just set the buffers dirty
+ * or jbddirty because all the journalling code will explode.
+ *
+ * So what we do is to mark the page "pending dirty" and next time writepage
+ * is called, propagate that into the buffers appropriately.
+ */
+static int ext4_journalled_set_page_dirty(struct page *page)
+{
+	SetPageChecked(page);
+	return __set_page_dirty_nobuffers(page);
+}
+
+static const struct address_space_operations ext4_ordered_aops = {
+	.readpage		= ext4_readpage,
+	.readpages		= ext4_readpages,
+	.writepage		= ext4_writepage,
+	.write_begin		= ext4_write_begin,
+	.write_end		= ext4_ordered_write_end,
+	.bmap			= ext4_bmap,
+	.invalidatepage		= ext4_invalidatepage,
+	.releasepage		= ext4_releasepage,
+	.direct_IO		= ext4_direct_IO,
+	.migratepage		= buffer_migrate_page,
+	.is_partially_uptodate  = block_is_partially_uptodate,
+	.error_remove_page	= generic_error_remove_page,
+};
+
+static const struct address_space_operations ext4_writeback_aops = {
+	.readpage		= ext4_readpage,
+	.readpages		= ext4_readpages,
+	.writepage		= ext4_writepage,
+	.write_begin		= ext4_write_begin,
+	.write_end		= ext4_writeback_write_end,
+	.bmap			= ext4_bmap,
+	.invalidatepage		= ext4_invalidatepage,
+	.releasepage		= ext4_releasepage,
+	.direct_IO		= ext4_direct_IO,
+	.migratepage		= buffer_migrate_page,
+	.is_partially_uptodate  = block_is_partially_uptodate,
+	.error_remove_page	= generic_error_remove_page,
+};
+
+static const struct address_space_operations ext4_journalled_aops = {
+	.readpage		= ext4_readpage,
+	.readpages		= ext4_readpages,
+	.writepage		= ext4_writepage,
+	.write_begin		= ext4_write_begin,
+	.write_end		= ext4_journalled_write_end,
+	.set_page_dirty		= ext4_journalled_set_page_dirty,
+	.bmap			= ext4_bmap,
+	.invalidatepage		= ext4_invalidatepage,
+	.releasepage		= ext4_releasepage,
+	.direct_IO		= ext4_direct_IO,
+	.is_partially_uptodate  = block_is_partially_uptodate,
+	.error_remove_page	= generic_error_remove_page,
+};
+
+static const struct address_space_operations ext4_da_aops = {
+	.readpage		= ext4_readpage,
+	.readpages		= ext4_readpages,
+	.writepage		= ext4_writepage,
+	.writepages		= ext4_da_writepages,
+	.write_begin		= ext4_da_write_begin,
+	.write_end		= ext4_da_write_end,
+	.bmap			= ext4_bmap,
+	.invalidatepage		= ext4_da_invalidatepage,
+	.releasepage		= ext4_releasepage,
+	.direct_IO		= ext4_direct_IO,
+	.migratepage		= buffer_migrate_page,
+	.is_partially_uptodate  = block_is_partially_uptodate,
+	.error_remove_page	= generic_error_remove_page,
+};
+
+void ext4_set_aops(struct inode *inode)
+{
+	switch (ext4_inode_journal_mode(inode)) {
+	case EXT4_INODE_ORDERED_DATA_MODE:
+		if (test_opt(inode->i_sb, DELALLOC))
+			inode->i_mapping->a_ops = &ext4_da_aops;
+		else
+			inode->i_mapping->a_ops = &ext4_ordered_aops;
+		break;
+	case EXT4_INODE_WRITEBACK_DATA_MODE:
+		if (test_opt(inode->i_sb, DELALLOC))
+			inode->i_mapping->a_ops = &ext4_da_aops;
+		else
+			inode->i_mapping->a_ops = &ext4_writeback_aops;
+		break;
+	case EXT4_INODE_JOURNAL_DATA_MODE:
+		inode->i_mapping->a_ops = &ext4_journalled_aops;
+		break;
+	default:
+		BUG();
+	}
+}
+
+
+/*
+ * ext4_discard_partial_page_buffers()
+ * Wrapper function for ext4_discard_partial_page_buffers_no_lock.
+ * This function finds and locks the page containing the offset
+ * "from" and passes it to ext4_discard_partial_page_buffers_no_lock.
+ * Calling functions that already have the page locked should call
+ * ext4_discard_partial_page_buffers_no_lock directly.
+ */
+int ext4_discard_partial_page_buffers(handle_t *handle,
+		struct address_space *mapping, loff_t from,
+		loff_t length, int flags)
+{
+	struct inode *inode = mapping->host;
+	struct page *page;
+	int err = 0;
+
+	page = find_or_create_page(mapping, from >> PAGE_CACHE_SHIFT,
+				   mapping_gfp_mask(mapping) & ~__GFP_FS);
+	if (!page)
+		return -ENOMEM;
+
+	err = ext4_discard_partial_page_buffers_no_lock(handle, inode, page,
+		from, length, flags);
+
+	unlock_page(page);
+	page_cache_release(page);
+	return err;
+}
+
+/*
+ * ext4_discard_partial_page_buffers_no_lock()
+ * Zeros a page range of length 'length' starting from offset 'from'.
+ * Buffer heads that correspond to the block aligned regions of the
+ * zeroed range will be unmapped.  Unblock aligned regions
+ * will have the corresponding buffer head mapped if needed so that
+ * that region of the page can be updated with the partial zero out.
+ *
+ * This function assumes that the page has already been  locked.  The
+ * The range to be discarded must be contained with in the given page.
+ * If the specified range exceeds the end of the page it will be shortened
+ * to the end of the page that corresponds to 'from'.  This function is
+ * appropriate for updating a page and it buffer heads to be unmapped and
+ * zeroed for blocks that have been either released, or are going to be
+ * released.
+ *
+ * handle: The journal handle
+ * inode:  The files inode
+ * page:   A locked page that contains the offset "from"
+ * from:   The starting byte offset (from the begining of the file)
+ *         to begin discarding
+ * len:    The length of bytes to discard
+ * flags:  Optional flags that may be used:
+ *
+ *         EXT4_DISCARD_PARTIAL_PG_ZERO_UNMAPPED
+ *         Only zero the regions of the page whose buffer heads
+ *         have already been unmapped.  This flag is appropriate
+ *         for updateing the contents of a page whose blocks may
+ *         have already been released, and we only want to zero
+ *         out the regions that correspond to those released blocks.
+ *
+ * Returns zero on sucess or negative on failure.
+ */
+int ext4_discard_partial_page_buffers_no_lock(handle_t *handle,
+		struct inode *inode, struct page *page, loff_t from,
+		loff_t length, int flags)
+{
+	ext4_fsblk_t index = from >> PAGE_CACHE_SHIFT;
+	unsigned int offset = from & (PAGE_CACHE_SIZE-1);
+	unsigned int blocksize, max, pos;
+	ext4_lblk_t iblock;
+	struct buffer_head *bh;
+	int err = 0;
+
+	blocksize = inode->i_sb->s_blocksize;
+	max = PAGE_CACHE_SIZE - offset;
+
+	if (index != page->index)
+		return -EINVAL;
+
+	/*
+	 * correct length if it does not fall between
+	 * 'from' and the end of the page
+	 */
+	if (length > max || length < 0)
+		length = max;
+
+	iblock = index << (PAGE_CACHE_SHIFT - inode->i_sb->s_blocksize_bits);
+
+	if (!page_has_buffers(page))
+		create_empty_buffers(page, blocksize, 0);
+
+	/* Find the buffer that contains "offset" */
+	bh = page_buffers(page);
+	pos = blocksize;
+	while (offset >= pos) {
+		bh = bh->b_this_page;
+		iblock++;
+		pos += blocksize;
+	}
+
+	pos = offset;
+	while (pos < offset + length) {
+		unsigned int end_of_block, range_to_discard;
+
+		err = 0;
+
+		/* The length of space left to zero and unmap */
+		range_to_discard = offset + length - pos;
+
+		/* The length of space until the end of the block */
+		end_of_block = blocksize - (pos & (blocksize-1));
+
+		/*
+		 * Do not unmap or zero past end of block
+		 * for this buffer head
+		 */
+		if (range_to_discard > end_of_block)
+			range_to_discard = end_of_block;
+
+
+		/*
+		 * Skip this buffer head if we are only zeroing unampped
+		 * regions of the page
+		 */
+		if (flags & EXT4_DISCARD_PARTIAL_PG_ZERO_UNMAPPED &&
+			buffer_mapped(bh))
+				goto next;
+
+		/* If the range is block aligned, unmap */
+		if (range_to_discard == blocksize) {
+			clear_buffer_dirty(bh);
+			bh->b_bdev = NULL;
+			clear_buffer_mapped(bh);
+			clear_buffer_req(bh);
+			clear_buffer_new(bh);
+			clear_buffer_delay(bh);
+			clear_buffer_unwritten(bh);
+			clear_buffer_uptodate(bh);
+			zero_user(page, pos, range_to_discard);
+			BUFFER_TRACE(bh, "Buffer discarded");
+			goto next;
+		}
+
+		/*
+		 * If this block is not completely contained in the range
+		 * to be discarded, then it is not going to be released. Because
+		 * we need to keep this block, we need to make sure this part
+		 * of the page is uptodate before we modify it by writeing
+		 * partial zeros on it.
+		 */
+		if (!buffer_mapped(bh)) {
+			/*
+			 * Buffer head must be mapped before we can read
+			 * from the block
+			 */
+			BUFFER_TRACE(bh, "unmapped");
+			ext4_get_block(inode, iblock, bh, 0);
+			/* unmapped? It's a hole - nothing to do */
+			if (!buffer_mapped(bh)) {
+				BUFFER_TRACE(bh, "still unmapped");
+				goto next;
+			}
+		}
+
+		/* Ok, it's mapped. Make sure it's up-to-date */
+		if (PageUptodate(page))
+			set_buffer_uptodate(bh);
+
+		if (!buffer_uptodate(bh)) {
+			err = -EIO;
+			ll_rw_block(READ, 1, &bh);
+			wait_on_buffer(bh);
+			/* Uhhuh. Read error. Complain and punt.*/
+			if (!buffer_uptodate(bh))
+				goto next;
+		}
+
+		if (ext4_should_journal_data(inode)) {
+			BUFFER_TRACE(bh, "get write access");
+			err = ext4_journal_get_write_access(handle, bh);
+			if (err)
+				goto next;
+		}
+
+		zero_user(page, pos, range_to_discard);
+
+		err = 0;
+		if (ext4_should_journal_data(inode)) {
+			err = ext4_handle_dirty_metadata(handle, inode, bh);
+		} else
+			mark_buffer_dirty(bh);
+
+		BUFFER_TRACE(bh, "Partial buffer zeroed");
+next:
+		bh = bh->b_this_page;
+		iblock++;
+		pos += range_to_discard;
+	}
+
+	return err;
+}
+
+/*
+ * ext4_block_truncate_page() zeroes out a mapping from file offset `from'
+ * up to the end of the block which corresponds to `from'.
+ * This required during truncate. We need to physically zero the tail end
+ * of that block so it doesn't yield old data if the file is later grown.
+ */
+int ext4_block_truncate_page(handle_t *handle,
+		struct address_space *mapping, loff_t from)
+{
+	unsigned offset = from & (PAGE_CACHE_SIZE-1);
+	unsigned length;
+	unsigned blocksize;
+	struct inode *inode = mapping->host;
+
+	blocksize = inode->i_sb->s_blocksize;
+	length = blocksize - (offset & (blocksize - 1));
+
+	return ext4_block_zero_page_range(handle, mapping, from, length);
+}
+
+/*
+ * ext4_block_zero_page_range() zeros out a mapping of length 'length'
+ * starting from file offset 'from'.  The range to be zero'd must
+ * be contained with in one block.  If the specified range exceeds
+ * the end of the block it will be shortened to end of the block
+ * that cooresponds to 'from'
+ */
+int ext4_block_zero_page_range(handle_t *handle,
+		struct address_space *mapping, loff_t from, loff_t length)
+{
+	ext4_fsblk_t index = from >> PAGE_CACHE_SHIFT;
+	unsigned offset = from & (PAGE_CACHE_SIZE-1);
+	unsigned blocksize, max, pos;
+	ext4_lblk_t iblock;
+	struct inode *inode = mapping->host;
+	struct buffer_head *bh;
+	struct page *page;
+	int err = 0;
+
+	page = find_or_create_page(mapping, from >> PAGE_CACHE_SHIFT,
+				   mapping_gfp_mask(mapping) & ~__GFP_FS);
+	if (!page)
+		return -ENOMEM;
+
+	blocksize = inode->i_sb->s_blocksize;
+	max = blocksize - (offset & (blocksize - 1));
+
+	/*
+	 * correct length if it does not fall between
+	 * 'from' and the end of the block
+	 */
+	if (length > max || length < 0)
+		length = max;
+
+	iblock = index << (PAGE_CACHE_SHIFT - inode->i_sb->s_blocksize_bits);
+
+	if (!page_has_buffers(page))
+		create_empty_buffers(page, blocksize, 0);
+
+	/* Find the buffer that contains "offset" */
+	bh = page_buffers(page);
+	pos = blocksize;
+	while (offset >= pos) {
+		bh = bh->b_this_page;
+		iblock++;
+		pos += blocksize;
+	}
+
+	err = 0;
+	if (buffer_freed(bh)) {
+		BUFFER_TRACE(bh, "freed: skip");
+		goto unlock;
+	}
+
+	if (!buffer_mapped(bh)) {
+		BUFFER_TRACE(bh, "unmapped");
+		ext4_get_block(inode, iblock, bh, 0);
+		/* unmapped? It's a hole - nothing to do */
+		if (!buffer_mapped(bh)) {
+			BUFFER_TRACE(bh, "still unmapped");
+			goto unlock;
+		}
+	}
+
+	/* Ok, it's mapped. Make sure it's up-to-date */
+	if (PageUptodate(page))
+		set_buffer_uptodate(bh);
+
+	if (!buffer_uptodate(bh)) {
+		err = -EIO;
+		ll_rw_block(READ, 1, &bh);
+		wait_on_buffer(bh);
+		/* Uhhuh. Read error. Complain and punt. */
+		if (!buffer_uptodate(bh))
+			goto unlock;
+	}
+
+	if (ext4_should_journal_data(inode)) {
+		BUFFER_TRACE(bh, "get write access");
+		err = ext4_journal_get_write_access(handle, bh);
+		if (err)
+			goto unlock;
+	}
+
+	zero_user(page, offset, length);
+
+	BUFFER_TRACE(bh, "zeroed end of block");
+
+	err = 0;
+	if (ext4_should_journal_data(inode)) {
+		err = ext4_handle_dirty_metadata(handle, inode, bh);
+	} else
+		mark_buffer_dirty(bh);
+
+unlock:
+	unlock_page(page);
+	page_cache_release(page);
+	return err;
+}
+
+int ext4_can_truncate(struct inode *inode)
+{
+	if (S_ISREG(inode->i_mode))
+		return 1;
+	if (S_ISDIR(inode->i_mode))
+		return 1;
+	if (S_ISLNK(inode->i_mode))
+		return !ext4_inode_is_fast_symlink(inode);
+	return 0;
+}
+
+/*
+ * ext4_punch_hole: punches a hole in a file by releaseing the blocks
+ * associated with the given offset and length
+ *
+ * @inode:  File inode
+ * @offset: The offset where the hole will begin
+ * @len:    The length of the hole
+ *
+ * Returns: 0 on sucess or negative on failure
+ */
+
+int ext4_punch_hole(struct file *file, loff_t offset, loff_t length)
+{
+	struct inode *inode = file->f_path.dentry->d_inode;
+	if (!S_ISREG(inode->i_mode))
+		return -ENOTSUPP;
+
+	if (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {
+		/* TODO: Add support for non extent hole punching */
+		return -ENOTSUPP;
+	}
+
+	if (EXT4_SB(inode->i_sb)->s_cluster_ratio > 1) {
+		/* TODO: Add support for bigalloc file systems */
+		return -ENOTSUPP;
+	}
+
+	return ext4_ext_punch_hole(file, offset, length);
+}
+
+/*
+ * ext4_truncate()
+ *
+ * We block out ext4_get_block() block instantiations across the entire
+ * transaction, and VFS/VM ensures that ext4_truncate() cannot run
+ * simultaneously on behalf of the same inode.
+ *
+ * As we work through the truncate and commmit bits of it to the journal there
+ * is one core, guiding principle: the file's tree must always be consistent on
+ * disk.  We must be able to restart the truncate after a crash.
+ *
+ * The file's tree may be transiently inconsistent in memory (although it
+ * probably isn't), but whenever we close off and commit a journal transaction,
+ * the contents of (the filesystem + the journal) must be consistent and
+ * restartable.  It's pretty simple, really: bottom up, right to left (although
+ * left-to-right works OK too).
+ *
+ * Note that at recovery time, journal replay occurs *before* the restart of
+ * truncate against the orphan inode list.
+ *
+ * The committed inode has the new, desired i_size (which is the same as
+ * i_disksize in this case).  After a crash, ext4_orphan_cleanup() will see
+ * that this inode's truncate did not complete and it will again call
+ * ext4_truncate() to have another go.  So there will be instantiated blocks
+ * to the right of the truncation point in a crashed ext4 filesystem.  But
+ * that's fine - as long as they are linked from the inode, the post-crash
+ * ext4_truncate() run will find them and release them.
+ */
+void ext4_truncate(struct inode *inode)
+{
+	trace_ext4_truncate_enter(inode);
+
+	if (!ext4_can_truncate(inode))
+		return;
+
+	ext4_clear_inode_flag(inode, EXT4_INODE_EOFBLOCKS);
+
+	if (inode->i_size == 0 && !test_opt(inode->i_sb, NO_AUTO_DA_ALLOC))
+		ext4_set_inode_state(inode, EXT4_STATE_DA_ALLOC_CLOSE);
+
+	if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))
+		ext4_ext_truncate(inode);
+	else
+		ext4_ind_truncate(inode);
+
+	trace_ext4_truncate_exit(inode);
+}
+
+/*
+ * ext4_get_inode_loc returns with an extra refcount against the inode's
+ * underlying buffer_head on success. If 'in_mem' is true, we have all
+ * data in memory that is needed to recreate the on-disk version of this
+ * inode.
+ */
+static int __ext4_get_inode_loc(struct inode *inode,
+				struct ext4_iloc *iloc, int in_mem)
+{
+	struct ext4_group_desc	*gdp;
+	struct buffer_head	*bh;
+	struct super_block	*sb = inode->i_sb;
+	ext4_fsblk_t		block;
+	int			inodes_per_block, inode_offset;
+
+	iloc->bh = NULL;
+	if (!ext4_valid_inum(sb, inode->i_ino))
+		return -EIO;
+
+	iloc->block_group = (inode->i_ino - 1) / EXT4_INODES_PER_GROUP(sb);
+	gdp = ext4_get_group_desc(sb, iloc->block_group, NULL);
+	if (!gdp)
+		return -EIO;
+
+	/*
+	 * Figure out the offset within the block group inode table
+	 */
+	inodes_per_block = EXT4_SB(sb)->s_inodes_per_block;
+	inode_offset = ((inode->i_ino - 1) %
+			EXT4_INODES_PER_GROUP(sb));
+	block = ext4_inode_table(sb, gdp) + (inode_offset / inodes_per_block);
+	iloc->offset = (inode_offset % inodes_per_block) * EXT4_INODE_SIZE(sb);
+
+	bh = sb_getblk(sb, block);
+	if (!bh)
+		return -ENOMEM;
+	if (!buffer_uptodate(bh)) {
+		lock_buffer(bh);
+
+		/*
+		 * If the buffer has the write error flag, we have failed
+		 * to write out another inode in the same block.  In this
+		 * case, we don't have to read the block because we may
+		 * read the old inode data successfully.
+		 */
+		if (buffer_write_io_error(bh) && !buffer_uptodate(bh))
+			set_buffer_uptodate(bh);
+
+		if (buffer_uptodate(bh)) {
+			/* someone brought it uptodate while we waited */
+			unlock_buffer(bh);
+			goto has_buffer;
+		}
+
+		/*
+		 * If we have all information of the inode in memory and this
+		 * is the only valid inode in the block, we need not read the
+		 * block.
+		 */
+		if (in_mem) {
+			struct buffer_head *bitmap_bh;
+			int i, start;
+
+			start = inode_offset & ~(inodes_per_block - 1);
+
+			/* Is the inode bitmap in cache? */
+			bitmap_bh = sb_getblk(sb, ext4_inode_bitmap(sb, gdp));
+			if (!bitmap_bh)
+				goto make_io;
+
+			/*
+			 * If the inode bitmap isn't in cache then the
+			 * optimisation may end up performing two reads instead
+			 * of one, so skip it.
+			 */
+			if (!buffer_uptodate(bitmap_bh)) {
+				brelse(bitmap_bh);
+				goto make_io;
+			}
+			for (i = start; i < start + inodes_per_block; i++) {
+				if (i == inode_offset)
+					continue;
+				if (ext4_test_bit(i, bitmap_bh->b_data))
+					break;
+			}
+			brelse(bitmap_bh);
+			if (i == start + inodes_per_block) {
+				/* all other inodes are free, so skip I/O */
+				memset(bh->b_data, 0, bh->b_size);
+				set_buffer_uptodate(bh);
+				unlock_buffer(bh);
+				goto has_buffer;
+			}
+		}
+
+make_io:
+		/*
+		 * If we need to do any I/O, try to pre-readahead extra
+		 * blocks from the inode table.
+		 */
+		if (EXT4_SB(sb)->s_inode_readahead_blks) {
+			ext4_fsblk_t b, end, table;
+			unsigned num;
+
+			table = ext4_inode_table(sb, gdp);
+			/* s_inode_readahead_blks is always a power of 2 */
+			b = block & ~(EXT4_SB(sb)->s_inode_readahead_blks-1);
+			if (table > b)
+				b = table;
+			end = b + EXT4_SB(sb)->s_inode_readahead_blks;
+			num = EXT4_INODES_PER_GROUP(sb);
+			if (EXT4_HAS_RO_COMPAT_FEATURE(sb,
+				       EXT4_FEATURE_RO_COMPAT_GDT_CSUM))
+				num -= ext4_itable_unused_count(sb, gdp);
+			table += num / inodes_per_block;
+			if (end > table)
+				end = table;
+			while (b <= end)
+				sb_breadahead(sb, b++);
+		}
+
+		/*
+		 * There are other valid inodes in the buffer, this inode
+		 * has in-inode xattrs, or we don't have this inode in memory.
+		 * Read the block from disk.
+		 */
+		trace_ext4_load_inode(inode);
+		get_bh(bh);
+		bh->b_end_io = end_buffer_read_sync;
+		submit_bh(READ | REQ_META | REQ_PRIO, bh);
+		wait_on_buffer(bh);
+		if (!buffer_uptodate(bh)) {
+			EXT4_ERROR_INODE_BLOCK(inode, block,
+					       "unable to read itable block");
+			brelse(bh);
+			return -EIO;
+		}
+	}
+has_buffer:
+	iloc->bh = bh;
+	return 0;
+}
+
+int ext4_get_inode_loc(struct inode *inode, struct ext4_iloc *iloc)
+{
+	/* We have all inode data except xattrs in memory here. */
+	return __ext4_get_inode_loc(inode, iloc,
+		!ext4_test_inode_state(inode, EXT4_STATE_XATTR));
+}
+
+void ext4_set_inode_flags(struct inode *inode)
+{
+	unsigned int flags = EXT4_I(inode)->i_flags;
+
+	inode->i_flags &= ~(S_SYNC|S_APPEND|S_IMMUTABLE|S_NOATIME|S_DIRSYNC);
+	if (flags & EXT4_SYNC_FL)
+		inode->i_flags |= S_SYNC;
+	if (flags & EXT4_APPEND_FL)
+		inode->i_flags |= S_APPEND;
+	if (flags & EXT4_IMMUTABLE_FL)
+		inode->i_flags |= S_IMMUTABLE;
+	if (flags & EXT4_NOATIME_FL)
+		inode->i_flags |= S_NOATIME;
+	if (flags & EXT4_DIRSYNC_FL)
+		inode->i_flags |= S_DIRSYNC;
+}
+
+/* Propagate flags from i_flags to EXT4_I(inode)->i_flags */
+void ext4_get_inode_flags(struct ext4_inode_info *ei)
+{
+	unsigned int vfs_fl;
+	unsigned long old_fl, new_fl;
+
+	do {
+		vfs_fl = ei->vfs_inode.i_flags;
+		old_fl = ei->i_flags;
+		new_fl = old_fl & ~(EXT4_SYNC_FL|EXT4_APPEND_FL|
+				EXT4_IMMUTABLE_FL|EXT4_NOATIME_FL|
+				EXT4_DIRSYNC_FL);
+		if (vfs_fl & S_SYNC)
+			new_fl |= EXT4_SYNC_FL;
+		if (vfs_fl & S_APPEND)
+			new_fl |= EXT4_APPEND_FL;
+		if (vfs_fl & S_IMMUTABLE)
+			new_fl |= EXT4_IMMUTABLE_FL;
+		if (vfs_fl & S_NOATIME)
+			new_fl |= EXT4_NOATIME_FL;
+		if (vfs_fl & S_DIRSYNC)
+			new_fl |= EXT4_DIRSYNC_FL;
+	} while (cmpxchg(&ei->i_flags, old_fl, new_fl) != old_fl);
+}
+
+static blkcnt_t ext4_inode_blocks(struct ext4_inode *raw_inode,
+				  struct ext4_inode_info *ei)
+{
+	blkcnt_t i_blocks ;
+	struct inode *inode = &(ei->vfs_inode);
+	struct super_block *sb = inode->i_sb;
+
+	if (EXT4_HAS_RO_COMPAT_FEATURE(sb,
+				EXT4_FEATURE_RO_COMPAT_HUGE_FILE)) {
+		/* we are using combined 48 bit field */
+		i_blocks = ((u64)le16_to_cpu(raw_inode->i_blocks_high)) << 32 |
+					le32_to_cpu(raw_inode->i_blocks_lo);
+		if (ext4_test_inode_flag(inode, EXT4_INODE_HUGE_FILE)) {
+			/* i_blocks represent file system block size */
+			return i_blocks  << (inode->i_blkbits - 9);
+		} else {
+			return i_blocks;
+		}
+	} else {
+		return le32_to_cpu(raw_inode->i_blocks_lo);
+	}
+}
+
+struct inode *ext4_iget(struct super_block *sb, unsigned long ino)
+{
+	struct ext4_iloc iloc;
+	struct ext4_inode *raw_inode;
+	struct ext4_inode_info *ei;
+	struct inode *inode;
+	journal_t *journal = EXT4_SB(sb)->s_journal;
+	long ret;
+	int block;
+
+	inode = iget_locked(sb, ino);
+	if (!inode)
+		return ERR_PTR(-ENOMEM);
+	if (!(inode->i_state & I_NEW))
+		return inode;
+
+	ei = EXT4_I(inode);
+	iloc.bh = NULL;
+
+	ret = __ext4_get_inode_loc(inode, &iloc, 0);
+	if (ret < 0)
+		goto bad_inode;
+	raw_inode = ext4_raw_inode(&iloc);
+	inode->i_mode = le16_to_cpu(raw_inode->i_mode);
+	inode->i_uid = (uid_t)le16_to_cpu(raw_inode->i_uid_low);
+	inode->i_gid = (gid_t)le16_to_cpu(raw_inode->i_gid_low);
+	if (!(test_opt(inode->i_sb, NO_UID32))) {
+		inode->i_uid |= le16_to_cpu(raw_inode->i_uid_high) << 16;
+		inode->i_gid |= le16_to_cpu(raw_inode->i_gid_high) << 16;
+	}
+	set_nlink(inode, le16_to_cpu(raw_inode->i_links_count));
+
+	ext4_clear_state_flags(ei);	/* Only relevant on 32-bit archs */
+	ei->i_dir_start_lookup = 0;
+	ei->i_dtime = le32_to_cpu(raw_inode->i_dtime);
+	/* We now have enough fields to check if the inode was active or not.
+	 * This is needed because nfsd might try to access dead inodes
+	 * the test is that same one that e2fsck uses
+	 * NeilBrown 1999oct15
+	 */
+	if (inode->i_nlink == 0) {
+		if (inode->i_mode == 0 ||
+		    !(EXT4_SB(inode->i_sb)->s_mount_state & EXT4_ORPHAN_FS)) {
+			/* this inode is deleted */
+			ret = -ESTALE;
+			goto bad_inode;
+		}
+		/* The only unlinked inodes we let through here have
+		 * valid i_mode and are being read by the orphan
+		 * recovery code: that's fine, we're about to complete
+		 * the process of deleting those. */
+	}
+	ei->i_flags = le32_to_cpu(raw_inode->i_flags);
+	inode->i_blocks = ext4_inode_blocks(raw_inode, ei);
+	ei->i_file_acl = le32_to_cpu(raw_inode->i_file_acl_lo);
+	if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_64BIT))
+		ei->i_file_acl |=
+			((__u64)le16_to_cpu(raw_inode->i_file_acl_high)) << 32;
+	inode->i_size = ext4_isize(raw_inode);
+	ei->i_disksize = inode->i_size;
+#ifdef CONFIG_QUOTA
+	ei->i_reserved_quota = 0;
+#endif
+	inode->i_generation = le32_to_cpu(raw_inode->i_generation);
+	ei->i_block_group = iloc.block_group;
+	ei->i_last_alloc_group = ~0;
+	/*
+	 * NOTE! The in-memory inode i_data array is in little-endian order
+	 * even on big-endian machines: we do NOT byteswap the block numbers!
+	 */
+	for (block = 0; block < EXT4_N_BLOCKS; block++)
+		ei->i_data[block] = raw_inode->i_block[block];
+	INIT_LIST_HEAD(&ei->i_orphan);
+
+	/*
+	 * Set transaction id's of transactions that have to be committed
+	 * to finish f[data]sync. We set them to currently running transaction
+	 * as we cannot be sure that the inode or some of its metadata isn't
+	 * part of the transaction - the inode could have been reclaimed and
+	 * now it is reread from disk.
+	 */
+	if (journal) {
+		transaction_t *transaction;
+		tid_t tid;
+
+		read_lock(&journal->j_state_lock);
+		if (journal->j_running_transaction)
+			transaction = journal->j_running_transaction;
+		else
+			transaction = journal->j_committing_transaction;
+		if (transaction)
+			tid = transaction->t_tid;
+		else
+			tid = journal->j_commit_sequence;
+		read_unlock(&journal->j_state_lock);
+		ei->i_sync_tid = tid;
+		ei->i_datasync_tid = tid;
+	}
+
+	if (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {
+		ei->i_extra_isize = le16_to_cpu(raw_inode->i_extra_isize);
+		if (EXT4_GOOD_OLD_INODE_SIZE + ei->i_extra_isize >
+		    EXT4_INODE_SIZE(inode->i_sb)) {
+			ret = -EIO;
+			goto bad_inode;
+		}
+		if (ei->i_extra_isize == 0) {
+			/* The extra space is currently unused. Use it. */
+			ei->i_extra_isize = sizeof(struct ext4_inode) -
+					    EXT4_GOOD_OLD_INODE_SIZE;
+		} else {
+			__le32 *magic = (void *)raw_inode +
+					EXT4_GOOD_OLD_INODE_SIZE +
+					ei->i_extra_isize;
+			if (*magic == cpu_to_le32(EXT4_XATTR_MAGIC))
+				ext4_set_inode_state(inode, EXT4_STATE_XATTR);
+		}
+	} else
+		ei->i_extra_isize = 0;
+
+	EXT4_INODE_GET_XTIME(i_ctime, inode, raw_inode);
+	EXT4_INODE_GET_XTIME(i_mtime, inode, raw_inode);
+	EXT4_INODE_GET_XTIME(i_atime, inode, raw_inode);
+	EXT4_EINODE_GET_XTIME(i_crtime, ei, raw_inode);
+
+	inode->i_version = le32_to_cpu(raw_inode->i_disk_version);
+	if (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {
+		if (EXT4_FITS_IN_INODE(raw_inode, ei, i_version_hi))
+			inode->i_version |=
+			(__u64)(le32_to_cpu(raw_inode->i_version_hi)) << 32;
+	}
+
+	ret = 0;
+	if (ei->i_file_acl &&
+	    !ext4_data_block_valid(EXT4_SB(sb), ei->i_file_acl, 1)) {
+		EXT4_ERROR_INODE(inode, "bad extended attribute block %llu",
+				 ei->i_file_acl);
+		ret = -EIO;
+		goto bad_inode;
+	} else if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {
+		if (S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||
+		    (S_ISLNK(inode->i_mode) &&
+		     !ext4_inode_is_fast_symlink(inode)))
+			/* Validate extent which is part of inode */
+			ret = ext4_ext_check_inode(inode);
+	} else if (S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||
+		   (S_ISLNK(inode->i_mode) &&
+		    !ext4_inode_is_fast_symlink(inode))) {
+		/* Validate block references which are part of inode */
+		ret = ext4_ind_check_inode(inode);
+	}
+	if (ret)
+		goto bad_inode;
+
+	if (S_ISREG(inode->i_mode)) {
+		inode->i_op = &ext4_file_inode_operations;
+		inode->i_fop = &ext4_file_operations;
+		ext4_set_aops(inode);
+	} else if (S_ISDIR(inode->i_mode)) {
+		inode->i_op = &ext4_dir_inode_operations;
+		inode->i_fop = &ext4_dir_operations;
+	} else if (S_ISLNK(inode->i_mode)) {
+		if (ext4_inode_is_fast_symlink(inode)) {
+			inode->i_op = &ext4_fast_symlink_inode_operations;
+			nd_terminate_link(ei->i_data, inode->i_size,
+				sizeof(ei->i_data) - 1);
+		} else {
+			inode->i_op = &ext4_symlink_inode_operations;
+			ext4_set_aops(inode);
+		}
+	} else if (S_ISCHR(inode->i_mode) || S_ISBLK(inode->i_mode) ||
+	      S_ISFIFO(inode->i_mode) || S_ISSOCK(inode->i_mode)) {
+		inode->i_op = &ext4_special_inode_operations;
+		if (raw_inode->i_block[0])
+			init_special_inode(inode, inode->i_mode,
+			   old_decode_dev(le32_to_cpu(raw_inode->i_block[0])));
+		else
+			init_special_inode(inode, inode->i_mode,
+			   new_decode_dev(le32_to_cpu(raw_inode->i_block[1])));
+	} else {
+		ret = -EIO;
+		EXT4_ERROR_INODE(inode, "bogus i_mode (%o)", inode->i_mode);
+		goto bad_inode;
+	}
+	brelse(iloc.bh);
+	ext4_set_inode_flags(inode);
+	unlock_new_inode(inode);
+	return inode;
+
+bad_inode:
+	brelse(iloc.bh);
+	iget_failed(inode);
+	return ERR_PTR(ret);
+}
+
+static int ext4_inode_blocks_set(handle_t *handle,
+				struct ext4_inode *raw_inode,
+				struct ext4_inode_info *ei)
+{
+	struct inode *inode = &(ei->vfs_inode);
+	u64 i_blocks = inode->i_blocks;
+	struct super_block *sb = inode->i_sb;
+
+	if (i_blocks <= ~0U) {
+		/*
+		 * i_blocks can be represnted in a 32 bit variable
+		 * as multiple of 512 bytes
+		 */
+		raw_inode->i_blocks_lo   = cpu_to_le32(i_blocks);
+		raw_inode->i_blocks_high = 0;
+		ext4_clear_inode_flag(inode, EXT4_INODE_HUGE_FILE);
+		return 0;
+	}
+	if (!EXT4_HAS_RO_COMPAT_FEATURE(sb, EXT4_FEATURE_RO_COMPAT_HUGE_FILE))
+		return -EFBIG;
+
+	if (i_blocks <= 0xffffffffffffULL) {
+		/*
+		 * i_blocks can be represented in a 48 bit variable
+		 * as multiple of 512 bytes
+		 */
+		raw_inode->i_blocks_lo   = cpu_to_le32(i_blocks);
+		raw_inode->i_blocks_high = cpu_to_le16(i_blocks >> 32);
+		ext4_clear_inode_flag(inode, EXT4_INODE_HUGE_FILE);
+	} else {
+		ext4_set_inode_flag(inode, EXT4_INODE_HUGE_FILE);
+		/* i_block is stored in file system block size */
+		i_blocks = i_blocks >> (inode->i_blkbits - 9);
+		raw_inode->i_blocks_lo   = cpu_to_le32(i_blocks);
+		raw_inode->i_blocks_high = cpu_to_le16(i_blocks >> 32);
+	}
+	return 0;
+}
+
+/*
+ * Post the struct inode info into an on-disk inode location in the
+ * buffer-cache.  This gobbles the caller's reference to the
+ * buffer_head in the inode location struct.
+ *
+ * The caller must have write access to iloc->bh.
+ */
+static int ext4_do_update_inode(handle_t *handle,
+				struct inode *inode,
+				struct ext4_iloc *iloc)
+{
+	struct ext4_inode *raw_inode = ext4_raw_inode(iloc);
+	struct ext4_inode_info *ei = EXT4_I(inode);
+	struct buffer_head *bh = iloc->bh;
+	int err = 0, rc, block;
+	int need_datasync = 0;
+
+	/* For fields not not tracking in the in-memory inode,
+	 * initialise them to zero for new inodes. */
+	if (ext4_test_inode_state(inode, EXT4_STATE_NEW))
+		memset(raw_inode, 0, EXT4_SB(inode->i_sb)->s_inode_size);
+
+	ext4_get_inode_flags(ei);
+	raw_inode->i_mode = cpu_to_le16(inode->i_mode);
+	if (!(test_opt(inode->i_sb, NO_UID32))) {
+		raw_inode->i_uid_low = cpu_to_le16(low_16_bits(inode->i_uid));
+		raw_inode->i_gid_low = cpu_to_le16(low_16_bits(inode->i_gid));
+/*
+ * Fix up interoperability with old kernels. Otherwise, old inodes get
+ * re-used with the upper 16 bits of the uid/gid intact
+ */
+		if (!ei->i_dtime) {
+			raw_inode->i_uid_high =
+				cpu_to_le16(high_16_bits(inode->i_uid));
+			raw_inode->i_gid_high =
+				cpu_to_le16(high_16_bits(inode->i_gid));
+		} else {
+			raw_inode->i_uid_high = 0;
+			raw_inode->i_gid_high = 0;
+		}
+	} else {
+		raw_inode->i_uid_low =
+			cpu_to_le16(fs_high2lowuid(inode->i_uid));
+		raw_inode->i_gid_low =
+			cpu_to_le16(fs_high2lowgid(inode->i_gid));
+		raw_inode->i_uid_high = 0;
+		raw_inode->i_gid_high = 0;
+	}
+	raw_inode->i_links_count = cpu_to_le16(inode->i_nlink);
+
+	EXT4_INODE_SET_XTIME(i_ctime, inode, raw_inode);
+	EXT4_INODE_SET_XTIME(i_mtime, inode, raw_inode);
+	EXT4_INODE_SET_XTIME(i_atime, inode, raw_inode);
+	EXT4_EINODE_SET_XTIME(i_crtime, ei, raw_inode);
+
+	if (ext4_inode_blocks_set(handle, raw_inode, ei))
+		goto out_brelse;
+	raw_inode->i_dtime = cpu_to_le32(ei->i_dtime);
+	raw_inode->i_flags = cpu_to_le32(ei->i_flags & 0xFFFFFFFF);
+	if (EXT4_SB(inode->i_sb)->s_es->s_creator_os !=
+	    cpu_to_le32(EXT4_OS_HURD))
+		raw_inode->i_file_acl_high =
+			cpu_to_le16(ei->i_file_acl >> 32);
+	raw_inode->i_file_acl_lo = cpu_to_le32(ei->i_file_acl);
+	if (ei->i_disksize != ext4_isize(raw_inode)) {
+		ext4_isize_set(raw_inode, ei->i_disksize);
+		need_datasync = 1;
+	}
+	if (ei->i_disksize > 0x7fffffffULL) {
+		struct super_block *sb = inode->i_sb;
+		if (!EXT4_HAS_RO_COMPAT_FEATURE(sb,
+				EXT4_FEATURE_RO_COMPAT_LARGE_FILE) ||
+				EXT4_SB(sb)->s_es->s_rev_level ==
+				cpu_to_le32(EXT4_GOOD_OLD_REV)) {
+			/* If this is the first large file
+			 * created, add a flag to the superblock.
+			 */
+			err = ext4_journal_get_write_access(handle,
+					EXT4_SB(sb)->s_sbh);
+			if (err)
+				goto out_brelse;
+			ext4_update_dynamic_rev(sb);
+			EXT4_SET_RO_COMPAT_FEATURE(sb,
+					EXT4_FEATURE_RO_COMPAT_LARGE_FILE);
+			sb->s_dirt = 1;
+			ext4_handle_sync(handle);
+			err = ext4_handle_dirty_metadata(handle, NULL,
+					EXT4_SB(sb)->s_sbh);
+		}
+	}
+	raw_inode->i_generation = cpu_to_le32(inode->i_generation);
+	if (S_ISCHR(inode->i_mode) || S_ISBLK(inode->i_mode)) {
+		if (old_valid_dev(inode->i_rdev)) {
+			raw_inode->i_block[0] =
+				cpu_to_le32(old_encode_dev(inode->i_rdev));
+			raw_inode->i_block[1] = 0;
+		} else {
+			raw_inode->i_block[0] = 0;
+			raw_inode->i_block[1] =
+				cpu_to_le32(new_encode_dev(inode->i_rdev));
+			raw_inode->i_block[2] = 0;
+		}
+	} else
+		for (block = 0; block < EXT4_N_BLOCKS; block++)
+			raw_inode->i_block[block] = ei->i_data[block];
+
+	raw_inode->i_disk_version = cpu_to_le32(inode->i_version);
+	if (ei->i_extra_isize) {
+		if (EXT4_FITS_IN_INODE(raw_inode, ei, i_version_hi))
+			raw_inode->i_version_hi =
+			cpu_to_le32(inode->i_version >> 32);
+		raw_inode->i_extra_isize = cpu_to_le16(ei->i_extra_isize);
+	}
+
+	BUFFER_TRACE(bh, "call ext4_handle_dirty_metadata");
+	rc = ext4_handle_dirty_metadata(handle, NULL, bh);
+	if (!err)
+		err = rc;
+	ext4_clear_inode_state(inode, EXT4_STATE_NEW);
+
+	ext4_update_inode_fsync_trans(handle, inode, need_datasync);
+out_brelse:
+	brelse(bh);
+	ext4_std_error(inode->i_sb, err);
+	return err;
+}
+
+/*
+ * ext4_write_inode()
+ *
+ * We are called from a few places:
+ *
+ * - Within generic_file_write() for O_SYNC files.
+ *   Here, there will be no transaction running. We wait for any running
+ *   trasnaction to commit.
+ *
+ * - Within sys_sync(), kupdate and such.
+ *   We wait on commit, if tol to.
+ *
+ * - Within prune_icache() (PF_MEMALLOC == true)
+ *   Here we simply return.  We can't afford to block kswapd on the
+ *   journal commit.
+ *
+ * In all cases it is actually safe for us to return without doing anything,
+ * because the inode has been copied into a raw inode buffer in
+ * ext4_mark_inode_dirty().  This is a correctness thing for O_SYNC and for
+ * knfsd.
+ *
+ * Note that we are absolutely dependent upon all inode dirtiers doing the
+ * right thing: they *must* call mark_inode_dirty() after dirtying info in
+ * which we are interested.
+ *
+ * It would be a bug for them to not do this.  The code:
+ *
+ *	mark_inode_dirty(inode)
+ *	stuff();
+ *	inode->i_size = expr;
+ *
+ * is in error because a kswapd-driven write_inode() could occur while
+ * `stuff()' is running, and the new i_size will be lost.  Plus the inode
+ * will no longer be on the superblock's dirty inode list.
+ */
+int ext4_write_inode(struct inode *inode, struct writeback_control *wbc)
+{
+	int err;
+
+	if (current->flags & PF_MEMALLOC)
+		return 0;
+
+	if (EXT4_SB(inode->i_sb)->s_journal) {
+		if (ext4_journal_current_handle()) {
+			jbd_debug(1, "called recursively, non-PF_MEMALLOC!\n");
+			dump_stack();
+			return -EIO;
+		}
+
+		if (wbc->sync_mode != WB_SYNC_ALL)
+			return 0;
+
+		err = ext4_force_commit(inode->i_sb);
+	} else {
+		struct ext4_iloc iloc;
+
+		err = __ext4_get_inode_loc(inode, &iloc, 0);
+		if (err)
+			return err;
+		if (wbc->sync_mode == WB_SYNC_ALL)
+			sync_dirty_buffer(iloc.bh);
+		if (buffer_req(iloc.bh) && !buffer_uptodate(iloc.bh)) {
+			EXT4_ERROR_INODE_BLOCK(inode, iloc.bh->b_blocknr,
+					 "IO error syncing inode");
+			err = -EIO;
+		}
+		brelse(iloc.bh);
+	}
+	return err;
+}
+
+/*
+ * ext4_setattr()
+ *
+ * Called from notify_change.
+ *
+ * We want to trap VFS attempts to truncate the file as soon as
+ * possible.  In particular, we want to make sure that when the VFS
+ * shrinks i_size, we put the inode on the orphan list and modify
+ * i_disksize immediately, so that during the subsequent flushing of
+ * dirty pages and freeing of disk blocks, we can guarantee that any
+ * commit will leave the blocks being flushed in an unused state on
+ * disk.  (On recovery, the inode will get truncated and the blocks will
+ * be freed, so we have a strong guarantee that no future commit will
+ * leave these blocks visible to the user.)
+ *
+ * Another thing we have to assure is that if we are in ordered mode
+ * and inode is still attached to the committing transaction, we must
+ * we start writeout of all the dirty pages which are being truncated.
+ * This way we are sure that all the data written in the previous
+ * transaction are already on disk (truncate waits for pages under
+ * writeback).
+ *
+ * Called with inode->i_mutex down.
+ */
+int ext4_setattr(struct dentry *dentry, struct iattr *attr)
+{
+	struct inode *inode = dentry->d_inode;
+	int error, rc = 0;
+	int orphan = 0;
+	const unsigned int ia_valid = attr->ia_valid;
+
+	error = inode_change_ok(inode, attr);
+	if (error)
+		return error;
+
+	if (is_quota_modification(inode, attr))
+		dquot_initialize(inode);
+	if ((ia_valid & ATTR_UID && attr->ia_uid != inode->i_uid) ||
+		(ia_valid & ATTR_GID && attr->ia_gid != inode->i_gid)) {
+		handle_t *handle;
+
+		/* (user+group)*(old+new) structure, inode write (sb,
+		 * inode block, ? - but truncate inode update has it) */
+		handle = ext4_journal_start(inode, (EXT4_MAXQUOTAS_INIT_BLOCKS(inode->i_sb)+
+					EXT4_MAXQUOTAS_DEL_BLOCKS(inode->i_sb))+3);
+		if (IS_ERR(handle)) {
+			error = PTR_ERR(handle);
+			goto err_out;
+		}
+		error = dquot_transfer(inode, attr);
+		if (error) {
+			ext4_journal_stop(handle);
+			return error;
+		}
+		/* Update corresponding info in inode so that everything is in
+		 * one transaction */
+		if (attr->ia_valid & ATTR_UID)
+			inode->i_uid = attr->ia_uid;
+		if (attr->ia_valid & ATTR_GID)
+			inode->i_gid = attr->ia_gid;
+		error = ext4_mark_inode_dirty(handle, inode);
+		ext4_journal_stop(handle);
+	}
+
+	if (attr->ia_valid & ATTR_SIZE) {
+		inode_dio_wait(inode);
+
+		if (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) {
+			struct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);
+
+			if (attr->ia_size > sbi->s_bitmap_maxbytes)
+				return -EFBIG;
+		}
+	}
+
+	if (S_ISREG(inode->i_mode) &&
+	    attr->ia_valid & ATTR_SIZE &&
+	    (attr->ia_size < inode->i_size)) {
+		handle_t *handle;
+
+		handle = ext4_journal_start(inode, 3);
+		if (IS_ERR(handle)) {
+			error = PTR_ERR(handle);
+			goto err_out;
+		}
+		if (ext4_handle_valid(handle)) {
+			error = ext4_orphan_add(handle, inode);
+			orphan = 1;
+		}
+		EXT4_I(inode)->i_disksize = attr->ia_size;
+		rc = ext4_mark_inode_dirty(handle, inode);
+		if (!error)
+			error = rc;
+		ext4_journal_stop(handle);
+
+		if (ext4_should_order_data(inode)) {
+			error = ext4_begin_ordered_truncate(inode,
+							    attr->ia_size);
+			if (error) {
+				/* Do as much error cleanup as possible */
+				handle = ext4_journal_start(inode, 3);
+				if (IS_ERR(handle)) {
+					ext4_orphan_del(NULL, inode);
+					goto err_out;
+				}
+				ext4_orphan_del(handle, inode);
+				orphan = 0;
+				ext4_journal_stop(handle);
+				goto err_out;
+			}
+		}
+	}
+
+	if (attr->ia_valid & ATTR_SIZE) {
+		if (attr->ia_size != i_size_read(inode)) {
+			truncate_setsize(inode, attr->ia_size);
+			ext4_truncate(inode);
+		} else if (ext4_test_inode_flag(inode, EXT4_INODE_EOFBLOCKS))
+			ext4_truncate(inode);
+	}
+
+	if (!rc) {
+		setattr_copy(inode, attr);
+		mark_inode_dirty(inode);
+	}
+
+	/*
+	 * If the call to ext4_truncate failed to get a transaction handle at
+	 * all, we need to clean up the in-core orphan list manually.
+	 */
+	if (orphan && inode->i_nlink)
+		ext4_orphan_del(NULL, inode);
+
+	if (!rc && (ia_valid & ATTR_MODE))
+		rc = ext4_acl_chmod(inode);
+
+err_out:
+	ext4_std_error(inode->i_sb, error);
+	if (!error)
+		error = rc;
+	return error;
+}
+
+int ext4_getattr(struct vfsmount *mnt, struct dentry *dentry,
+		 struct kstat *stat)
+{
+	struct inode *inode;
+	unsigned long long delalloc_blocks;
+
+	inode = dentry->d_inode;
+	generic_fillattr(inode, stat);
+
+	/*
+	 * We can't update i_blocks if the block allocation is delayed
+	 * otherwise in the case of system crash before the real block
+	 * allocation is done, we will have i_blocks inconsistent with
+	 * on-disk file blocks.
+	 * We always keep i_blocks updated together with real
+	 * allocation. But to not confuse with user, stat
+	 * will return the blocks that include the delayed allocation
+	 * blocks for this file.
+	 */
+	delalloc_blocks = EXT4_I(inode)->i_reserved_data_blocks;
+
+	stat->blocks += delalloc_blocks << (inode->i_sb->s_blocksize_bits-9);
+	return 0;
+}
+
+static int ext4_index_trans_blocks(struct inode *inode, int nrblocks, int chunk)
+{
+	if (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)))
+		return ext4_ind_trans_blocks(inode, nrblocks, chunk);
+	return ext4_ext_index_trans_blocks(inode, nrblocks, chunk);
+}
+
+/*
+ * Account for index blocks, block groups bitmaps and block group
+ * descriptor blocks if modify datablocks and index blocks
+ * worse case, the indexs blocks spread over different block groups
+ *
+ * If datablocks are discontiguous, they are possible to spread over
+ * different block groups too. If they are contiuguous, with flexbg,
+ * they could still across block group boundary.
+ *
+ * Also account for superblock, inode, quota and xattr blocks
+ */
+static int ext4_meta_trans_blocks(struct inode *inode, int nrblocks, int chunk)
+{
+	ext4_group_t groups, ngroups = ext4_get_groups_count(inode->i_sb);
+	int gdpblocks;
+	int idxblocks;
+	int ret = 0;
+
+	/*
+	 * How many index blocks need to touch to modify nrblocks?
+	 * The "Chunk" flag indicating whether the nrblocks is
+	 * physically contiguous on disk
+	 *
+	 * For Direct IO and fallocate, they calls get_block to allocate
+	 * one single extent at a time, so they could set the "Chunk" flag
+	 */
+	idxblocks = ext4_index_trans_blocks(inode, nrblocks, chunk);
+
+	ret = idxblocks;
+
+	/*
+	 * Now let's see how many group bitmaps and group descriptors need
+	 * to account
+	 */
+	groups = idxblocks;
+	if (chunk)
+		groups += 1;
+	else
+		groups += nrblocks;
+
+	gdpblocks = groups;
+	if (groups > ngroups)
+		groups = ngroups;
+	if (groups > EXT4_SB(inode->i_sb)->s_gdb_count)
+		gdpblocks = EXT4_SB(inode->i_sb)->s_gdb_count;
+
+	/* bitmaps and block group descriptor blocks */
+	ret += groups + gdpblocks;
+
+	/* Blocks for super block, inode, quota and xattr blocks */
+	ret += EXT4_META_TRANS_BLOCKS(inode->i_sb);
+
+	return ret;
+}
+
+/*
+ * Calculate the total number of credits to reserve to fit
+ * the modification of a single pages into a single transaction,
+ * which may include multiple chunks of block allocations.
+ *
+ * This could be called via ext4_write_begin()
+ *
+ * We need to consider the worse case, when
+ * one new block per extent.
+ */
+int ext4_writepage_trans_blocks(struct inode *inode)
+{
+	int bpp = ext4_journal_blocks_per_page(inode);
+	int ret;
+
+	ret = ext4_meta_trans_blocks(inode, bpp, 0);
+
+	/* Account for data blocks for journalled mode */
+	if (ext4_should_journal_data(inode))
+		ret += bpp;
+	return ret;
+}
+
+/*
+ * Calculate the journal credits for a chunk of data modification.
+ *
+ * This is called from DIO, fallocate or whoever calling
+ * ext4_map_blocks() to map/allocate a chunk of contiguous disk blocks.
+ *
+ * journal buffers for data blocks are not included here, as DIO
+ * and fallocate do no need to journal data buffers.
+ */
+int ext4_chunk_trans_blocks(struct inode *inode, int nrblocks)
+{
+	return ext4_meta_trans_blocks(inode, nrblocks, 1);
+}
+
+/*
+ * The caller must have previously called ext4_reserve_inode_write().
+ * Give this, we know that the caller already has write access to iloc->bh.
+ */
+int ext4_mark_iloc_dirty(handle_t *handle,
+			 struct inode *inode, struct ext4_iloc *iloc)
+{
+	int err = 0;
+
+	if (test_opt(inode->i_sb, I_VERSION))
+		inode_inc_iversion(inode);
+
+	/* the do_update_inode consumes one bh->b_count */
+	get_bh(iloc->bh);
+
+	/* ext4_do_update_inode() does jbd2_journal_dirty_metadata */
+	err = ext4_do_update_inode(handle, inode, iloc);
+	put_bh(iloc->bh);
+	return err;
+}
+
+/*
+ * On success, We end up with an outstanding reference count against
+ * iloc->bh.  This _must_ be cleaned up later.
+ */
+
+int
+ext4_reserve_inode_write(handle_t *handle, struct inode *inode,
+			 struct ext4_iloc *iloc)
+{
+	int err;
+
+	err = ext4_get_inode_loc(inode, iloc);
+	if (!err) {
+		BUFFER_TRACE(iloc->bh, "get_write_access");
+		err = ext4_journal_get_write_access(handle, iloc->bh);
+		if (err) {
+			brelse(iloc->bh);
+			iloc->bh = NULL;
+		}
+	}
+	ext4_std_error(inode->i_sb, err);
+	return err;
+}
+
+/*
+ * Expand an inode by new_extra_isize bytes.
+ * Returns 0 on success or negative error number on failure.
+ */
+static int ext4_expand_extra_isize(struct inode *inode,
+				   unsigned int new_extra_isize,
+				   struct ext4_iloc iloc,
+				   handle_t *handle)
+{
+	struct ext4_inode *raw_inode;
+	struct ext4_xattr_ibody_header *header;
+
+	if (EXT4_I(inode)->i_extra_isize >= new_extra_isize)
+		return 0;
+
+	raw_inode = ext4_raw_inode(&iloc);
+
+	header = IHDR(inode, raw_inode);
+
+	/* No extended attributes present */
+	if (!ext4_test_inode_state(inode, EXT4_STATE_XATTR) ||
+	    header->h_magic != cpu_to_le32(EXT4_XATTR_MAGIC)) {
+		memset((void *)raw_inode + EXT4_GOOD_OLD_INODE_SIZE, 0,
+			new_extra_isize);
+		EXT4_I(inode)->i_extra_isize = new_extra_isize;
+		return 0;
+	}
+
+	/* try to expand with EAs present */
+	return ext4_expand_extra_isize_ea(inode, new_extra_isize,
+					  raw_inode, handle);
+}
+
+/*
+ * What we do here is to mark the in-core inode as clean with respect to inode
+ * dirtiness (it may still be data-dirty).
+ * This means that the in-core inode may be reaped by prune_icache
+ * without having to perform any I/O.  This is a very good thing,
+ * because *any* task may call prune_icache - even ones which
+ * have a transaction open against a different journal.
+ *
+ * Is this cheating?  Not really.  Sure, we haven't written the
+ * inode out, but prune_icache isn't a user-visible syncing function.
+ * Whenever the user wants stuff synced (sys_sync, sys_msync, sys_fsync)
+ * we start and wait on commits.
+ *
+ * Is this efficient/effective?  Well, we're being nice to the system
+ * by cleaning up our inodes proactively so they can be reaped
+ * without I/O.  But we are potentially leaving up to five seconds'
+ * worth of inodes floating about which prune_icache wants us to
+ * write out.  One way to fix that would be to get prune_icache()
+ * to do a write_super() to free up some memory.  It has the desired
+ * effect.
+ */
+int ext4_mark_inode_dirty(handle_t *handle, struct inode *inode)
+{
+	struct ext4_iloc iloc;
+	struct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);
+	static unsigned int mnt_count;
+	int err, ret;
+
+	might_sleep();
+	trace_ext4_mark_inode_dirty(inode, _RET_IP_);
+	err = ext4_reserve_inode_write(handle, inode, &iloc);
+	if (ext4_handle_valid(handle) &&
+	    EXT4_I(inode)->i_extra_isize < sbi->s_want_extra_isize &&
+	    !ext4_test_inode_state(inode, EXT4_STATE_NO_EXPAND)) {
+		/*
+		 * We need extra buffer credits since we may write into EA block
+		 * with this same handle. If journal_extend fails, then it will
+		 * only result in a minor loss of functionality for that inode.
+		 * If this is felt to be critical, then e2fsck should be run to
+		 * force a large enough s_min_extra_isize.
+		 */
+		if ((jbd2_journal_extend(handle,
+			     EXT4_DATA_TRANS_BLOCKS(inode->i_sb))) == 0) {
+			ret = ext4_expand_extra_isize(inode,
+						      sbi->s_want_extra_isize,
+						      iloc, handle);
+			if (ret) {
+				ext4_set_inode_state(inode,
+						     EXT4_STATE_NO_EXPAND);
+				if (mnt_count !=
+					le16_to_cpu(sbi->s_es->s_mnt_count)) {
+					ext4_warning(inode->i_sb,
+					"Unable to expand inode %lu. Delete"
+					" some EAs or run e2fsck.",
+					inode->i_ino);
+					mnt_count =
+					  le16_to_cpu(sbi->s_es->s_mnt_count);
+				}
+			}
+		}
+	}
+	if (!err)
+		err = ext4_mark_iloc_dirty(handle, inode, &iloc);
+	return err;
+}
+
+/*
+ * ext4_dirty_inode() is called from __mark_inode_dirty()
+ *
+ * We're really interested in the case where a file is being extended.
+ * i_size has been changed by generic_commit_write() and we thus need
+ * to include the updated inode in the current transaction.
+ *
+ * Also, dquot_alloc_block() will always dirty the inode when blocks
+ * are allocated to the file.
+ *
+ * If the inode is marked synchronous, we don't honour that here - doing
+ * so would cause a commit on atime updates, which we don't bother doing.
+ * We handle synchronous inodes at the highest possible level.
+ */
+void ext4_dirty_inode(struct inode *inode, int flags)
+{
+	handle_t *handle;
+
+	handle = ext4_journal_start(inode, 2);
+	if (IS_ERR(handle))
+		goto out;
+
+	ext4_mark_inode_dirty(handle, inode);
+
+	ext4_journal_stop(handle);
+out:
+	return;
+}
+
+#if 0
+/*
+ * Bind an inode's backing buffer_head into this transaction, to prevent
+ * it from being flushed to disk early.  Unlike
+ * ext4_reserve_inode_write, this leaves behind no bh reference and
+ * returns no iloc structure, so the caller needs to repeat the iloc
+ * lookup to mark the inode dirty later.
+ */
+static int ext4_pin_inode(handle_t *handle, struct inode *inode)
+{
+	struct ext4_iloc iloc;
+
+	int err = 0;
+	if (handle) {
+		err = ext4_get_inode_loc(inode, &iloc);
+		if (!err) {
+			BUFFER_TRACE(iloc.bh, "get_write_access");
+			err = jbd2_journal_get_write_access(handle, iloc.bh);
+			if (!err)
+				err = ext4_handle_dirty_metadata(handle,
+								 NULL,
+								 iloc.bh);
+			brelse(iloc.bh);
+		}
+	}
+	ext4_std_error(inode->i_sb, err);
+	return err;
+}
+#endif
+
+int ext4_change_inode_journal_flag(struct inode *inode, int val)
+{
+	journal_t *journal;
+	handle_t *handle;
+	int err;
+
+	/*
+	 * We have to be very careful here: changing a data block's
+	 * journaling status dynamically is dangerous.  If we write a
+	 * data block to the journal, change the status and then delete
+	 * that block, we risk forgetting to revoke the old log record
+	 * from the journal and so a subsequent replay can corrupt data.
+	 * So, first we make sure that the journal is empty and that
+	 * nobody is changing anything.
+	 */
+
+	journal = EXT4_JOURNAL(inode);
+	if (!journal)
+		return 0;
+	if (is_journal_aborted(journal))
+		return -EROFS;
+
+	jbd2_journal_lock_updates(journal);
+	jbd2_journal_flush(journal);
+
+	/*
+	 * OK, there are no updates running now, and all cached data is
+	 * synced to disk.  We are now in a completely consistent state
+	 * which doesn't have anything in the journal, and we know that
+	 * no filesystem updates are running, so it is safe to modify
+	 * the inode's in-core data-journaling state flag now.
+	 */
+
+	if (val)
+		ext4_set_inode_flag(inode, EXT4_INODE_JOURNAL_DATA);
+	else
+		ext4_clear_inode_flag(inode, EXT4_INODE_JOURNAL_DATA);
+	ext4_set_aops(inode);
+
+	jbd2_journal_unlock_updates(journal);
+
+	/* Finally we can mark the inode as dirty. */
+
+	handle = ext4_journal_start(inode, 1);
+	if (IS_ERR(handle))
+		return PTR_ERR(handle);
+
+	err = ext4_mark_inode_dirty(handle, inode);
+	ext4_handle_sync(handle);
+	ext4_journal_stop(handle);
+	ext4_std_error(inode->i_sb, err);
+
+	return err;
+}
+
+static int ext4_bh_unmapped(handle_t *handle, struct buffer_head *bh)
+{
+	return !buffer_mapped(bh);
+}
+
+int ext4_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)
+{
+	struct page *page = vmf->page;
+	loff_t size;
+	unsigned long len;
+	int ret;
+	struct file *file = vma->vm_file;
+	struct inode *inode = file->f_path.dentry->d_inode;
+	struct address_space *mapping = inode->i_mapping;
+	handle_t *handle;
+	get_block_t *get_block;
+	int retries = 0;
+
+	/*
+	 * This check is racy but catches the common case. We rely on
+	 * __block_page_mkwrite() to do a reliable check.
+	 */
+	vfs_check_frozen(inode->i_sb, SB_FREEZE_WRITE);
+	/* Delalloc case is easy... */
+	if (test_opt(inode->i_sb, DELALLOC) &&
+	    !ext4_should_journal_data(inode) &&
+	    !ext4_nonda_switch(inode->i_sb)) {
+		do {
+			ret = __block_page_mkwrite(vma, vmf,
+						   ext4_da_get_block_prep);
+		} while (ret == -ENOSPC &&
+		       ext4_should_retry_alloc(inode->i_sb, &retries));
+		goto out_ret;
+	}
+
+	lock_page(page);
+	size = i_size_read(inode);
+	/* Page got truncated from under us? */
+	if (page->mapping != mapping || page_offset(page) > size) {
+		unlock_page(page);
+		ret = VM_FAULT_NOPAGE;
+		goto out;
+	}
+
+	if (page->index == size >> PAGE_CACHE_SHIFT)
+		len = size & ~PAGE_CACHE_MASK;
+	else
+		len = PAGE_CACHE_SIZE;
+	/*
+	 * Return if we have all the buffers mapped. This avoids the need to do
+	 * journal_start/journal_stop which can block and take a long time
+	 */
+	if (page_has_buffers(page)) {
+		if (!walk_page_buffers(NULL, page_buffers(page), 0, len, NULL,
+					ext4_bh_unmapped)) {
+			/* Wait so that we don't change page under IO */
+			wait_on_page_writeback(page);
+			ret = VM_FAULT_LOCKED;
+			goto out;
+		}
+	}
+	unlock_page(page);
+	/* OK, we need to fill the hole... */
+	if (ext4_should_dioread_nolock(inode))
+		get_block = ext4_get_block_write;
+	else
+		get_block = ext4_get_block;
+retry_alloc:
+	handle = ext4_journal_start(inode, ext4_writepage_trans_blocks(inode));
+	if (IS_ERR(handle)) {
+		ret = VM_FAULT_SIGBUS;
+		goto out;
+	}
+	ret = __block_page_mkwrite(vma, vmf, get_block);
+	if (!ret && ext4_should_journal_data(inode)) {
+		if (walk_page_buffers(handle, page_buffers(page), 0,
+			  PAGE_CACHE_SIZE, NULL, do_journal_get_write_access)) {
+			unlock_page(page);
+			ret = VM_FAULT_SIGBUS;
+			ext4_journal_stop(handle);
+			goto out;
+		}
+		ext4_set_inode_state(inode, EXT4_STATE_JDATA);
+	}
+	ext4_journal_stop(handle);
+	if (ret == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries))
+		goto retry_alloc;
+out_ret:
+	ret = block_page_mkwrite_return(ret);
+out:
+	return ret;
+}
diff -rubBN linux-3.2.51/fs/xfs/xfs_aops.c ../split-release/release/linux-3.2.51/fs/xfs/xfs_aops.c
--- linux-3.2.51/fs/xfs/xfs_aops.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/fs/xfs/xfs_aops.c	2015-09-01 05:42:49.000000000 +0200
@@ -38,6 +38,8 @@
 #include <linux/pagevec.h>
 #include <linux/writeback.h>
 
+#include <linux/cause_tags.h>
+
 void
 xfs_count_page_state(
 	struct page		*page,
@@ -441,6 +443,7 @@
 
 static inline int bio_add_buffer(struct bio *bio, struct buffer_head *bh)
 {
+	move_causes_bh_to_bio(bh, bio);
 	return bio_add_page(bio, bh->b_page, bh->b_size, bh_offset(bh));
 }
 
diff -rubBN linux-3.2.51/fs/xfs/xfs_inode.c ../split-release/release/linux-3.2.51/fs/xfs/xfs_inode.c
--- linux-3.2.51/fs/xfs/xfs_inode.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/fs/xfs/xfs_inode.c	2015-09-01 05:42:49.000000000 +0200
@@ -1372,6 +1372,8 @@
 		if (ip->i_size != new_size) {
 			ip->i_d.di_size = new_size;
 			ip->i_size = new_size;
+			//yangsuli
+			//here dirties data...
 			xfs_trans_log_inode(*tpp, ip, XFS_ILOG_CORE);
 		}
 	}
@@ -1478,6 +1480,11 @@
 	agi->agi_unlinked[bucket_index] = cpu_to_be32(agino);
 	offset = offsetof(xfs_agi_t, agi_unlinked) +
 		(sizeof(xfs_agino_t) * bucket_index);
+	//yangsuli
+	//here we dirty
+	//but note that here the dirtier created/modified bp?
+	//is it true that who ever calls trans_log_buf dirtied the buf?
+	//possible...
 	xfs_trans_log_buf(tp, agibp, offset,
 			  (offset + sizeof(xfs_agino_t) - 1));
 	return 0;
@@ -2593,6 +2600,7 @@
 	if (error)
 		goto cluster_corrupt_out;
 
+	//yangsuli: here dirty
 	if (flags & SYNC_WAIT)
 		error = xfs_bwrite(bp);
 	else
diff -rubBN linux-3.2.51/fs/xfs/xfs_vnodeops.c ../split-release/release/linux-3.2.51/fs/xfs/xfs_vnodeops.c
--- linux-3.2.51/fs/xfs/xfs_vnodeops.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/fs/xfs/xfs_vnodeops.c	2015-09-01 05:42:49.000000000 +0200
@@ -946,6 +946,8 @@
 	 * the transaction cancel unlocking dp so don't do it explicitly in the
 	 * error path.
 	 */
+	//yangsuli
+	//here is where metadata actually got dirtied....
 	xfs_trans_ijoin(tp, dp, XFS_ILOCK_EXCL);
 	unlock_dp_on_error = B_FALSE;
 
diff -rubBN linux-3.2.51/.gitignore ../split-release/release/linux-3.2.51/.gitignore
--- linux-3.2.51/.gitignore	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/.gitignore	2015-09-01 05:42:29.000000000 +0200
@@ -84,3 +84,5 @@
 *.orig
 *~
 \#*#
+/build.sh
+/install.sh
diff -rubBN linux-3.2.51/include/linux/blkdev.h ../split-release/release/linux-3.2.51/include/linux/blkdev.h
--- linux-3.2.51/include/linux/blkdev.h	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/include/linux/blkdev.h	2015-09-01 05:42:28.000000000 +0200
@@ -8,6 +8,7 @@
 #include <linux/genhd.h>
 #include <linux/list.h>
 #include <linux/timer.h>
+#include <linux/module.h>
 #include <linux/workqueue.h>
 #include <linux/pagemap.h>
 #include <linux/backing-dev.h>
@@ -18,6 +19,7 @@
 #include <linux/gfp.h>
 #include <linux/bsg.h>
 #include <linux/smp.h>
+#include <linux/module.h>
 
 #include <asm/scatterlist.h>
 
@@ -106,6 +108,7 @@
 		struct rb_node rb_node;	/* sort/lookup */
 		void *completion_data;
 	};
+	struct rb_node expire_rb_node;
 
 	/*
 	 * Three pointers are available for the IO schedulers, if they need
@@ -387,6 +390,9 @@
 	/* Throttle data */
 	struct throtl_data *td;
 #endif
+
+	// this allows us to identify when schedulers have switched
+	int sched_uniq;
 };
 
 #define QUEUE_FLAG_QUEUED	1	/* uses generic tag queueing */
@@ -1358,4 +1364,21 @@
 
 #endif /* CONFIG_BLOCK */
 
+#include <linux/elevator.h>
+struct request_queue;
+
+struct request_queue *inode_to_request_queue(struct inode *inode);
+
+void get_elevator_call_info_from_inode(struct inode* inode,
+										  struct request_queue **rq,
+										  struct module **module,
+										  struct elevator_syscall_ops *sops); 
+
+
+void get_elevator_call_info(struct file* filp,
+										  struct request_queue **rq,
+										  struct module **module,
+										  struct elevator_syscall_ops *sops); 
+
+
 #endif
diff -rubBN linux-3.2.51/include/linux/blk_types.h ../split-release/release/linux-3.2.51/include/linux/blk_types.h
--- linux-3.2.51/include/linux/blk_types.h	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/include/linux/blk_types.h	2015-09-01 05:42:28.000000000 +0200
@@ -72,6 +72,8 @@
 
 	bio_destructor_t	*bi_destructor;	/* destructor */
 
+	struct cause_list_list *cll;
+
 	/*
 	 * We can inline a number of vecs at the end of the bio, to avoid
 	 * double allocations for a small number of bio_vecs. This member
diff -rubBN linux-3.2.51/include/linux/buffer_head.h ../split-release/release/linux-3.2.51/include/linux/buffer_head.h
--- linux-3.2.51/include/linux/buffer_head.h	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/include/linux/buffer_head.h	2015-09-01 05:42:28.000000000 +0200
@@ -72,6 +72,9 @@
 	struct address_space *b_assoc_map;	/* mapping this buffer is
 						   associated with */
 	atomic_t b_count;		/* users using this buffer_head */
+
+	spinlock_t causes_lock;
+	struct cause_list* causes;
 };
 
 /*
diff -rubBN linux-3.2.51/include/linux/cause_tags.h ../split-release/release/linux-3.2.51/include/linux/cause_tags.h
--- linux-3.2.51/include/linux/cause_tags.h	1970-01-01 01:00:00.000000000 +0100
+++ ../split-release/release/linux-3.2.51/include/linux/cause_tags.h	2015-09-01 05:42:28.000000000 +0200
@@ -0,0 +1,141 @@
+#ifndef _LINUX_CAUSE_TAGS_H
+#define _LINUX_CAUSE_TAGS_H
+
+#include <linux/list.h>
+#include <linux/kref.h>
+
+
+// uncomment this to disable (nearly) all cause_list
+// code for debugging purposes.
+//
+// #define DISABLE_CAUSES
+
+// uncomment this to disable alloc/free counters
+//#define DISABLE_CAUSES_ALLOC_COUNTERS
+
+// uncomment this to disable magic checks and line tracking for new
+#define DISABLE_CAUSES_DEBUG
+
+struct buffer_head;
+struct bio;
+#define SPLIT_UNKNOWN       (0)
+#define SPLIT_DATA          (1)
+#define SPLIT_JOURNAL       (2)
+#define SPLIT_CHECKPOINT    (3)
+#define SPLIT_ZERO          (4)
+#define SPLIT_JOURNAL_META  (5)
+
+#define CAUSES_MAGIC_GOOD 0xAAAAAAAA
+#define CAUSES_MAGIC_BAD  0xBBBBBBBB
+
+struct io_cause {
+	struct list_head list;
+	int account_id;
+};
+
+struct cause_list_mem_desc{
+	int cause_list_alloc;
+	int cause_list_free;
+	int cause_item_alloc;
+	int cause_item_free;
+};
+
+struct cause_list {
+#ifndef DISABLE_CAUSES_DEBUG
+	unsigned magic;
+	int new_line;
+#endif
+
+	struct kref refcount;
+	int type;                  // (e.g., SPLIT_META)
+	struct list_head items;    // list of io_causes's
+	int item_count;
+
+	// assume one request_queue per buffer.
+	//
+	// when a cause_list is free'd, we may notify the scheduler.
+	// but only if
+	//    callback_q != NULL
+    //   AND
+	//    callback_q->sched_uniq == sched_uniq
+	struct request_queue *callback_q;
+	int sched_uniq;
+	size_t size;
+
+	struct inode* inode;
+
+	// for scheduler to as it pleases
+	long private; // (maybe use as cost estimate)
+};
+
+struct cause_list_list {
+	struct cause_list* uniq_causes;
+	size_t size;
+	int item_count;
+	int item_capacity;
+	struct cause_list *items[0]; // must be last field!
+};
+
+// a cause list with a lock to be used with pages
+struct cause_list_locked {
+    spinlock_t lock;
+    struct cause_list* causes;
+};
+
+#define new_cause_list() __new_cause_list(__LINE__)
+
+#ifndef DISABLE_CAUSES
+
+#ifndef DISABLE_CAUSES_DEBUG
+int cause_list_check(struct cause_list *);
+#else
+static inline int cause_list_check(struct cause_list *cl) { return 0; }
+#endif
+
+void cause_list_debug(void);
+// all of these can accept a NULL cause_list; in this case, they do
+// nothing.  new_cause_list() returns NULL if kmalloc fails.
+struct cause_list* __new_cause_list(int line);
+struct cause_list* get_cause_list(struct cause_list*);
+void put_cause_list(struct cause_list*);
+void put_cause_list_safe(struct cause_list*);
+void add_causes_zero_to_bio(struct bio *, int);
+void move_causes_bh_to_bio(struct buffer_head *, struct bio *);
+int __cause_list_add(struct cause_list*, int);
+int cause_list_add(struct cause_list**, struct task_struct*);
+int cause_list_copy(struct cause_list*, struct cause_list**);
+void set_cause_list_type(struct cause_list*, int);
+
+struct cause_list_list* new_cause_list_list(int item_capacity);
+void del_cause_list_list(struct cause_list_list*);
+void cause_list_list_add(struct cause_list_list*, struct cause_list*);
+
+struct cause_list_mem_desc get_cause_list_mem(void);
+
+#else
+
+// for debugging
+static int cause_list_check(struct cause_list *cl) { return 0; }
+static void cause_list_debug(void) {};
+static struct cause_list* __new_cause_list(int line) { return NULL; }
+static struct cause_list* get_cause_list(struct cause_list* c) { return NULL; }
+static void put_cause_list(struct cause_list* c) {}
+static void put_cause_list_safe(struct cause_list* c) {}
+static void add_causes_zero_to_bio(struct bio *bio, int size) {}
+static void move_causes_bh_to_bio(struct buffer_head *buf,
+										 struct bio *bio) {}
+static int __cause_list_add(struct cause_list* cause_list, int account_id) { return 0; }
+static int cause_list_add(struct cause_list** c,
+							  struct task_struct* t) { return 0;}
+static int cause_list_copy(struct cause_list* from,
+								  struct cause_list** to) { return 0;}
+static void set_cause_list_type(struct cause_list* cl, int type) {}
+
+static struct cause_list_list* new_cause_list_list(int item_capacity) { return NULL; }
+static void del_cause_list_list(struct cause_list_list *cll) {}
+static void cause_list_list_add(struct cause_list_list *cll,
+								struct cause_list *cl) {}
+static struct cause_list_mem_desc get_cause_list_mem(void) {struct cause_list_mem_desc desc; return desc; }
+#endif
+
+#endif
diff -rubBN linux-3.2.51/include/linux/elevator.h ../split-release/release/linux-3.2.51/include/linux/elevator.h
--- linux-3.2.51/include/linux/elevator.h	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/include/linux/elevator.h	2015-09-01 05:42:28.000000000 +0200
@@ -32,6 +32,73 @@
 typedef void *(elevator_init_fn) (struct request_queue *);
 typedef void (elevator_exit_fn) (struct elevator_queue *);
 
+typedef ssize_t (elevator_read_entry_fn) (struct request_queue *rq,
+										  struct file* filp,
+										  size_t count,
+										  loff_t *pos,
+										  void** opaque,
+										  int sched_uniq);
+
+typedef ssize_t (elevator_write_entry_fn) (struct request_queue *rq,
+										   struct file* filp,
+										   size_t count,
+										   loff_t *pos,
+										   void** opaque,
+										   int sched_uniq);
+
+typedef int (elevator_mkdir_entry_fn) (struct request_queue* rq,
+										   struct inode *dir,
+										   struct dentry *dentry, 
+										   int mode,
+										   void **opaque,
+										   int sched_uniq);
+
+typedef int (elevator_create_entry_fn) (struct request_queue* rq,
+										struct inode *dir,
+										struct dentry *dentry, 
+										int mode,
+										void **opaque,
+										int sched_uniq);
+
+typedef int (elevator_fsync_entry_fn) (struct request_queue *rq,
+									   struct file* filp,
+									   int datasync,
+									   void** opaque,
+									   int sched_uniq);
+										 
+typedef void (elevator_read_return_fn) (struct request_queue *rq,
+										void *opaque,
+										ssize_t rv,
+										int sched_uniq);
+
+typedef void (elevator_write_return_fn) (struct request_queue *rq,
+										 void *opaque,
+										 ssize_t rv,
+										 int sched_uniq);
+
+typedef void (elevator_fsync_return_fn) (struct request_queue *rq,
+										 void *opaque,
+										 int rv,
+										 int sched_uniq);
+
+typedef void (elevator_mkdir_return_fn) (struct request_queue *rq,
+										 void *opaque,
+										 int rv,
+										 int sched_uniq);
+typedef void (elevator_create_return_fn) (struct request_queue *rq,
+										  void *opaque,
+										  int rv,
+										  int sched_uniq);
+
+typedef void (elevator_causes_dirty_fn) (struct request_queue *rq,
+										 struct cause_list* causes,
+										 struct task_struct* new,
+										 struct inode *inode,
+										 long pos);
+
+typedef void (elevator_causes_free_fn) (struct request_queue *rq,
+										struct cause_list* causes);
+
 struct elevator_ops
 {
 	elevator_merge_fn *elevator_merge_fn;
@@ -64,6 +131,39 @@
 	elevator_init_fn *elevator_init_fn;
 	elevator_exit_fn *elevator_exit_fn;
 	void (*trim)(struct io_context *);
+
+	// these are called WITHOUT the queue_lock
+	elevator_read_entry_fn *elevator_read_entry_fn;
+	elevator_write_entry_fn *elevator_write_entry_fn;
+	elevator_fsync_entry_fn *elevator_fsync_entry_fn;
+	elevator_mkdir_entry_fn *elevator_mkdir_entry_fn;
+	elevator_create_entry_fn *elevator_create_entry_fn;
+	elevator_read_return_fn *elevator_read_return_fn;
+	elevator_write_return_fn *elevator_write_return_fn;
+	elevator_fsync_return_fn *elevator_fsync_return_fn;
+	elevator_mkdir_return_fn *elevator_mkdir_return_fn;
+	elevator_create_return_fn *elevator_create_return_fn;
+
+	// these are called WITH the queue_lock
+	elevator_causes_dirty_fn *elevator_causes_dirty_fn;
+	// don't take a reference to the cause_list here!
+	elevator_causes_free_fn *elevator_causes_free_fn;
+};
+
+// fields of elevator_ops pertaining to syscalls
+struct elevator_syscall_ops
+{
+	elevator_read_entry_fn *read_entry_fn;
+	elevator_write_entry_fn *write_entry_fn;
+	elevator_fsync_entry_fn *fsync_entry_fn;
+	elevator_mkdir_entry_fn *mkdir_entry_fn;
+	elevator_create_entry_fn *create_entry_fn;
+	elevator_read_return_fn *read_return_fn;
+	elevator_write_return_fn *write_return_fn;
+	elevator_fsync_return_fn *fsync_return_fn;
+	elevator_mkdir_return_fn *mkdir_return_fn;
+	elevator_create_return_fn *create_return_fn;
+	int sched_uniq;
 };
 
 #define ELV_NAME_MAX	(16)
@@ -105,6 +205,7 @@
  */
 extern void elv_dispatch_sort(struct request_queue *, struct request *);
 extern void elv_dispatch_add_tail(struct request_queue *, struct request *);
+extern void elv_dispatch_add_head(struct request_queue *, struct request *);
 extern void elv_add_request(struct request_queue *, struct request *, int);
 extern void __elv_add_request(struct request_queue *, struct request *, int);
 extern int elv_merge(struct request_queue *, struct request **, struct bio *);
diff -rubBN linux-3.2.51/include/linux/fs.h ../split-release/release/linux-3.2.51/include/linux/fs.h
--- linux-3.2.51/include/linux/fs.h	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/include/linux/fs.h	2015-09-01 05:42:28.000000000 +0200
@@ -839,6 +839,9 @@
 	atomic_t		i_readcount; /* struct files open RO */
 #endif
 	void			*i_private; /* fs or device private pointer */
+
+	// for scheduler to use as it pleases
+	long i_private1; // (maybe use as last write offset)
 };
 
 static inline int inode_unhashed(struct inode *inode)
diff -rubBN linux-3.2.51/include/linux/hashtable.h ../split-release/release/linux-3.2.51/include/linux/hashtable.h
--- linux-3.2.51/include/linux/hashtable.h	1970-01-01 01:00:00.000000000 +0100
+++ ../split-release/release/linux-3.2.51/include/linux/hashtable.h	2015-09-01 05:42:28.000000000 +0200
@@ -0,0 +1,192 @@
+/*
+ * Statically sized hash table implementation
+ * (C) 2012  Sasha Levin <levinsasha928@gmail.com>
+ */
+
+#ifndef _LINUX_HASHTABLE_H
+#define _LINUX_HASHTABLE_H
+
+#include <linux/list.h>
+#include <linux/types.h>
+#include <linux/kernel.h>
+#include <linux/hash.h>
+#include <linux/rculist.h>
+
+#define DEFINE_HASHTABLE(name, bits)						\
+	struct hlist_head name[1 << (bits)] =					\
+			{ [0 ... ((1 << (bits)) - 1)] = HLIST_HEAD_INIT }
+
+#define DECLARE_HASHTABLE(name, bits)                                   	\
+	struct hlist_head name[1 << (bits)]
+
+#define HASH_SIZE(name) (ARRAY_SIZE(name))
+#define HASH_BITS(name) ilog2(HASH_SIZE(name))
+
+/* Use hash_32 when possible to allow for fast 32bit hashing in 64bit kernels. */
+#define hash_min(val, bits)							\
+	(sizeof(val) <= 4 ? hash_32(val, bits) : hash_long(val, bits))
+
+static inline void __hash_init(struct hlist_head *ht, unsigned int sz)
+{
+	unsigned int i;
+
+	for (i = 0; i < sz; i++)
+		INIT_HLIST_HEAD(&ht[i]);
+}
+
+/**
+ * hash_init - initialize a hash table
+ * @hashtable: hashtable to be initialized
+ *
+ * Calculates the size of the hashtable from the given parameter, otherwise
+ * same as hash_init_size.
+ *
+ * This has to be a macro since HASH_BITS() will not work on pointers since
+ * it calculates the size during preprocessing.
+ */
+#define hash_init(hashtable) __hash_init(hashtable, HASH_SIZE(hashtable))
+
+/**
+ * hash_add - add an object to a hashtable
+ * @hashtable: hashtable to add to
+ * @node: the &struct hlist_node of the object to be added
+ * @key: the key of the object to be added
+ */
+#define hash_add(hashtable, node, key)						\
+	hlist_add_head(node, &hashtable[hash_min(key, HASH_BITS(hashtable))])
+
+/**
+ * hash_add_rcu - add an object to a rcu enabled hashtable
+ * @hashtable: hashtable to add to
+ * @node: the &struct hlist_node of the object to be added
+ * @key: the key of the object to be added
+ */
+#define hash_add_rcu(hashtable, node, key)					\
+	hlist_add_head_rcu(node, &hashtable[hash_min(key, HASH_BITS(hashtable))])
+
+/**
+ * hash_hashed - check whether an object is in any hashtable
+ * @node: the &struct hlist_node of the object to be checked
+ */
+static inline bool hash_hashed(struct hlist_node *node)
+{
+	return !hlist_unhashed(node);
+}
+
+static inline bool __hash_empty(struct hlist_head *ht, unsigned int sz)
+{
+	unsigned int i;
+
+	for (i = 0; i < sz; i++)
+		if (!hlist_empty(&ht[i]))
+			return false;
+
+	return true;
+}
+
+/**
+ * hash_empty - check whether a hashtable is empty
+ * @hashtable: hashtable to check
+ *
+ * This has to be a macro since HASH_BITS() will not work on pointers since
+ * it calculates the size during preprocessing.
+ */
+#define hash_empty(hashtable) __hash_empty(hashtable, HASH_SIZE(hashtable))
+
+/**
+ * hash_del - remove an object from a hashtable
+ * @node: &struct hlist_node of the object to remove
+ */
+static inline void hash_del(struct hlist_node *node)
+{
+	hlist_del_init(node);
+}
+
+/**
+ * hash_del_rcu - remove an object from a rcu enabled hashtable
+ * @node: &struct hlist_node of the object to remove
+ */
+static inline void hash_del_rcu(struct hlist_node *node)
+{
+	hlist_del_init_rcu(node);
+}
+
+/**
+ * hash_for_each - iterate over a hashtable
+ * @name: hashtable to iterate
+ * @bkt: integer to use as bucket loop cursor
+ * @node: the &struct list_head to use as a loop cursor for each entry
+ * @obj: the type * to use as a loop cursor for each entry
+ * @member: the name of the hlist_node within the struct
+ */
+#define hash_for_each(name, bkt, node, obj, member)				\
+	for ((bkt) = 0, node = NULL; node == NULL && (bkt) < HASH_SIZE(name); (bkt)++)\
+		hlist_for_each_entry(obj, node, &name[bkt], member)
+
+/**
+ * hash_for_each_rcu - iterate over a rcu enabled hashtable
+ * @name: hashtable to iterate
+ * @bkt: integer to use as bucket loop cursor
+ * @node: the &struct list_head to use as a loop cursor for each entry
+ * @obj: the type * to use as a loop cursor for each entry
+ * @member: the name of the hlist_node within the struct
+ */
+#define hash_for_each_rcu(name, bkt, node, obj, member)				\
+	for ((bkt) = 0, node = NULL; node == NULL && (bkt) < HASH_SIZE(name); (bkt)++)\
+		hlist_for_each_entry_rcu(obj, node, &name[bkt], member)
+
+/**
+ * hash_for_each_safe - iterate over a hashtable safe against removal of
+ * hash entry
+ * @name: hashtable to iterate
+ * @bkt: integer to use as bucket loop cursor
+ * @node: the &struct hlist_node to use as a loop cursor for each entry
+ * @tmp: a &struct hlist_node used for temporary storage
+ * @obj: the type * to use as a loop cursor for each entry
+ * @member: the name of the hlist_node within the struct
+ */
+#define hash_for_each_safe(name, bkt, node, tmp, obj, member)			\
+	for ((bkt) = 0, node = NULL; node == NULL && (bkt) < HASH_SIZE(name); (bkt)++)\
+		hlist_for_each_entry_safe(obj, node, tmp, &name[bkt], member)
+
+/**
+ * hash_for_each_possible - iterate over all possible objects hashing to the
+ * same bucket
+ * @name: hashtable to iterate
+ * @obj: the type * to use as a loop cursor for each entry
+ * @node: the &struct list_head to use as a loop cursor for each entry
+ * @member: the name of the hlist_node within the struct
+ * @key: the key of the objects to iterate over
+ */
+#define hash_for_each_possible(name, obj, node, member, key)			\
+	hlist_for_each_entry(obj, node,	&name[hash_min(key, HASH_BITS(name))], member)
+
+/**
+ * hash_for_each_possible_rcu - iterate over all possible objects hashing to the
+ * same bucket in an rcu enabled hashtable
+ * in a rcu enabled hashtable
+ * @name: hashtable to iterate
+ * @obj: the type * to use as a loop cursor for each entry
+ * @node: the &struct list_head to use as a loop cursor for each entry
+ * @member: the name of the hlist_node within the struct
+ * @key: the key of the objects to iterate over
+ */
+#define hash_for_each_possible_rcu(name, obj, node, member, key)		\
+	hlist_for_each_entry_rcu(obj, node, &name[hash_min(key, HASH_BITS(name))], member)
+
+/**
+ * hash_for_each_possible_safe - iterate over all possible objects hashing to the
+ * same bucket safe against removals
+ * @name: hashtable to iterate
+ * @obj: the type * to use as a loop cursor for each entry
+ * @node: the &struct list_head to use as a loop cursor for each entry
+ * @tmp: a &struct used for temporary storage
+ * @member: the name of the hlist_node within the struct
+ * @key: the key of the objects to iterate over
+ */
+#define hash_for_each_possible_safe(name, obj, node, tmp, member, key)		\
+	hlist_for_each_entry_safe(obj, node, tmp,				\
+		&name[hash_min(key, HASH_BITS(name))], member)
+
+
+#endif
diff -rubBN linux-3.2.51/include/linux/init_task.h ../split-release/release/linux-3.2.51/include/linux/init_task.h
--- linux-3.2.51/include/linux/init_task.h	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/include/linux/init_task.h	2015-09-01 05:42:28.000000000 +0200
@@ -150,6 +150,8 @@
  */
 #define INIT_TASK(tsk)	\
 {									\
+	.causes     = NULL,                     \
+	.account_id     = ATOMIC_INIT(0),	\
 	.state		= 0,						\
 	.stack		= &init_thread_info,				\
 	.usage		= ATOMIC_INIT(2),				\
diff -rubBN linux-3.2.51/include/linux/jbd2.h ../split-release/release/linux-3.2.51/include/linux/jbd2.h
--- linux-3.2.51/include/linux/jbd2.h	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/include/linux/jbd2.h	2015-09-01 05:42:28.000000000 +0200
@@ -606,6 +606,8 @@
 	 * structures associated with the transaction
 	 */
 	struct list_head	t_private_list;
+
+	struct cause_list* causes;
 };
 
 struct transaction_run_stats_s {
@@ -981,6 +983,8 @@
 int __jbd2_journal_remove_checkpoint(struct journal_head *);
 void __jbd2_journal_insert_checkpoint(struct journal_head *, transaction_t *);
 
+/* Transaction management */
+void jbd2_free_transaction(transaction_t *);
 
 /*
  * Triggers
@@ -1145,7 +1149,8 @@
 extern int	   jbd2_journal_revoke (handle_t *, unsigned long long, struct buffer_head *);
 extern int	   jbd2_journal_cancel_revoke(handle_t *, struct journal_head *);
 extern void	   jbd2_journal_write_revoke_records(journal_t *,
-						     transaction_t *, int);
+												 transaction_t *, int,
+												 struct cause_list*);
 
 /* Recovery revoke support */
 extern int	jbd2_journal_set_revoke(journal_t *, unsigned long long, tid_t);
diff -rubBN linux-3.2.51/include/linux/journal-head.h ../split-release/release/linux-3.2.51/include/linux/journal-head.h
--- linux-3.2.51/include/linux/journal-head.h	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/include/linux/journal-head.h	2015-09-01 05:42:28.000000000 +0200
@@ -102,6 +102,8 @@
 
 	/* Trigger type for the committing transaction's frozen data */
 	struct jbd2_buffer_trigger_type *b_frozen_triggers;
+
+	struct cause_list* causes;
 };
 
 #endif		/* JOURNAL_HEAD_H_INCLUDED */
diff -rubBN linux-3.2.51/include/linux/mm_types.h ../split-release/release/linux-3.2.51/include/linux/mm_types.h
--- linux-3.2.51/include/linux/mm_types.h	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/include/linux/mm_types.h	2015-09-01 05:42:28.000000000 +0200
@@ -149,7 +149,13 @@
 	 */
 	void *shadow;
 #endif
+
+    // cause list for btrfs
+#ifndef DISABLE_CAUSES
+        void* locked_causes;
+#endif
 }
+
 /*
  * If another subsystem starts using the double word pairing for atomic
  * operations on struct page then it must change the #if to ensure
diff -rubBN linux-3.2.51/include/linux/sched.h ../split-release/release/linux-3.2.51/include/linux/sched.h
--- linux-3.2.51/include/linux/sched.h	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/include/linux/sched.h	2015-09-01 05:42:28.000000000 +0200
@@ -1576,6 +1576,13 @@
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
 	atomic_t ptrace_bp_refcnt;
 #endif
+
+	// sometimes a task acts as a proxy for other threads (e.g.,
+	// a flush thread acting on behalf of other tasks that did
+	// delayed allocation).
+	struct cause_list* causes;
+
+	atomic_t account_id;
 };
 
 /* Future-safe accessor for struct task_struct's cpus_allowed. */
diff -rubBN linux-3.2.51/kernel/hrtimer.c ../split-release/release/linux-3.2.51/kernel/hrtimer.c
--- linux-3.2.51/kernel/hrtimer.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/kernel/hrtimer.c	2015-09-01 05:42:50.000000000 +0200
@@ -1624,6 +1624,7 @@
 	destroy_hrtimer_on_stack(&t.timer);
 	return ret;
 }
+EXPORT_SYMBOL_GPL(hrtimer_nanosleep);
 
 SYSCALL_DEFINE2(nanosleep, struct timespec __user *, rqtp,
 		struct timespec __user *, rmtp)
diff -rubBN linux-3.2.51/kernel/kthread.c ../split-release/release/linux-3.2.51/kernel/kthread.c
--- linux-3.2.51/kernel/kthread.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/kernel/kthread.c	2015-09-01 05:42:51.000000000 +0200
@@ -70,6 +70,7 @@
 {
 	return to_kthread(task)->data;
 }
+EXPORT_SYMBOL(kthread_data);
 
 static int kthread(void *_create)
 {
diff -rubBN linux-3.2.51/mm/filemap.c ../split-release/release/linux-3.2.51/mm/filemap.c
--- linux-3.2.51/mm/filemap.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/mm/filemap.c	2015-09-01 05:42:46.000000000 +0200
@@ -776,6 +776,10 @@
 		page = __page_cache_alloc(gfp_mask);
 		if (!page)
 			return NULL;
+
+#ifndef DISABLE_CAUSES
+		page->locked_causes = NULL;
+#endif
 		/*
 		 * We want a regular kernel memory (not highmem or DMA etc)
 		 * allocation for the radix tree nodes, but we need to honour
diff -rubBN linux-3.2.51/mm/page-writeback.c ../split-release/release/linux-3.2.51/mm/page-writeback.c
--- linux-3.2.51/mm/page-writeback.c	2013-09-10 02:57:42.000000000 +0200
+++ ../split-release/release/linux-3.2.51/mm/page-writeback.c	2015-09-01 05:42:46.000000000 +0200
@@ -1679,6 +1679,7 @@
 		ret = generic_writepages(mapping, wbc);
 	return ret;
 }
+EXPORT_SYMBOL(do_writepages);
 
 /**
  * write_one_page - write out a single page and optionally wait on I/O
Binary files linux-3.2.51/samples/hidraw/hid-example and ../split-release/release/linux-3.2.51/samples/hidraw/hid-example differ
diff -rubBN linux-3.2.51/scripts/genksyms/keywords.hash.c ../split-release/release/linux-3.2.51/scripts/genksyms/keywords.hash.c
--- linux-3.2.51/scripts/genksyms/keywords.hash.c	1970-01-01 01:00:00.000000000 +0100
+++ ../split-release/release/linux-3.2.51/scripts/genksyms/keywords.hash.c	2015-09-01 05:42:46.000000000 +0200
@@ -0,0 +1,220 @@
+/* ANSI-C code produced by gperf version 3.0.4 */
+/* Command-line: gperf -t --output-file scripts/genksyms/keywords.hash.c_shipped -a -C -E -g -k '1,3,$' -p -t scripts/genksyms/keywords.gperf  */
+
+#if !((' ' == 32) && ('!' == 33) && ('"' == 34) && ('#' == 35) \
+      && ('%' == 37) && ('&' == 38) && ('\'' == 39) && ('(' == 40) \
+      && (')' == 41) && ('*' == 42) && ('+' == 43) && (',' == 44) \
+      && ('-' == 45) && ('.' == 46) && ('/' == 47) && ('0' == 48) \
+      && ('1' == 49) && ('2' == 50) && ('3' == 51) && ('4' == 52) \
+      && ('5' == 53) && ('6' == 54) && ('7' == 55) && ('8' == 56) \
+      && ('9' == 57) && (':' == 58) && (';' == 59) && ('<' == 60) \
+      && ('=' == 61) && ('>' == 62) && ('?' == 63) && ('A' == 65) \
+      && ('B' == 66) && ('C' == 67) && ('D' == 68) && ('E' == 69) \
+      && ('F' == 70) && ('G' == 71) && ('H' == 72) && ('I' == 73) \
+      && ('J' == 74) && ('K' == 75) && ('L' == 76) && ('M' == 77) \
+      && ('N' == 78) && ('O' == 79) && ('P' == 80) && ('Q' == 81) \
+      && ('R' == 82) && ('S' == 83) && ('T' == 84) && ('U' == 85) \
+      && ('V' == 86) && ('W' == 87) && ('X' == 88) && ('Y' == 89) \
+      && ('Z' == 90) && ('[' == 91) && ('\\' == 92) && (']' == 93) \
+      && ('^' == 94) && ('_' == 95) && ('a' == 97) && ('b' == 98) \
+      && ('c' == 99) && ('d' == 100) && ('e' == 101) && ('f' == 102) \
+      && ('g' == 103) && ('h' == 104) && ('i' == 105) && ('j' == 106) \
+      && ('k' == 107) && ('l' == 108) && ('m' == 109) && ('n' == 110) \
+      && ('o' == 111) && ('p' == 112) && ('q' == 113) && ('r' == 114) \
+      && ('s' == 115) && ('t' == 116) && ('u' == 117) && ('v' == 118) \
+      && ('w' == 119) && ('x' == 120) && ('y' == 121) && ('z' == 122) \
+      && ('{' == 123) && ('|' == 124) && ('}' == 125) && ('~' == 126))
+/* The character set is not based on ISO-646.  */
+#error "gperf generated tables don't work with this execution character set. Please report a bug to <bug-gnu-gperf@gnu.org>."
+#endif
+
+#line 4 "scripts/genksyms/keywords.gperf"
+
+struct resword;
+static const struct resword *is_reserved_word(register const char *str, register unsigned int len);
+#line 8 "scripts/genksyms/keywords.gperf"
+struct resword { const char *name; int token; };
+/* maximum key range = 64, duplicates = 0 */
+
+#ifdef __GNUC__
+__inline
+#else
+#ifdef __cplusplus
+inline
+#endif
+#endif
+static unsigned int
+is_reserved_hash (register const char *str, register unsigned int len)
+{
+  static const unsigned char asso_values[] =
+    {
+      67, 67, 67, 67, 67, 67, 67, 67, 67, 67,
+      67, 67, 67, 67, 67, 67, 67, 67, 67, 67,
+      67, 67, 67, 67, 67, 67, 67, 67, 67, 67,
+      67, 67, 67, 67, 67, 67, 67, 67, 67, 67,
+      67, 67, 67, 67, 67, 67, 67, 67, 67, 67,
+      67, 67, 67, 67, 67, 67, 67, 67, 67, 67,
+      67, 67, 67, 67, 67, 67, 67, 67, 67,  0,
+      67, 67, 67, 67, 67, 67, 15, 67, 67, 67,
+       0, 67, 67, 67, 67, 67, 67, 67, 67, 67,
+      67, 67, 67, 67, 67,  0, 67,  0, 67,  5,
+      25, 20, 15, 30, 67, 15, 67, 67, 10,  0,
+      10, 40, 20, 67, 10,  5,  0, 10, 15, 67,
+      67, 67, 67, 67, 67, 67, 67, 67, 67, 67,
+      67, 67, 67, 67, 67, 67, 67, 67, 67, 67,
+      67, 67, 67, 67, 67, 67, 67, 67, 67, 67,
+      67, 67, 67, 67, 67, 67, 67, 67, 67, 67,
+      67, 67, 67, 67, 67, 67, 67, 67, 67, 67,
+      67, 67, 67, 67, 67, 67, 67, 67, 67, 67,
+      67, 67, 67, 67, 67, 67, 67, 67, 67, 67,
+      67, 67, 67, 67, 67, 67, 67, 67, 67, 67,
+      67, 67, 67, 67, 67, 67, 67, 67, 67, 67,
+      67, 67, 67, 67, 67, 67, 67, 67, 67, 67,
+      67, 67, 67, 67, 67, 67, 67, 67, 67, 67,
+      67, 67, 67, 67, 67, 67, 67, 67, 67, 67,
+      67, 67, 67, 67, 67, 67, 67, 67, 67, 67,
+      67, 67, 67, 67, 67, 67
+    };
+  return len + asso_values[(unsigned char)str[2]] + asso_values[(unsigned char)str[0]] + asso_values[(unsigned char)str[len - 1]];
+}
+
+#ifdef __GNUC__
+__inline
+#if defined __GNUC_STDC_INLINE__ || defined __GNUC_GNU_INLINE__
+__attribute__ ((__gnu_inline__))
+#endif
+#endif
+const struct resword *
+is_reserved_word (register const char *str, register unsigned int len)
+{
+  enum
+    {
+      TOTAL_KEYWORDS = 45,
+      MIN_WORD_LENGTH = 3,
+      MAX_WORD_LENGTH = 24,
+      MIN_HASH_VALUE = 3,
+      MAX_HASH_VALUE = 66
+    };
+
+  static const struct resword wordlist[] =
+    {
+      {""}, {""}, {""},
+#line 33 "scripts/genksyms/keywords.gperf"
+      {"asm", ASM_KEYW},
+      {""},
+#line 15 "scripts/genksyms/keywords.gperf"
+      {"__asm", ASM_KEYW},
+      {""},
+#line 16 "scripts/genksyms/keywords.gperf"
+      {"__asm__", ASM_KEYW},
+      {""}, {""},
+#line 59 "scripts/genksyms/keywords.gperf"
+      {"__typeof__", TYPEOF_KEYW},
+      {""},
+#line 19 "scripts/genksyms/keywords.gperf"
+      {"__const", CONST_KEYW},
+#line 18 "scripts/genksyms/keywords.gperf"
+      {"__attribute__", ATTRIBUTE_KEYW},
+#line 20 "scripts/genksyms/keywords.gperf"
+      {"__const__", CONST_KEYW},
+#line 25 "scripts/genksyms/keywords.gperf"
+      {"__signed__", SIGNED_KEYW},
+#line 51 "scripts/genksyms/keywords.gperf"
+      {"static", STATIC_KEYW},
+      {""},
+#line 46 "scripts/genksyms/keywords.gperf"
+      {"int", INT_KEYW},
+#line 39 "scripts/genksyms/keywords.gperf"
+      {"char", CHAR_KEYW},
+#line 40 "scripts/genksyms/keywords.gperf"
+      {"const", CONST_KEYW},
+#line 52 "scripts/genksyms/keywords.gperf"
+      {"struct", STRUCT_KEYW},
+#line 31 "scripts/genksyms/keywords.gperf"
+      {"__restrict__", RESTRICT_KEYW},
+#line 32 "scripts/genksyms/keywords.gperf"
+      {"restrict", RESTRICT_KEYW},
+#line 12 "scripts/genksyms/keywords.gperf"
+      {"EXPORT_SYMBOL_GPL_FUTURE", EXPORT_SYMBOL_KEYW},
+#line 23 "scripts/genksyms/keywords.gperf"
+      {"__inline__", INLINE_KEYW},
+      {""},
+#line 27 "scripts/genksyms/keywords.gperf"
+      {"__volatile__", VOLATILE_KEYW},
+#line 10 "scripts/genksyms/keywords.gperf"
+      {"EXPORT_SYMBOL", EXPORT_SYMBOL_KEYW},
+#line 30 "scripts/genksyms/keywords.gperf"
+      {"_restrict", RESTRICT_KEYW},
+      {""},
+#line 17 "scripts/genksyms/keywords.gperf"
+      {"__attribute", ATTRIBUTE_KEYW},
+#line 11 "scripts/genksyms/keywords.gperf"
+      {"EXPORT_SYMBOL_GPL", EXPORT_SYMBOL_KEYW},
+#line 21 "scripts/genksyms/keywords.gperf"
+      {"__extension__", EXTENSION_KEYW},
+#line 42 "scripts/genksyms/keywords.gperf"
+      {"enum", ENUM_KEYW},
+#line 13 "scripts/genksyms/keywords.gperf"
+      {"EXPORT_UNUSED_SYMBOL", EXPORT_SYMBOL_KEYW},
+#line 43 "scripts/genksyms/keywords.gperf"
+      {"extern", EXTERN_KEYW},
+      {""},
+#line 24 "scripts/genksyms/keywords.gperf"
+      {"__signed", SIGNED_KEYW},
+#line 14 "scripts/genksyms/keywords.gperf"
+      {"EXPORT_UNUSED_SYMBOL_GPL", EXPORT_SYMBOL_KEYW},
+#line 54 "scripts/genksyms/keywords.gperf"
+      {"union", UNION_KEYW},
+#line 58 "scripts/genksyms/keywords.gperf"
+      {"typeof", TYPEOF_KEYW},
+#line 53 "scripts/genksyms/keywords.gperf"
+      {"typedef", TYPEDEF_KEYW},
+#line 22 "scripts/genksyms/keywords.gperf"
+      {"__inline", INLINE_KEYW},
+#line 38 "scripts/genksyms/keywords.gperf"
+      {"auto", AUTO_KEYW},
+#line 26 "scripts/genksyms/keywords.gperf"
+      {"__volatile", VOLATILE_KEYW},
+      {""}, {""},
+#line 55 "scripts/genksyms/keywords.gperf"
+      {"unsigned", UNSIGNED_KEYW},
+      {""},
+#line 49 "scripts/genksyms/keywords.gperf"
+      {"short", SHORT_KEYW},
+#line 45 "scripts/genksyms/keywords.gperf"
+      {"inline", INLINE_KEYW},
+      {""},
+#line 57 "scripts/genksyms/keywords.gperf"
+      {"volatile", VOLATILE_KEYW},
+#line 47 "scripts/genksyms/keywords.gperf"
+      {"long", LONG_KEYW},
+#line 29 "scripts/genksyms/keywords.gperf"
+      {"_Bool", BOOL_KEYW},
+      {""}, {""},
+#line 48 "scripts/genksyms/keywords.gperf"
+      {"register", REGISTER_KEYW},
+#line 56 "scripts/genksyms/keywords.gperf"
+      {"void", VOID_KEYW},
+#line 44 "scripts/genksyms/keywords.gperf"
+      {"float", FLOAT_KEYW},
+#line 41 "scripts/genksyms/keywords.gperf"
+      {"double", DOUBLE_KEYW},
+      {""}, {""}, {""}, {""},
+#line 50 "scripts/genksyms/keywords.gperf"
+      {"signed", SIGNED_KEYW}
+    };
+
+  if (len <= MAX_WORD_LENGTH && len >= MIN_WORD_LENGTH)
+    {
+      register int key = is_reserved_hash (str, len);
+
+      if (key <= MAX_HASH_VALUE && key >= 0)
+        {
+          register const char *s = wordlist[key].name;
+
+          if (*str == *s && !strcmp (str + 1, s + 1))
+            return &wordlist[key];
+        }
+    }
+  return 0;
+}
diff -rubBN linux-3.2.51/scripts/genksyms/lex.lex.c ../split-release/release/linux-3.2.51/scripts/genksyms/lex.lex.c
--- linux-3.2.51/scripts/genksyms/lex.lex.c	1970-01-01 01:00:00.000000000 +0100
+++ ../split-release/release/linux-3.2.51/scripts/genksyms/lex.lex.c	2015-09-01 05:42:46.000000000 +0200
@@ -0,0 +1,2245 @@
+
+#line 3 "scripts/genksyms/lex.lex.c_shipped"
+
+#define  YY_INT_ALIGNED short int
+
+/* A lexical scanner generated by flex */
+
+#define FLEX_SCANNER
+#define YY_FLEX_MAJOR_VERSION 2
+#define YY_FLEX_MINOR_VERSION 5
+#define YY_FLEX_SUBMINOR_VERSION 35
+#if YY_FLEX_SUBMINOR_VERSION > 0
+#define FLEX_BETA
+#endif
+
+/* First, we deal with  platform-specific or compiler-specific issues. */
+
+/* begin standard C headers. */
+#include <stdio.h>
+#include <string.h>
+#include <errno.h>
+#include <stdlib.h>
+
+/* end standard C headers. */
+
+/* flex integer type definitions */
+
+#ifndef FLEXINT_H
+#define FLEXINT_H
+
+/* C99 systems have <inttypes.h>. Non-C99 systems may or may not. */
+
+#if defined (__STDC_VERSION__) && __STDC_VERSION__ >= 199901L
+
+/* C99 says to define __STDC_LIMIT_MACROS before including stdint.h,
+ * if you want the limit (max/min) macros for int types. 
+ */
+#ifndef __STDC_LIMIT_MACROS
+#define __STDC_LIMIT_MACROS 1
+#endif
+
+#include <inttypes.h>
+typedef int8_t flex_int8_t;
+typedef uint8_t flex_uint8_t;
+typedef int16_t flex_int16_t;
+typedef uint16_t flex_uint16_t;
+typedef int32_t flex_int32_t;
+typedef uint32_t flex_uint32_t;
+#else
+typedef signed char flex_int8_t;
+typedef short int flex_int16_t;
+typedef int flex_int32_t;
+typedef unsigned char flex_uint8_t; 
+typedef unsigned short int flex_uint16_t;
+typedef unsigned int flex_uint32_t;
+#endif /* ! C99 */
+
+/* Limits of integral types. */
+#ifndef INT8_MIN
+#define INT8_MIN               (-128)
+#endif
+#ifndef INT16_MIN
+#define INT16_MIN              (-32767-1)
+#endif
+#ifndef INT32_MIN
+#define INT32_MIN              (-2147483647-1)
+#endif
+#ifndef INT8_MAX
+#define INT8_MAX               (127)
+#endif
+#ifndef INT16_MAX
+#define INT16_MAX              (32767)
+#endif
+#ifndef INT32_MAX
+#define INT32_MAX              (2147483647)
+#endif
+#ifndef UINT8_MAX
+#define UINT8_MAX              (255U)
+#endif
+#ifndef UINT16_MAX
+#define UINT16_MAX             (65535U)
+#endif
+#ifndef UINT32_MAX
+#define UINT32_MAX             (4294967295U)
+#endif
+
+#endif /* ! FLEXINT_H */
+
+#ifdef __cplusplus
+
+/* The "const" storage-class-modifier is valid. */
+#define YY_USE_CONST
+
+#else	/* ! __cplusplus */
+
+/* C99 requires __STDC__ to be defined as 1. */
+#if defined (__STDC__)
+
+#define YY_USE_CONST
+
+#endif	/* defined (__STDC__) */
+#endif	/* ! __cplusplus */
+
+#ifdef YY_USE_CONST
+#define yyconst const
+#else
+#define yyconst
+#endif
+
+/* Returned upon end-of-file. */
+#define YY_NULL 0
+
+/* Promotes a possibly negative, possibly signed char to an unsigned
+ * integer for use as an array index.  If the signed char is negative,
+ * we want to instead treat it as an 8-bit unsigned char, hence the
+ * double cast.
+ */
+#define YY_SC_TO_UI(c) ((unsigned int) (unsigned char) c)
+
+/* Enter a start condition.  This macro really ought to take a parameter,
+ * but we do it the disgusting crufty way forced on us by the ()-less
+ * definition of BEGIN.
+ */
+#define BEGIN (yy_start) = 1 + 2 *
+
+/* Translate the current start state into a value that can be later handed
+ * to BEGIN to return to the state.  The YYSTATE alias is for lex
+ * compatibility.
+ */
+#define YY_START (((yy_start) - 1) / 2)
+#define YYSTATE YY_START
+
+/* Action number for EOF rule of a given start state. */
+#define YY_STATE_EOF(state) (YY_END_OF_BUFFER + state + 1)
+
+/* Special action meaning "start processing a new file". */
+#define YY_NEW_FILE yyrestart(yyin  )
+
+#define YY_END_OF_BUFFER_CHAR 0
+
+/* Size of default input buffer. */
+#ifndef YY_BUF_SIZE
+#define YY_BUF_SIZE 16384
+#endif
+
+/* The state buf must be large enough to hold one state per character in the main buffer.
+ */
+#define YY_STATE_BUF_SIZE   ((YY_BUF_SIZE + 2) * sizeof(yy_state_type))
+
+#ifndef YY_TYPEDEF_YY_BUFFER_STATE
+#define YY_TYPEDEF_YY_BUFFER_STATE
+typedef struct yy_buffer_state *YY_BUFFER_STATE;
+#endif
+
+extern int yyleng;
+
+extern FILE *yyin, *yyout;
+
+#define EOB_ACT_CONTINUE_SCAN 0
+#define EOB_ACT_END_OF_FILE 1
+#define EOB_ACT_LAST_MATCH 2
+
+    #define YY_LESS_LINENO(n)
+    
+/* Return all but the first "n" matched characters back to the input stream. */
+#define yyless(n) \
+	do \
+		{ \
+		/* Undo effects of setting up yytext. */ \
+        int yyless_macro_arg = (n); \
+        YY_LESS_LINENO(yyless_macro_arg);\
+		*yy_cp = (yy_hold_char); \
+		YY_RESTORE_YY_MORE_OFFSET \
+		(yy_c_buf_p) = yy_cp = yy_bp + yyless_macro_arg - YY_MORE_ADJ; \
+		YY_DO_BEFORE_ACTION; /* set up yytext again */ \
+		} \
+	while ( 0 )
+
+#define unput(c) yyunput( c, (yytext_ptr)  )
+
+#ifndef YY_TYPEDEF_YY_SIZE_T
+#define YY_TYPEDEF_YY_SIZE_T
+typedef size_t yy_size_t;
+#endif
+
+#ifndef YY_STRUCT_YY_BUFFER_STATE
+#define YY_STRUCT_YY_BUFFER_STATE
+struct yy_buffer_state
+	{
+	FILE *yy_input_file;
+
+	char *yy_ch_buf;		/* input buffer */
+	char *yy_buf_pos;		/* current position in input buffer */
+
+	/* Size of input buffer in bytes, not including room for EOB
+	 * characters.
+	 */
+	yy_size_t yy_buf_size;
+
+	/* Number of characters read into yy_ch_buf, not including EOB
+	 * characters.
+	 */
+	int yy_n_chars;
+
+	/* Whether we "own" the buffer - i.e., we know we created it,
+	 * and can realloc() it to grow it, and should free() it to
+	 * delete it.
+	 */
+	int yy_is_our_buffer;
+
+	/* Whether this is an "interactive" input source; if so, and
+	 * if we're using stdio for input, then we want to use getc()
+	 * instead of fread(), to make sure we stop fetching input after
+	 * each newline.
+	 */
+	int yy_is_interactive;
+
+	/* Whether we're considered to be at the beginning of a line.
+	 * If so, '^' rules will be active on the next match, otherwise
+	 * not.
+	 */
+	int yy_at_bol;
+
+    int yy_bs_lineno; /**< The line count. */
+    int yy_bs_column; /**< The column count. */
+    
+	/* Whether to try to fill the input buffer when we reach the
+	 * end of it.
+	 */
+	int yy_fill_buffer;
+
+	int yy_buffer_status;
+
+#define YY_BUFFER_NEW 0
+#define YY_BUFFER_NORMAL 1
+	/* When an EOF's been seen but there's still some text to process
+	 * then we mark the buffer as YY_EOF_PENDING, to indicate that we
+	 * shouldn't try reading from the input source any more.  We might
+	 * still have a bunch of tokens to match, though, because of
+	 * possible backing-up.
+	 *
+	 * When we actually see the EOF, we change the status to "new"
+	 * (via yyrestart()), so that the user can continue scanning by
+	 * just pointing yyin at a new input file.
+	 */
+#define YY_BUFFER_EOF_PENDING 2
+
+	};
+#endif /* !YY_STRUCT_YY_BUFFER_STATE */
+
+/* Stack of input buffers. */
+static size_t yy_buffer_stack_top = 0; /**< index of top of stack. */
+static size_t yy_buffer_stack_max = 0; /**< capacity of stack. */
+static YY_BUFFER_STATE * yy_buffer_stack = 0; /**< Stack as an array. */
+
+/* We provide macros for accessing buffer states in case in the
+ * future we want to put the buffer states in a more general
+ * "scanner state".
+ *
+ * Returns the top of the stack, or NULL.
+ */
+#define YY_CURRENT_BUFFER ( (yy_buffer_stack) \
+                          ? (yy_buffer_stack)[(yy_buffer_stack_top)] \
+                          : NULL)
+
+/* Same as previous macro, but useful when we know that the buffer stack is not
+ * NULL or when we need an lvalue. For internal use only.
+ */
+#define YY_CURRENT_BUFFER_LVALUE (yy_buffer_stack)[(yy_buffer_stack_top)]
+
+/* yy_hold_char holds the character lost when yytext is formed. */
+static char yy_hold_char;
+static int yy_n_chars;		/* number of characters read into yy_ch_buf */
+int yyleng;
+
+/* Points to current character in buffer. */
+static char *yy_c_buf_p = (char *) 0;
+static int yy_init = 0;		/* whether we need to initialize */
+static int yy_start = 0;	/* start state number */
+
+/* Flag which is used to allow yywrap()'s to do buffer switches
+ * instead of setting up a fresh yyin.  A bit of a hack ...
+ */
+static int yy_did_buffer_switch_on_eof;
+
+void yyrestart (FILE *input_file  );
+void yy_switch_to_buffer (YY_BUFFER_STATE new_buffer  );
+YY_BUFFER_STATE yy_create_buffer (FILE *file,int size  );
+void yy_delete_buffer (YY_BUFFER_STATE b  );
+void yy_flush_buffer (YY_BUFFER_STATE b  );
+void yypush_buffer_state (YY_BUFFER_STATE new_buffer  );
+void yypop_buffer_state (void );
+
+static void yyensure_buffer_stack (void );
+static void yy_load_buffer_state (void );
+static void yy_init_buffer (YY_BUFFER_STATE b,FILE *file  );
+
+#define YY_FLUSH_BUFFER yy_flush_buffer(YY_CURRENT_BUFFER )
+
+YY_BUFFER_STATE yy_scan_buffer (char *base,yy_size_t size  );
+YY_BUFFER_STATE yy_scan_string (yyconst char *yy_str  );
+YY_BUFFER_STATE yy_scan_bytes (yyconst char *bytes,int len  );
+
+void *yyalloc (yy_size_t  );
+void *yyrealloc (void *,yy_size_t  );
+void yyfree (void *  );
+
+#define yy_new_buffer yy_create_buffer
+
+#define yy_set_interactive(is_interactive) \
+	{ \
+	if ( ! YY_CURRENT_BUFFER ){ \
+        yyensure_buffer_stack (); \
+		YY_CURRENT_BUFFER_LVALUE =    \
+            yy_create_buffer(yyin,YY_BUF_SIZE ); \
+	} \
+	YY_CURRENT_BUFFER_LVALUE->yy_is_interactive = is_interactive; \
+	}
+
+#define yy_set_bol(at_bol) \
+	{ \
+	if ( ! YY_CURRENT_BUFFER ){\
+        yyensure_buffer_stack (); \
+		YY_CURRENT_BUFFER_LVALUE =    \
+            yy_create_buffer(yyin,YY_BUF_SIZE ); \
+	} \
+	YY_CURRENT_BUFFER_LVALUE->yy_at_bol = at_bol; \
+	}
+
+#define YY_AT_BOL() (YY_CURRENT_BUFFER_LVALUE->yy_at_bol)
+
+/* Begin user sect3 */
+
+#define yywrap(n) 1
+#define YY_SKIP_YYWRAP
+
+typedef unsigned char YY_CHAR;
+
+FILE *yyin = (FILE *) 0, *yyout = (FILE *) 0;
+
+typedef int yy_state_type;
+
+extern int yylineno;
+
+int yylineno = 1;
+
+extern char *yytext;
+#define yytext_ptr yytext
+
+static yy_state_type yy_get_previous_state (void );
+static yy_state_type yy_try_NUL_trans (yy_state_type current_state  );
+static int yy_get_next_buffer (void );
+static void yy_fatal_error (yyconst char msg[]  );
+
+/* Done after the current pattern has been matched and before the
+ * corresponding action - sets up yytext.
+ */
+#define YY_DO_BEFORE_ACTION \
+	(yytext_ptr) = yy_bp; \
+	yyleng = (size_t) (yy_cp - yy_bp); \
+	(yy_hold_char) = *yy_cp; \
+	*yy_cp = '\0'; \
+	(yy_c_buf_p) = yy_cp;
+
+#define YY_NUM_RULES 13
+#define YY_END_OF_BUFFER 14
+/* This struct is not used in this scanner,
+   but its presence is necessary. */
+struct yy_trans_info
+	{
+	flex_int32_t yy_verify;
+	flex_int32_t yy_nxt;
+	};
+static yyconst flex_int16_t yy_accept[73] =
+    {   0,
+        0,    0,   14,   12,    4,    3,   12,    7,   12,   12,
+       12,   12,   12,    9,    9,   12,   12,    7,   12,   12,
+        4,    0,    5,    0,    7,    8,    0,    6,    0,    0,
+       10,   10,    9,    0,    0,    9,    9,    0,    9,    0,
+        0,    0,    0,    2,    0,    0,   11,    0,   10,    0,
+       10,    9,    9,    0,    0,    0,   10,   10,    0,    0,
+        0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
+        1,    0
+    } ;
+
+static yyconst flex_int32_t yy_ec[256] =
+    {   0,
+        1,    1,    1,    1,    1,    1,    1,    1,    2,    3,
+        4,    4,    4,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    2,    1,    5,    6,    7,    8,    9,   10,    1,
+        1,    8,   11,    1,   12,   13,    8,   14,   15,   15,
+       15,   15,   15,   15,   15,   16,   16,    1,    1,   17,
+       18,   19,    1,    1,   20,   20,   20,   20,   21,   22,
+        7,    7,    7,    7,    7,   23,    7,    7,    7,    7,
+        7,    7,    7,    7,   24,    7,    7,   25,    7,    7,
+        1,   26,    1,    8,    7,    1,   20,   20,   20,   20,
+
+       21,   22,    7,    7,    7,    7,    7,   27,    7,    7,
+        7,    7,    7,    7,    7,    7,   24,    7,    7,   25,
+        7,    7,    1,   28,    1,    8,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1
+    } ;
+
+static yyconst flex_int32_t yy_meta[29] =
+    {   0,
+        1,    1,    2,    1,    1,    1,    3,    1,    1,    1,
+        4,    4,    5,    6,    6,    6,    1,    1,    1,    7,
+        8,    7,    3,    3,    3,    1,    3,    1
+    } ;
+
+static yyconst flex_int16_t yy_base[85] =
+    {   0,
+        0,  145,  150,  266,   27,  266,   25,    0,  131,   23,
+       23,   16,   23,   39,   31,   25,   39,   60,   22,   65,
+       57,   43,  266,    0,    0,  266,   61,  266,    0,  128,
+       74,    0,  113,   59,   62,  113,   52,    0,    0,   72,
+       66,  110,  100,  266,   73,   74,  266,   70,  266,   90,
+      103,  266,   84,  129,  108,  113,  143,  266,  107,   66,
+      118,  137,  168,  120,   80,   91,  145,  143,   83,   41,
+      266,  266,  190,  196,  204,  212,  220,  228,  232,  237,
+      238,  243,  249,  257
+    } ;
+
+static yyconst flex_int16_t yy_def[85] =
+    {   0,
+       72,    1,   72,   72,   72,   72,   73,   74,   72,   72,
+       75,   72,   72,   72,   14,   72,   72,   74,   72,   76,
+       72,   73,   72,   77,   74,   72,   75,   72,   78,   72,
+       72,   31,   14,   79,   80,   72,   72,   81,   15,   73,
+       75,   76,   76,   72,   73,   75,   72,   82,   72,   72,
+       72,   72,   81,   76,   54,   72,   72,   72,   76,   54,
+       76,   76,   76,   54,   83,   76,   63,   83,   84,   84,
+       72,    0,   72,   72,   72,   72,   72,   72,   72,   72,
+       72,   72,   72,   72
+    } ;
+
+static yyconst flex_int16_t yy_nxt[295] =
+    {   0,
+        4,    5,    6,    5,    7,    4,    8,    9,   10,   11,
+        9,   12,   13,   14,   15,   15,   16,    9,   17,    8,
+        8,    8,   18,    8,    8,    4,    8,   19,   21,   23,
+       21,   26,   28,   26,   26,   30,   31,   31,   31,   26,
+       26,   26,   26,   71,   39,   39,   39,   23,   29,   26,
+       24,   32,   33,   33,   34,   72,   26,   26,   21,   35,
+       21,   36,   37,   38,   40,   36,   43,   44,   24,   41,
+       28,   32,   50,   50,   52,   28,   23,   23,   52,   35,
+       56,   56,   44,   28,   42,   71,   29,   31,   31,   31,
+       42,   29,   59,   44,   48,   49,   49,   24,   24,   29,
+
+       49,   43,   44,   51,   51,   51,   36,   37,   59,   44,
+       36,   65,   44,   54,   55,   55,   51,   51,   51,   59,
+       44,   64,   64,   64,   58,   58,   57,   57,   57,   58,
+       59,   44,   42,   64,   64,   64,   52,   72,   59,   44,
+       47,   66,   60,   60,   42,   44,   59,   69,   26,   72,
+       20,   61,   62,   63,   72,   61,   57,   57,   57,   66,
+       72,   72,   72,   66,   49,   49,   72,   61,   62,   49,
+       44,   61,   72,   72,   72,   72,   72,   72,   72,   72,
+       72,   67,   67,   67,   72,   72,   72,   67,   67,   67,
+       22,   22,   22,   22,   22,   22,   22,   22,   25,   72,
+
+       72,   25,   25,   25,   27,   27,   27,   27,   27,   27,
+       27,   27,   42,   42,   42,   42,   42,   42,   42,   42,
+       45,   72,   45,   45,   45,   45,   45,   45,   46,   72,
+       46,   46,   46,   46,   46,   46,   34,   34,   72,   34,
+       51,   72,   51,   53,   53,   53,   57,   72,   57,   68,
+       68,   68,   68,   68,   68,   68,   68,   70,   70,   70,
+       70,   70,   70,   70,   70,    3,   72,   72,   72,   72,
+       72,   72,   72,   72,   72,   72,   72,   72,   72,   72,
+       72,   72,   72,   72,   72,   72,   72,   72,   72,   72,
+       72,   72,   72,   72
+
+    } ;
+
+static yyconst flex_int16_t yy_chk[295] =
+    {   0,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
+        1,    1,    1,    1,    1,    1,    1,    1,    5,    7,
+        5,   10,   11,   12,   12,   13,   13,   13,   13,   19,
+       10,   16,   16,   70,   15,   15,   15,   22,   11,   19,
+        7,   14,   14,   14,   14,   15,   17,   17,   21,   14,
+       21,   14,   14,   14,   18,   14,   20,   20,   22,   18,
+       27,   34,   35,   35,   37,   41,   40,   45,   37,   34,
+       48,   48,   65,   46,   65,   69,   27,   31,   31,   31,
+       60,   41,   66,   66,   31,   31,   31,   40,   45,   46,
+
+       31,   43,   43,   50,   50,   50,   53,   53,   59,   59,
+       53,   59,   42,   43,   43,   43,   51,   51,   51,   61,
+       61,   55,   55,   55,   51,   51,   56,   56,   56,   51,
+       54,   54,   55,   64,   64,   64,   36,   33,   62,   62,
+       30,   61,   54,   54,   64,   68,   67,   68,    9,    3,
+        2,   54,   54,   54,    0,   54,   57,   57,   57,   62,
+        0,    0,    0,   62,   57,   57,    0,   67,   67,   57,
+       63,   67,    0,    0,    0,    0,    0,    0,    0,    0,
+        0,   63,   63,   63,    0,    0,    0,   63,   63,   63,
+       73,   73,   73,   73,   73,   73,   73,   73,   74,    0,
+
+        0,   74,   74,   74,   75,   75,   75,   75,   75,   75,
+       75,   75,   76,   76,   76,   76,   76,   76,   76,   76,
+       77,    0,   77,   77,   77,   77,   77,   77,   78,    0,
+       78,   78,   78,   78,   78,   78,   79,   79,    0,   79,
+       80,    0,   80,   81,   81,   81,   82,    0,   82,   83,
+       83,   83,   83,   83,   83,   83,   83,   84,   84,   84,
+       84,   84,   84,   84,   84,   72,   72,   72,   72,   72,
+       72,   72,   72,   72,   72,   72,   72,   72,   72,   72,
+       72,   72,   72,   72,   72,   72,   72,   72,   72,   72,
+       72,   72,   72,   72
+
+    } ;
+
+static yy_state_type yy_last_accepting_state;
+static char *yy_last_accepting_cpos;
+
+extern int yy_flex_debug;
+int yy_flex_debug = 0;
+
+/* The intent behind this definition is that it'll catch
+ * any uses of REJECT which flex missed.
+ */
+#define REJECT reject_used_but_not_detected
+#define yymore() yymore_used_but_not_detected
+#define YY_MORE_ADJ 0
+#define YY_RESTORE_YY_MORE_OFFSET
+char *yytext;
+/* Lexical analysis for genksyms.
+   Copyright 1996, 1997 Linux International.
+
+   New implementation contributed by Richard Henderson <rth@tamu.edu>
+   Based on original work by Bjorn Ekwall <bj0rn@blox.se>
+
+   Taken from Linux modutils 2.4.22.
+
+   This program is free software; you can redistribute it and/or modify it
+   under the terms of the GNU General Public License as published by the
+   Free Software Foundation; either version 2 of the License, or (at your
+   option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software Foundation,
+   Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.  */
+
+#include <limits.h>
+#include <stdlib.h>
+#include <string.h>
+#include <ctype.h>
+
+#include "genksyms.h"
+#include "parse.tab.h"
+
+/* We've got a two-level lexer here.  We let flex do basic tokenization
+   and then we categorize those basic tokens in the second stage.  */
+#define YY_DECL		static int yylex1(void)
+
+/* We don't do multiple input files.  */
+#define YY_NO_INPUT 1
+
+#define INITIAL 0
+
+#ifndef YY_NO_UNISTD_H
+/* Special case for "unistd.h", since it is non-ANSI. We include it way
+ * down here because we want the user's section 1 to have been scanned first.
+ * The user has a chance to override it with an option.
+ */
+#include <unistd.h>
+#endif
+
+#ifndef YY_EXTRA_TYPE
+#define YY_EXTRA_TYPE void *
+#endif
+
+static int yy_init_globals (void );
+
+/* Accessor methods to globals.
+   These are made visible to non-reentrant scanners for convenience. */
+
+int yylex_destroy (void );
+
+int yyget_debug (void );
+
+void yyset_debug (int debug_flag  );
+
+YY_EXTRA_TYPE yyget_extra (void );
+
+void yyset_extra (YY_EXTRA_TYPE user_defined  );
+
+FILE *yyget_in (void );
+
+void yyset_in  (FILE * in_str  );
+
+FILE *yyget_out (void );
+
+void yyset_out  (FILE * out_str  );
+
+int yyget_leng (void );
+
+char *yyget_text (void );
+
+int yyget_lineno (void );
+
+void yyset_lineno (int line_number  );
+
+/* Macros after this point can all be overridden by user definitions in
+ * section 1.
+ */
+
+#ifndef YY_SKIP_YYWRAP
+#ifdef __cplusplus
+extern "C" int yywrap (void );
+#else
+extern int yywrap (void );
+#endif
+#endif
+
+    static void yyunput (int c,char *buf_ptr  );
+    
+#ifndef yytext_ptr
+static void yy_flex_strncpy (char *,yyconst char *,int );
+#endif
+
+#ifdef YY_NEED_STRLEN
+static int yy_flex_strlen (yyconst char * );
+#endif
+
+#ifndef YY_NO_INPUT
+
+#ifdef __cplusplus
+static int yyinput (void );
+#else
+static int input (void );
+#endif
+
+#endif
+
+/* Amount of stuff to slurp up with each read. */
+#ifndef YY_READ_BUF_SIZE
+#define YY_READ_BUF_SIZE 8192
+#endif
+
+/* Copy whatever the last rule matched to the standard output. */
+#ifndef ECHO
+/* This used to be an fputs(), but since the string might contain NUL's,
+ * we now use fwrite().
+ */
+#define ECHO fwrite( yytext, yyleng, 1, yyout )
+#endif
+
+/* Gets input and stuffs it into "buf".  number of characters read, or YY_NULL,
+ * is returned in "result".
+ */
+#ifndef YY_INPUT
+#define YY_INPUT(buf,result,max_size) \
+	if ( YY_CURRENT_BUFFER_LVALUE->yy_is_interactive ) \
+		{ \
+		int c = '*'; \
+		int n; \
+		for ( n = 0; n < max_size && \
+			     (c = getc( yyin )) != EOF && c != '\n'; ++n ) \
+			buf[n] = (char) c; \
+		if ( c == '\n' ) \
+			buf[n++] = (char) c; \
+		if ( c == EOF && ferror( yyin ) ) \
+			YY_FATAL_ERROR( "input in flex scanner failed" ); \
+		result = n; \
+		} \
+	else \
+		{ \
+		errno=0; \
+		while ( (result = fread(buf, 1, max_size, yyin))==0 && ferror(yyin)) \
+			{ \
+			if( errno != EINTR) \
+				{ \
+				YY_FATAL_ERROR( "input in flex scanner failed" ); \
+				break; \
+				} \
+			errno=0; \
+			clearerr(yyin); \
+			} \
+		}\
+\
+
+#endif
+
+/* No semi-colon after return; correct usage is to write "yyterminate();" -
+ * we don't want an extra ';' after the "return" because that will cause
+ * some compilers to complain about unreachable statements.
+ */
+#ifndef yyterminate
+#define yyterminate() return YY_NULL
+#endif
+
+/* Number of entries by which start-condition stack grows. */
+#ifndef YY_START_STACK_INCR
+#define YY_START_STACK_INCR 25
+#endif
+
+/* Report a fatal error. */
+#ifndef YY_FATAL_ERROR
+#define YY_FATAL_ERROR(msg) yy_fatal_error( msg )
+#endif
+
+/* end tables serialization structures and prototypes */
+
+/* Default declaration of generated scanner - a define so the user can
+ * easily add parameters.
+ */
+#ifndef YY_DECL
+#define YY_DECL_IS_OURS 1
+
+extern int yylex (void);
+
+#define YY_DECL int yylex (void)
+#endif /* !YY_DECL */
+
+/* Code executed at the beginning of each rule, after yytext and yyleng
+ * have been set up.
+ */
+#ifndef YY_USER_ACTION
+#define YY_USER_ACTION
+#endif
+
+/* Code executed at the end of each rule. */
+#ifndef YY_BREAK
+#define YY_BREAK break;
+#endif
+
+#define YY_RULE_SETUP \
+	if ( yyleng > 0 ) \
+		YY_CURRENT_BUFFER_LVALUE->yy_at_bol = \
+				(yytext[yyleng - 1] == '\n'); \
+	YY_USER_ACTION
+
+/** The main scanner function which does all the work.
+ */
+YY_DECL
+{
+	register yy_state_type yy_current_state;
+	register char *yy_cp, *yy_bp;
+	register int yy_act;
+    
+ /* Keep track of our location in the original source files.  */
+
+	if ( !(yy_init) )
+		{
+		(yy_init) = 1;
+
+#ifdef YY_USER_INIT
+		YY_USER_INIT;
+#endif
+
+		if ( ! (yy_start) )
+			(yy_start) = 1;	/* first start state */
+
+		if ( ! yyin )
+			yyin = stdin;
+
+		if ( ! yyout )
+			yyout = stdout;
+
+		if ( ! YY_CURRENT_BUFFER ) {
+			yyensure_buffer_stack ();
+			YY_CURRENT_BUFFER_LVALUE =
+				yy_create_buffer(yyin,YY_BUF_SIZE );
+		}
+
+		yy_load_buffer_state( );
+		}
+
+	while ( 1 )		/* loops until end-of-file is reached */
+		{
+		yy_cp = (yy_c_buf_p);
+
+		/* Support of yytext. */
+		*yy_cp = (yy_hold_char);
+
+		/* yy_bp points to the position in yy_ch_buf of the start of
+		 * the current run.
+		 */
+		yy_bp = yy_cp;
+
+		yy_current_state = (yy_start);
+		yy_current_state += YY_AT_BOL();
+yy_match:
+		do
+			{
+			register YY_CHAR yy_c = yy_ec[YY_SC_TO_UI(*yy_cp)];
+			if ( yy_accept[yy_current_state] )
+				{
+				(yy_last_accepting_state) = yy_current_state;
+				(yy_last_accepting_cpos) = yy_cp;
+				}
+			while ( yy_chk[yy_base[yy_current_state] + yy_c] != yy_current_state )
+				{
+				yy_current_state = (int) yy_def[yy_current_state];
+				if ( yy_current_state >= 73 )
+					yy_c = yy_meta[(unsigned int) yy_c];
+				}
+			yy_current_state = yy_nxt[yy_base[yy_current_state] + (unsigned int) yy_c];
+			++yy_cp;
+			}
+		while ( yy_base[yy_current_state] != 266 );
+
+yy_find_action:
+		yy_act = yy_accept[yy_current_state];
+		if ( yy_act == 0 )
+			{ /* have to back up */
+			yy_cp = (yy_last_accepting_cpos);
+			yy_current_state = (yy_last_accepting_state);
+			yy_act = yy_accept[yy_current_state];
+			}
+
+		YY_DO_BEFORE_ACTION;
+
+do_action:	/* This label is used only to access EOF actions. */
+
+		switch ( yy_act )
+	{ /* beginning of action switch */
+			case 0: /* must back up */
+			/* undo the effects of YY_DO_BEFORE_ACTION */
+			*yy_cp = (yy_hold_char);
+			yy_cp = (yy_last_accepting_cpos);
+			yy_current_state = (yy_last_accepting_state);
+			goto yy_find_action;
+
+case 1:
+/* rule 1 can match eol */
+YY_RULE_SETUP
+return FILENAME;
+	YY_BREAK
+case 2:
+/* rule 2 can match eol */
+YY_RULE_SETUP
+cur_line++;
+	YY_BREAK
+case 3:
+/* rule 3 can match eol */
+YY_RULE_SETUP
+cur_line++;
+	YY_BREAK
+/* Ignore all other whitespace.  */
+case 4:
+YY_RULE_SETUP
+;
+	YY_BREAK
+case 5:
+/* rule 5 can match eol */
+YY_RULE_SETUP
+return STRING;
+	YY_BREAK
+case 6:
+/* rule 6 can match eol */
+YY_RULE_SETUP
+return CHAR;
+	YY_BREAK
+case 7:
+YY_RULE_SETUP
+return IDENT;
+	YY_BREAK
+/* The Pedant requires that the other C multi-character tokens be
+    recognized as tokens.  We don't actually use them since we don't
+    parse expressions, but we do want whitespace to be arranged
+    around them properly.  */
+case 8:
+YY_RULE_SETUP
+return OTHER;
+	YY_BREAK
+case 9:
+YY_RULE_SETUP
+return INT;
+	YY_BREAK
+case 10:
+YY_RULE_SETUP
+return REAL;
+	YY_BREAK
+case 11:
+YY_RULE_SETUP
+return DOTS;
+	YY_BREAK
+/* All other tokens are single characters.  */
+case 12:
+YY_RULE_SETUP
+return yytext[0];
+	YY_BREAK
+case 13:
+YY_RULE_SETUP
+ECHO;
+	YY_BREAK
+case YY_STATE_EOF(INITIAL):
+	yyterminate();
+
+	case YY_END_OF_BUFFER:
+		{
+		/* Amount of text matched not including the EOB char. */
+		int yy_amount_of_matched_text = (int) (yy_cp - (yytext_ptr)) - 1;
+
+		/* Undo the effects of YY_DO_BEFORE_ACTION. */
+		*yy_cp = (yy_hold_char);
+		YY_RESTORE_YY_MORE_OFFSET
+
+		if ( YY_CURRENT_BUFFER_LVALUE->yy_buffer_status == YY_BUFFER_NEW )
+			{
+			/* We're scanning a new file or input source.  It's
+			 * possible that this happened because the user
+			 * just pointed yyin at a new source and called
+			 * yylex().  If so, then we have to assure
+			 * consistency between YY_CURRENT_BUFFER and our
+			 * globals.  Here is the right place to do so, because
+			 * this is the first action (other than possibly a
+			 * back-up) that will match for the new input source.
+			 */
+			(yy_n_chars) = YY_CURRENT_BUFFER_LVALUE->yy_n_chars;
+			YY_CURRENT_BUFFER_LVALUE->yy_input_file = yyin;
+			YY_CURRENT_BUFFER_LVALUE->yy_buffer_status = YY_BUFFER_NORMAL;
+			}
+
+		/* Note that here we test for yy_c_buf_p "<=" to the position
+		 * of the first EOB in the buffer, since yy_c_buf_p will
+		 * already have been incremented past the NUL character
+		 * (since all states make transitions on EOB to the
+		 * end-of-buffer state).  Contrast this with the test
+		 * in input().
+		 */
+		if ( (yy_c_buf_p) <= &YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[(yy_n_chars)] )
+			{ /* This was really a NUL. */
+			yy_state_type yy_next_state;
+
+			(yy_c_buf_p) = (yytext_ptr) + yy_amount_of_matched_text;
+
+			yy_current_state = yy_get_previous_state(  );
+
+			/* Okay, we're now positioned to make the NUL
+			 * transition.  We couldn't have
+			 * yy_get_previous_state() go ahead and do it
+			 * for us because it doesn't know how to deal
+			 * with the possibility of jamming (and we don't
+			 * want to build jamming into it because then it
+			 * will run more slowly).
+			 */
+
+			yy_next_state = yy_try_NUL_trans( yy_current_state );
+
+			yy_bp = (yytext_ptr) + YY_MORE_ADJ;
+
+			if ( yy_next_state )
+				{
+				/* Consume the NUL. */
+				yy_cp = ++(yy_c_buf_p);
+				yy_current_state = yy_next_state;
+				goto yy_match;
+				}
+
+			else
+				{
+				yy_cp = (yy_c_buf_p);
+				goto yy_find_action;
+				}
+			}
+
+		else switch ( yy_get_next_buffer(  ) )
+			{
+			case EOB_ACT_END_OF_FILE:
+				{
+				(yy_did_buffer_switch_on_eof) = 0;
+
+				if ( yywrap( ) )
+					{
+					/* Note: because we've taken care in
+					 * yy_get_next_buffer() to have set up
+					 * yytext, we can now set up
+					 * yy_c_buf_p so that if some total
+					 * hoser (like flex itself) wants to
+					 * call the scanner after we return the
+					 * YY_NULL, it'll still work - another
+					 * YY_NULL will get returned.
+					 */
+					(yy_c_buf_p) = (yytext_ptr) + YY_MORE_ADJ;
+
+					yy_act = YY_STATE_EOF(YY_START);
+					goto do_action;
+					}
+
+				else
+					{
+					if ( ! (yy_did_buffer_switch_on_eof) )
+						YY_NEW_FILE;
+					}
+				break;
+				}
+
+			case EOB_ACT_CONTINUE_SCAN:
+				(yy_c_buf_p) =
+					(yytext_ptr) + yy_amount_of_matched_text;
+
+				yy_current_state = yy_get_previous_state(  );
+
+				yy_cp = (yy_c_buf_p);
+				yy_bp = (yytext_ptr) + YY_MORE_ADJ;
+				goto yy_match;
+
+			case EOB_ACT_LAST_MATCH:
+				(yy_c_buf_p) =
+				&YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[(yy_n_chars)];
+
+				yy_current_state = yy_get_previous_state(  );
+
+				yy_cp = (yy_c_buf_p);
+				yy_bp = (yytext_ptr) + YY_MORE_ADJ;
+				goto yy_find_action;
+			}
+		break;
+		}
+
+	default:
+		YY_FATAL_ERROR(
+			"fatal flex scanner internal error--no action found" );
+	} /* end of action switch */
+		} /* end of scanning one token */
+} /* end of yylex */
+
+/* yy_get_next_buffer - try to read in a new buffer
+ *
+ * Returns a code representing an action:
+ *	EOB_ACT_LAST_MATCH -
+ *	EOB_ACT_CONTINUE_SCAN - continue scanning from current position
+ *	EOB_ACT_END_OF_FILE - end of file
+ */
+static int yy_get_next_buffer (void)
+{
+    	register char *dest = YY_CURRENT_BUFFER_LVALUE->yy_ch_buf;
+	register char *source = (yytext_ptr);
+	register int number_to_move, i;
+	int ret_val;
+
+	if ( (yy_c_buf_p) > &YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[(yy_n_chars) + 1] )
+		YY_FATAL_ERROR(
+		"fatal flex scanner internal error--end of buffer missed" );
+
+	if ( YY_CURRENT_BUFFER_LVALUE->yy_fill_buffer == 0 )
+		{ /* Don't try to fill the buffer, so this is an EOF. */
+		if ( (yy_c_buf_p) - (yytext_ptr) - YY_MORE_ADJ == 1 )
+			{
+			/* We matched a single character, the EOB, so
+			 * treat this as a final EOF.
+			 */
+			return EOB_ACT_END_OF_FILE;
+			}
+
+		else
+			{
+			/* We matched some text prior to the EOB, first
+			 * process it.
+			 */
+			return EOB_ACT_LAST_MATCH;
+			}
+		}
+
+	/* Try to read more data. */
+
+	/* First move last chars to start of buffer. */
+	number_to_move = (int) ((yy_c_buf_p) - (yytext_ptr)) - 1;
+
+	for ( i = 0; i < number_to_move; ++i )
+		*(dest++) = *(source++);
+
+	if ( YY_CURRENT_BUFFER_LVALUE->yy_buffer_status == YY_BUFFER_EOF_PENDING )
+		/* don't do the read, it's not guaranteed to return an EOF,
+		 * just force an EOF
+		 */
+		YY_CURRENT_BUFFER_LVALUE->yy_n_chars = (yy_n_chars) = 0;
+
+	else
+		{
+			int num_to_read =
+			YY_CURRENT_BUFFER_LVALUE->yy_buf_size - number_to_move - 1;
+
+		while ( num_to_read <= 0 )
+			{ /* Not enough room in the buffer - grow it. */
+
+			/* just a shorter name for the current buffer */
+			YY_BUFFER_STATE b = YY_CURRENT_BUFFER;
+
+			int yy_c_buf_p_offset =
+				(int) ((yy_c_buf_p) - b->yy_ch_buf);
+
+			if ( b->yy_is_our_buffer )
+				{
+				int new_size = b->yy_buf_size * 2;
+
+				if ( new_size <= 0 )
+					b->yy_buf_size += b->yy_buf_size / 8;
+				else
+					b->yy_buf_size *= 2;
+
+				b->yy_ch_buf = (char *)
+					/* Include room in for 2 EOB chars. */
+					yyrealloc((void *) b->yy_ch_buf,b->yy_buf_size + 2  );
+				}
+			else
+				/* Can't grow it, we don't own it. */
+				b->yy_ch_buf = 0;
+
+			if ( ! b->yy_ch_buf )
+				YY_FATAL_ERROR(
+				"fatal error - scanner input buffer overflow" );
+
+			(yy_c_buf_p) = &b->yy_ch_buf[yy_c_buf_p_offset];
+
+			num_to_read = YY_CURRENT_BUFFER_LVALUE->yy_buf_size -
+						number_to_move - 1;
+
+			}
+
+		if ( num_to_read > YY_READ_BUF_SIZE )
+			num_to_read = YY_READ_BUF_SIZE;
+
+		/* Read in more data. */
+		YY_INPUT( (&YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[number_to_move]),
+			(yy_n_chars), (size_t) num_to_read );
+
+		YY_CURRENT_BUFFER_LVALUE->yy_n_chars = (yy_n_chars);
+		}
+
+	if ( (yy_n_chars) == 0 )
+		{
+		if ( number_to_move == YY_MORE_ADJ )
+			{
+			ret_val = EOB_ACT_END_OF_FILE;
+			yyrestart(yyin  );
+			}
+
+		else
+			{
+			ret_val = EOB_ACT_LAST_MATCH;
+			YY_CURRENT_BUFFER_LVALUE->yy_buffer_status =
+				YY_BUFFER_EOF_PENDING;
+			}
+		}
+
+	else
+		ret_val = EOB_ACT_CONTINUE_SCAN;
+
+	if ((yy_size_t) ((yy_n_chars) + number_to_move) > YY_CURRENT_BUFFER_LVALUE->yy_buf_size) {
+		/* Extend the array by 50%, plus the number we really need. */
+		yy_size_t new_size = (yy_n_chars) + number_to_move + ((yy_n_chars) >> 1);
+		YY_CURRENT_BUFFER_LVALUE->yy_ch_buf = (char *) yyrealloc((void *) YY_CURRENT_BUFFER_LVALUE->yy_ch_buf,new_size  );
+		if ( ! YY_CURRENT_BUFFER_LVALUE->yy_ch_buf )
+			YY_FATAL_ERROR( "out of dynamic memory in yy_get_next_buffer()" );
+	}
+
+	(yy_n_chars) += number_to_move;
+	YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[(yy_n_chars)] = YY_END_OF_BUFFER_CHAR;
+	YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[(yy_n_chars) + 1] = YY_END_OF_BUFFER_CHAR;
+
+	(yytext_ptr) = &YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[0];
+
+	return ret_val;
+}
+
+/* yy_get_previous_state - get the state just before the EOB char was reached */
+
+    static yy_state_type yy_get_previous_state (void)
+{
+	register yy_state_type yy_current_state;
+	register char *yy_cp;
+    
+	yy_current_state = (yy_start);
+	yy_current_state += YY_AT_BOL();
+
+	for ( yy_cp = (yytext_ptr) + YY_MORE_ADJ; yy_cp < (yy_c_buf_p); ++yy_cp )
+		{
+		register YY_CHAR yy_c = (*yy_cp ? yy_ec[YY_SC_TO_UI(*yy_cp)] : 1);
+		if ( yy_accept[yy_current_state] )
+			{
+			(yy_last_accepting_state) = yy_current_state;
+			(yy_last_accepting_cpos) = yy_cp;
+			}
+		while ( yy_chk[yy_base[yy_current_state] + yy_c] != yy_current_state )
+			{
+			yy_current_state = (int) yy_def[yy_current_state];
+			if ( yy_current_state >= 73 )
+				yy_c = yy_meta[(unsigned int) yy_c];
+			}
+		yy_current_state = yy_nxt[yy_base[yy_current_state] + (unsigned int) yy_c];
+		}
+
+	return yy_current_state;
+}
+
+/* yy_try_NUL_trans - try to make a transition on the NUL character
+ *
+ * synopsis
+ *	next_state = yy_try_NUL_trans( current_state );
+ */
+    static yy_state_type yy_try_NUL_trans  (yy_state_type yy_current_state )
+{
+	register int yy_is_jam;
+    	register char *yy_cp = (yy_c_buf_p);
+
+	register YY_CHAR yy_c = 1;
+	if ( yy_accept[yy_current_state] )
+		{
+		(yy_last_accepting_state) = yy_current_state;
+		(yy_last_accepting_cpos) = yy_cp;
+		}
+	while ( yy_chk[yy_base[yy_current_state] + yy_c] != yy_current_state )
+		{
+		yy_current_state = (int) yy_def[yy_current_state];
+		if ( yy_current_state >= 73 )
+			yy_c = yy_meta[(unsigned int) yy_c];
+		}
+	yy_current_state = yy_nxt[yy_base[yy_current_state] + (unsigned int) yy_c];
+	yy_is_jam = (yy_current_state == 72);
+
+	return yy_is_jam ? 0 : yy_current_state;
+}
+
+    static void yyunput (int c, register char * yy_bp )
+{
+	register char *yy_cp;
+    
+    yy_cp = (yy_c_buf_p);
+
+	/* undo effects of setting up yytext */
+	*yy_cp = (yy_hold_char);
+
+	if ( yy_cp < YY_CURRENT_BUFFER_LVALUE->yy_ch_buf + 2 )
+		{ /* need to shift things up to make room */
+		/* +2 for EOB chars. */
+		register int number_to_move = (yy_n_chars) + 2;
+		register char *dest = &YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[
+					YY_CURRENT_BUFFER_LVALUE->yy_buf_size + 2];
+		register char *source =
+				&YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[number_to_move];
+
+		while ( source > YY_CURRENT_BUFFER_LVALUE->yy_ch_buf )
+			*--dest = *--source;
+
+		yy_cp += (int) (dest - source);
+		yy_bp += (int) (dest - source);
+		YY_CURRENT_BUFFER_LVALUE->yy_n_chars =
+			(yy_n_chars) = YY_CURRENT_BUFFER_LVALUE->yy_buf_size;
+
+		if ( yy_cp < YY_CURRENT_BUFFER_LVALUE->yy_ch_buf + 2 )
+			YY_FATAL_ERROR( "flex scanner push-back overflow" );
+		}
+
+	*--yy_cp = (char) c;
+
+	(yytext_ptr) = yy_bp;
+	(yy_hold_char) = *yy_cp;
+	(yy_c_buf_p) = yy_cp;
+}
+
+#ifndef YY_NO_INPUT
+#ifdef __cplusplus
+    static int yyinput (void)
+#else
+    static int input  (void)
+#endif
+
+{
+	int c;
+    
+	*(yy_c_buf_p) = (yy_hold_char);
+
+	if ( *(yy_c_buf_p) == YY_END_OF_BUFFER_CHAR )
+		{
+		/* yy_c_buf_p now points to the character we want to return.
+		 * If this occurs *before* the EOB characters, then it's a
+		 * valid NUL; if not, then we've hit the end of the buffer.
+		 */
+		if ( (yy_c_buf_p) < &YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[(yy_n_chars)] )
+			/* This was really a NUL. */
+			*(yy_c_buf_p) = '\0';
+
+		else
+			{ /* need more input */
+			int offset = (yy_c_buf_p) - (yytext_ptr);
+			++(yy_c_buf_p);
+
+			switch ( yy_get_next_buffer(  ) )
+				{
+				case EOB_ACT_LAST_MATCH:
+					/* This happens because yy_g_n_b()
+					 * sees that we've accumulated a
+					 * token and flags that we need to
+					 * try matching the token before
+					 * proceeding.  But for input(),
+					 * there's no matching to consider.
+					 * So convert the EOB_ACT_LAST_MATCH
+					 * to EOB_ACT_END_OF_FILE.
+					 */
+
+					/* Reset buffer status. */
+					yyrestart(yyin );
+
+					/*FALLTHROUGH*/
+
+				case EOB_ACT_END_OF_FILE:
+					{
+					if ( yywrap( ) )
+						return EOF;
+
+					if ( ! (yy_did_buffer_switch_on_eof) )
+						YY_NEW_FILE;
+#ifdef __cplusplus
+					return yyinput();
+#else
+					return input();
+#endif
+					}
+
+				case EOB_ACT_CONTINUE_SCAN:
+					(yy_c_buf_p) = (yytext_ptr) + offset;
+					break;
+				}
+			}
+		}
+
+	c = *(unsigned char *) (yy_c_buf_p);	/* cast for 8-bit char's */
+	*(yy_c_buf_p) = '\0';	/* preserve yytext */
+	(yy_hold_char) = *++(yy_c_buf_p);
+
+	YY_CURRENT_BUFFER_LVALUE->yy_at_bol = (c == '\n');
+
+	return c;
+}
+#endif	/* ifndef YY_NO_INPUT */
+
+/** Immediately switch to a different input stream.
+ * @param input_file A readable stream.
+ * 
+ * @note This function does not reset the start condition to @c INITIAL .
+ */
+    void yyrestart  (FILE * input_file )
+{
+    
+	if ( ! YY_CURRENT_BUFFER ){
+        yyensure_buffer_stack ();
+		YY_CURRENT_BUFFER_LVALUE =
+            yy_create_buffer(yyin,YY_BUF_SIZE );
+	}
+
+	yy_init_buffer(YY_CURRENT_BUFFER,input_file );
+	yy_load_buffer_state( );
+}
+
+/** Switch to a different input buffer.
+ * @param new_buffer The new input buffer.
+ * 
+ */
+    void yy_switch_to_buffer  (YY_BUFFER_STATE  new_buffer )
+{
+    
+	/* TODO. We should be able to replace this entire function body
+	 * with
+	 *		yypop_buffer_state();
+	 *		yypush_buffer_state(new_buffer);
+     */
+	yyensure_buffer_stack ();
+	if ( YY_CURRENT_BUFFER == new_buffer )
+		return;
+
+	if ( YY_CURRENT_BUFFER )
+		{
+		/* Flush out information for old buffer. */
+		*(yy_c_buf_p) = (yy_hold_char);
+		YY_CURRENT_BUFFER_LVALUE->yy_buf_pos = (yy_c_buf_p);
+		YY_CURRENT_BUFFER_LVALUE->yy_n_chars = (yy_n_chars);
+		}
+
+	YY_CURRENT_BUFFER_LVALUE = new_buffer;
+	yy_load_buffer_state( );
+
+	/* We don't actually know whether we did this switch during
+	 * EOF (yywrap()) processing, but the only time this flag
+	 * is looked at is after yywrap() is called, so it's safe
+	 * to go ahead and always set it.
+	 */
+	(yy_did_buffer_switch_on_eof) = 1;
+}
+
+static void yy_load_buffer_state  (void)
+{
+    	(yy_n_chars) = YY_CURRENT_BUFFER_LVALUE->yy_n_chars;
+	(yytext_ptr) = (yy_c_buf_p) = YY_CURRENT_BUFFER_LVALUE->yy_buf_pos;
+	yyin = YY_CURRENT_BUFFER_LVALUE->yy_input_file;
+	(yy_hold_char) = *(yy_c_buf_p);
+}
+
+/** Allocate and initialize an input buffer state.
+ * @param file A readable stream.
+ * @param size The character buffer size in bytes. When in doubt, use @c YY_BUF_SIZE.
+ * 
+ * @return the allocated buffer state.
+ */
+    YY_BUFFER_STATE yy_create_buffer  (FILE * file, int  size )
+{
+	YY_BUFFER_STATE b;
+    
+	b = (YY_BUFFER_STATE) yyalloc(sizeof( struct yy_buffer_state )  );
+	if ( ! b )
+		YY_FATAL_ERROR( "out of dynamic memory in yy_create_buffer()" );
+
+	b->yy_buf_size = size;
+
+	/* yy_ch_buf has to be 2 characters longer than the size given because
+	 * we need to put in 2 end-of-buffer characters.
+	 */
+	b->yy_ch_buf = (char *) yyalloc(b->yy_buf_size + 2  );
+	if ( ! b->yy_ch_buf )
+		YY_FATAL_ERROR( "out of dynamic memory in yy_create_buffer()" );
+
+	b->yy_is_our_buffer = 1;
+
+	yy_init_buffer(b,file );
+
+	return b;
+}
+
+/** Destroy the buffer.
+ * @param b a buffer created with yy_create_buffer()
+ * 
+ */
+    void yy_delete_buffer (YY_BUFFER_STATE  b )
+{
+    
+	if ( ! b )
+		return;
+
+	if ( b == YY_CURRENT_BUFFER ) /* Not sure if we should pop here. */
+		YY_CURRENT_BUFFER_LVALUE = (YY_BUFFER_STATE) 0;
+
+	if ( b->yy_is_our_buffer )
+		yyfree((void *) b->yy_ch_buf  );
+
+	yyfree((void *) b  );
+}
+
+#ifndef __cplusplus
+extern int isatty (int );
+#endif /* __cplusplus */
+    
+/* Initializes or reinitializes a buffer.
+ * This function is sometimes called more than once on the same buffer,
+ * such as during a yyrestart() or at EOF.
+ */
+    static void yy_init_buffer  (YY_BUFFER_STATE  b, FILE * file )
+
+{
+	int oerrno = errno;
+    
+	yy_flush_buffer(b );
+
+	b->yy_input_file = file;
+	b->yy_fill_buffer = 1;
+
+    /* If b is the current buffer, then yy_init_buffer was _probably_
+     * called from yyrestart() or through yy_get_next_buffer.
+     * In that case, we don't want to reset the lineno or column.
+     */
+    if (b != YY_CURRENT_BUFFER){
+        b->yy_bs_lineno = 1;
+        b->yy_bs_column = 0;
+    }
+
+        b->yy_is_interactive = file ? (isatty( fileno(file) ) > 0) : 0;
+    
+	errno = oerrno;
+}
+
+/** Discard all buffered characters. On the next scan, YY_INPUT will be called.
+ * @param b the buffer state to be flushed, usually @c YY_CURRENT_BUFFER.
+ * 
+ */
+    void yy_flush_buffer (YY_BUFFER_STATE  b )
+{
+    	if ( ! b )
+		return;
+
+	b->yy_n_chars = 0;
+
+	/* We always need two end-of-buffer characters.  The first causes
+	 * a transition to the end-of-buffer state.  The second causes
+	 * a jam in that state.
+	 */
+	b->yy_ch_buf[0] = YY_END_OF_BUFFER_CHAR;
+	b->yy_ch_buf[1] = YY_END_OF_BUFFER_CHAR;
+
+	b->yy_buf_pos = &b->yy_ch_buf[0];
+
+	b->yy_at_bol = 1;
+	b->yy_buffer_status = YY_BUFFER_NEW;
+
+	if ( b == YY_CURRENT_BUFFER )
+		yy_load_buffer_state( );
+}
+
+/** Pushes the new state onto the stack. The new state becomes
+ *  the current state. This function will allocate the stack
+ *  if necessary.
+ *  @param new_buffer The new state.
+ *  
+ */
+void yypush_buffer_state (YY_BUFFER_STATE new_buffer )
+{
+    	if (new_buffer == NULL)
+		return;
+
+	yyensure_buffer_stack();
+
+	/* This block is copied from yy_switch_to_buffer. */
+	if ( YY_CURRENT_BUFFER )
+		{
+		/* Flush out information for old buffer. */
+		*(yy_c_buf_p) = (yy_hold_char);
+		YY_CURRENT_BUFFER_LVALUE->yy_buf_pos = (yy_c_buf_p);
+		YY_CURRENT_BUFFER_LVALUE->yy_n_chars = (yy_n_chars);
+		}
+
+	/* Only push if top exists. Otherwise, replace top. */
+	if (YY_CURRENT_BUFFER)
+		(yy_buffer_stack_top)++;
+	YY_CURRENT_BUFFER_LVALUE = new_buffer;
+
+	/* copied from yy_switch_to_buffer. */
+	yy_load_buffer_state( );
+	(yy_did_buffer_switch_on_eof) = 1;
+}
+
+/** Removes and deletes the top of the stack, if present.
+ *  The next element becomes the new top.
+ *  
+ */
+void yypop_buffer_state (void)
+{
+    	if (!YY_CURRENT_BUFFER)
+		return;
+
+	yy_delete_buffer(YY_CURRENT_BUFFER );
+	YY_CURRENT_BUFFER_LVALUE = NULL;
+	if ((yy_buffer_stack_top) > 0)
+		--(yy_buffer_stack_top);
+
+	if (YY_CURRENT_BUFFER) {
+		yy_load_buffer_state( );
+		(yy_did_buffer_switch_on_eof) = 1;
+	}
+}
+
+/* Allocates the stack if it does not exist.
+ *  Guarantees space for at least one push.
+ */
+static void yyensure_buffer_stack (void)
+{
+	int num_to_alloc;
+    
+	if (!(yy_buffer_stack)) {
+
+		/* First allocation is just for 2 elements, since we don't know if this
+		 * scanner will even need a stack. We use 2 instead of 1 to avoid an
+		 * immediate realloc on the next call.
+         */
+		num_to_alloc = 1;
+		(yy_buffer_stack) = (struct yy_buffer_state**)yyalloc
+								(num_to_alloc * sizeof(struct yy_buffer_state*)
+								);
+		if ( ! (yy_buffer_stack) )
+			YY_FATAL_ERROR( "out of dynamic memory in yyensure_buffer_stack()" );
+								  
+		memset((yy_buffer_stack), 0, num_to_alloc * sizeof(struct yy_buffer_state*));
+				
+		(yy_buffer_stack_max) = num_to_alloc;
+		(yy_buffer_stack_top) = 0;
+		return;
+	}
+
+	if ((yy_buffer_stack_top) >= ((yy_buffer_stack_max)) - 1){
+
+		/* Increase the buffer to prepare for a possible push. */
+		int grow_size = 8 /* arbitrary grow size */;
+
+		num_to_alloc = (yy_buffer_stack_max) + grow_size;
+		(yy_buffer_stack) = (struct yy_buffer_state**)yyrealloc
+								((yy_buffer_stack),
+								num_to_alloc * sizeof(struct yy_buffer_state*)
+								);
+		if ( ! (yy_buffer_stack) )
+			YY_FATAL_ERROR( "out of dynamic memory in yyensure_buffer_stack()" );
+
+		/* zero only the new slots.*/
+		memset((yy_buffer_stack) + (yy_buffer_stack_max), 0, grow_size * sizeof(struct yy_buffer_state*));
+		(yy_buffer_stack_max) = num_to_alloc;
+	}
+}
+
+/** Setup the input buffer state to scan directly from a user-specified character buffer.
+ * @param base the character buffer
+ * @param size the size in bytes of the character buffer
+ * 
+ * @return the newly allocated buffer state object. 
+ */
+YY_BUFFER_STATE yy_scan_buffer  (char * base, yy_size_t  size )
+{
+	YY_BUFFER_STATE b;
+    
+	if ( size < 2 ||
+	     base[size-2] != YY_END_OF_BUFFER_CHAR ||
+	     base[size-1] != YY_END_OF_BUFFER_CHAR )
+		/* They forgot to leave room for the EOB's. */
+		return 0;
+
+	b = (YY_BUFFER_STATE) yyalloc(sizeof( struct yy_buffer_state )  );
+	if ( ! b )
+		YY_FATAL_ERROR( "out of dynamic memory in yy_scan_buffer()" );
+
+	b->yy_buf_size = size - 2;	/* "- 2" to take care of EOB's */
+	b->yy_buf_pos = b->yy_ch_buf = base;
+	b->yy_is_our_buffer = 0;
+	b->yy_input_file = 0;
+	b->yy_n_chars = b->yy_buf_size;
+	b->yy_is_interactive = 0;
+	b->yy_at_bol = 1;
+	b->yy_fill_buffer = 0;
+	b->yy_buffer_status = YY_BUFFER_NEW;
+
+	yy_switch_to_buffer(b  );
+
+	return b;
+}
+
+/** Setup the input buffer state to scan a string. The next call to yylex() will
+ * scan from a @e copy of @a str.
+ * @param yystr a NUL-terminated string to scan
+ * 
+ * @return the newly allocated buffer state object.
+ * @note If you want to scan bytes that may contain NUL values, then use
+ *       yy_scan_bytes() instead.
+ */
+YY_BUFFER_STATE yy_scan_string (yyconst char * yystr )
+{
+    
+	return yy_scan_bytes(yystr,strlen(yystr) );
+}
+
+/** Setup the input buffer state to scan the given bytes. The next call to yylex() will
+ * scan from a @e copy of @a bytes.
+ * @param bytes the byte buffer to scan
+ * @param len the number of bytes in the buffer pointed to by @a bytes.
+ * 
+ * @return the newly allocated buffer state object.
+ */
+YY_BUFFER_STATE yy_scan_bytes  (yyconst char * yybytes, int  _yybytes_len )
+{
+	YY_BUFFER_STATE b;
+	char *buf;
+	yy_size_t n;
+	int i;
+    
+	/* Get memory for full buffer, including space for trailing EOB's. */
+	n = _yybytes_len + 2;
+	buf = (char *) yyalloc(n  );
+	if ( ! buf )
+		YY_FATAL_ERROR( "out of dynamic memory in yy_scan_bytes()" );
+
+	for ( i = 0; i < _yybytes_len; ++i )
+		buf[i] = yybytes[i];
+
+	buf[_yybytes_len] = buf[_yybytes_len+1] = YY_END_OF_BUFFER_CHAR;
+
+	b = yy_scan_buffer(buf,n );
+	if ( ! b )
+		YY_FATAL_ERROR( "bad buffer in yy_scan_bytes()" );
+
+	/* It's okay to grow etc. this buffer, and we should throw it
+	 * away when we're done.
+	 */
+	b->yy_is_our_buffer = 1;
+
+	return b;
+}
+
+#ifndef YY_EXIT_FAILURE
+#define YY_EXIT_FAILURE 2
+#endif
+
+static void yy_fatal_error (yyconst char* msg )
+{
+    	(void) fprintf( stderr, "%s\n", msg );
+	exit( YY_EXIT_FAILURE );
+}
+
+/* Redefine yyless() so it works in section 3 code. */
+
+#undef yyless
+#define yyless(n) \
+	do \
+		{ \
+		/* Undo effects of setting up yytext. */ \
+        int yyless_macro_arg = (n); \
+        YY_LESS_LINENO(yyless_macro_arg);\
+		yytext[yyleng] = (yy_hold_char); \
+		(yy_c_buf_p) = yytext + yyless_macro_arg; \
+		(yy_hold_char) = *(yy_c_buf_p); \
+		*(yy_c_buf_p) = '\0'; \
+		yyleng = yyless_macro_arg; \
+		} \
+	while ( 0 )
+
+/* Accessor  methods (get/set functions) to struct members. */
+
+/** Get the current line number.
+ * 
+ */
+int yyget_lineno  (void)
+{
+        
+    return yylineno;
+}
+
+/** Get the input stream.
+ * 
+ */
+FILE *yyget_in  (void)
+{
+        return yyin;
+}
+
+/** Get the output stream.
+ * 
+ */
+FILE *yyget_out  (void)
+{
+        return yyout;
+}
+
+/** Get the length of the current token.
+ * 
+ */
+int yyget_leng  (void)
+{
+        return yyleng;
+}
+
+/** Get the current token.
+ * 
+ */
+
+char *yyget_text  (void)
+{
+        return yytext;
+}
+
+/** Set the current line number.
+ * @param line_number
+ * 
+ */
+void yyset_lineno (int  line_number )
+{
+    
+    yylineno = line_number;
+}
+
+/** Set the input stream. This does not discard the current
+ * input buffer.
+ * @param in_str A readable stream.
+ * 
+ * @see yy_switch_to_buffer
+ */
+void yyset_in (FILE *  in_str )
+{
+        yyin = in_str ;
+}
+
+void yyset_out (FILE *  out_str )
+{
+        yyout = out_str ;
+}
+
+int yyget_debug  (void)
+{
+        return yy_flex_debug;
+}
+
+void yyset_debug (int  bdebug )
+{
+        yy_flex_debug = bdebug ;
+}
+
+static int yy_init_globals (void)
+{
+        /* Initialization is the same as for the non-reentrant scanner.
+     * This function is called from yylex_destroy(), so don't allocate here.
+     */
+
+    (yy_buffer_stack) = 0;
+    (yy_buffer_stack_top) = 0;
+    (yy_buffer_stack_max) = 0;
+    (yy_c_buf_p) = (char *) 0;
+    (yy_init) = 0;
+    (yy_start) = 0;
+
+/* Defined in main.c */
+#ifdef YY_STDINIT
+    yyin = stdin;
+    yyout = stdout;
+#else
+    yyin = (FILE *) 0;
+    yyout = (FILE *) 0;
+#endif
+
+    /* For future reference: Set errno on error, since we are called by
+     * yylex_init()
+     */
+    return 0;
+}
+
+/* yylex_destroy is for both reentrant and non-reentrant scanners. */
+int yylex_destroy  (void)
+{
+    
+    /* Pop the buffer stack, destroying each element. */
+	while(YY_CURRENT_BUFFER){
+		yy_delete_buffer(YY_CURRENT_BUFFER  );
+		YY_CURRENT_BUFFER_LVALUE = NULL;
+		yypop_buffer_state();
+	}
+
+	/* Destroy the stack itself. */
+	yyfree((yy_buffer_stack) );
+	(yy_buffer_stack) = NULL;
+
+    /* Reset the globals. This is important in a non-reentrant scanner so the next time
+     * yylex() is called, initialization will occur. */
+    yy_init_globals( );
+
+    return 0;
+}
+
+/*
+ * Internal utility routines.
+ */
+
+#ifndef yytext_ptr
+static void yy_flex_strncpy (char* s1, yyconst char * s2, int n )
+{
+	register int i;
+	for ( i = 0; i < n; ++i )
+		s1[i] = s2[i];
+}
+#endif
+
+#ifdef YY_NEED_STRLEN
+static int yy_flex_strlen (yyconst char * s )
+{
+	register int n;
+	for ( n = 0; s[n]; ++n )
+		;
+
+	return n;
+}
+#endif
+
+void *yyalloc (yy_size_t  size )
+{
+	return (void *) malloc( size );
+}
+
+void *yyrealloc  (void * ptr, yy_size_t  size )
+{
+	/* The cast to (char *) in the following accommodates both
+	 * implementations that use char* generic pointers, and those
+	 * that use void* generic pointers.  It works with the latter
+	 * because both ANSI C and C++ allow castless assignment from
+	 * any pointer type to void*, and deal with argument conversions
+	 * as though doing an assignment.
+	 */
+	return (void *) realloc( (char *) ptr, size );
+}
+
+void yyfree (void * ptr )
+{
+	free( (char *) ptr );	/* see yyrealloc() for (char *) cast */
+}
+
+#define YYTABLES_NAME "yytables"
+
+/* Bring in the keyword recognizer.  */
+
+#include "keywords.hash.c"
+
+/* Macros to append to our phrase collection list.  */
+
+/*
+ * We mark any token, that that equals to a known enumerator, as
+ * SYM_ENUM_CONST. The parser will change this for struct and union tags later,
+ * the only problem is struct and union members:
+ *    enum e { a, b }; struct s { int a, b; }
+ * but in this case, the only effect will be, that the ABI checksums become
+ * more volatile, which is acceptable. Also, such collisions are quite rare,
+ * so far it was only observed in include/linux/telephony.h.
+ */
+#define _APP(T,L)	do {						   \
+			  cur_node = next_node;				   \
+			  next_node = xmalloc(sizeof(*next_node));	   \
+			  next_node->next = cur_node;			   \
+			  cur_node->string = memcpy(xmalloc(L+1), T, L+1); \
+			  cur_node->tag =				   \
+			    find_symbol(cur_node->string, SYM_ENUM_CONST, 1)?\
+			    SYM_ENUM_CONST : SYM_NORMAL ;		   \
+			  cur_node->in_source_file = in_source_file;       \
+			} while (0)
+
+#define APP		_APP(yytext, yyleng)
+
+/* The second stage lexer.  Here we incorporate knowledge of the state
+   of the parser to tailor the tokens that are returned.  */
+
+int
+yylex(void)
+{
+  static enum {
+    ST_NOTSTARTED, ST_NORMAL, ST_ATTRIBUTE, ST_ASM, ST_BRACKET, ST_BRACE,
+    ST_EXPRESSION, ST_TABLE_1, ST_TABLE_2, ST_TABLE_3, ST_TABLE_4,
+    ST_TABLE_5, ST_TABLE_6
+  } lexstate = ST_NOTSTARTED;
+
+  static int suppress_type_lookup, dont_want_brace_phrase;
+  static struct string_list *next_node;
+
+  int token, count = 0;
+  struct string_list *cur_node;
+
+  if (lexstate == ST_NOTSTARTED)
+    {
+      next_node = xmalloc(sizeof(*next_node));
+      next_node->next = NULL;
+      lexstate = ST_NORMAL;
+    }
+
+repeat:
+  token = yylex1();
+
+  if (token == 0)
+    return 0;
+  else if (token == FILENAME)
+    {
+      char *file, *e;
+
+      /* Save the filename and line number for later error messages.  */
+
+      if (cur_filename)
+	free(cur_filename);
+
+      file = strchr(yytext, '\"')+1;
+      e = strchr(file, '\"');
+      *e = '\0';
+      cur_filename = memcpy(xmalloc(e-file+1), file, e-file+1);
+      cur_line = atoi(yytext+2);
+
+      if (!source_file) {
+        source_file = xstrdup(cur_filename);
+        in_source_file = 1;
+      } else {
+        in_source_file = (strcmp(cur_filename, source_file) == 0);
+      }
+
+      goto repeat;
+    }
+
+  switch (lexstate)
+    {
+    case ST_NORMAL:
+      switch (token)
+	{
+	case IDENT:
+	  APP;
+	  {
+	    const struct resword *r = is_reserved_word(yytext, yyleng);
+	    if (r)
+	      {
+		switch (token = r->token)
+		  {
+		  case ATTRIBUTE_KEYW:
+		    lexstate = ST_ATTRIBUTE;
+		    count = 0;
+		    goto repeat;
+		  case ASM_KEYW:
+		    lexstate = ST_ASM;
+		    count = 0;
+		    goto repeat;
+
+		  case STRUCT_KEYW:
+		  case UNION_KEYW:
+		  case ENUM_KEYW:
+		    dont_want_brace_phrase = 3;
+		    suppress_type_lookup = 2;
+		    goto fini;
+
+		  case EXPORT_SYMBOL_KEYW:
+		      goto fini;
+		  }
+	      }
+	    if (!suppress_type_lookup)
+	      {
+		if (find_symbol(yytext, SYM_TYPEDEF, 1))
+		  token = TYPE;
+	      }
+	  }
+	  break;
+
+	case '[':
+	  APP;
+	  lexstate = ST_BRACKET;
+	  count = 1;
+	  goto repeat;
+
+	case '{':
+	  APP;
+	  if (dont_want_brace_phrase)
+	    break;
+	  lexstate = ST_BRACE;
+	  count = 1;
+	  goto repeat;
+
+	case '=': case ':':
+	  APP;
+	  lexstate = ST_EXPRESSION;
+	  break;
+
+	case DOTS:
+	default:
+	  APP;
+	  break;
+	}
+      break;
+
+    case ST_ATTRIBUTE:
+      APP;
+      switch (token)
+	{
+	case '(':
+	  ++count;
+	  goto repeat;
+	case ')':
+	  if (--count == 0)
+	    {
+	      lexstate = ST_NORMAL;
+	      token = ATTRIBUTE_PHRASE;
+	      break;
+	    }
+	  goto repeat;
+	default:
+	  goto repeat;
+	}
+      break;
+
+    case ST_ASM:
+      APP;
+      switch (token)
+	{
+	case '(':
+	  ++count;
+	  goto repeat;
+	case ')':
+	  if (--count == 0)
+	    {
+	      lexstate = ST_NORMAL;
+	      token = ASM_PHRASE;
+	      break;
+	    }
+	  goto repeat;
+	default:
+	  goto repeat;
+	}
+      break;
+
+    case ST_BRACKET:
+      APP;
+      switch (token)
+	{
+	case '[':
+	  ++count;
+	  goto repeat;
+	case ']':
+	  if (--count == 0)
+	    {
+	      lexstate = ST_NORMAL;
+	      token = BRACKET_PHRASE;
+	      break;
+	    }
+	  goto repeat;
+	default:
+	  goto repeat;
+	}
+      break;
+
+    case ST_BRACE:
+      APP;
+      switch (token)
+	{
+	case '{':
+	  ++count;
+	  goto repeat;
+	case '}':
+	  if (--count == 0)
+	    {
+	      lexstate = ST_NORMAL;
+	      token = BRACE_PHRASE;
+	      break;
+	    }
+	  goto repeat;
+	default:
+	  goto repeat;
+	}
+      break;
+
+    case ST_EXPRESSION:
+      switch (token)
+	{
+	case '(': case '[': case '{':
+	  ++count;
+	  APP;
+	  goto repeat;
+	case '}':
+	  /* is this the last line of an enum declaration? */
+	  if (count == 0)
+	    {
+	      /* Put back the token we just read so's we can find it again
+		 after registering the expression.  */
+	      unput(token);
+
+	      lexstate = ST_NORMAL;
+	      token = EXPRESSION_PHRASE;
+	      break;
+	    }
+	  /* FALLTHRU */
+	case ')': case ']':
+	  --count;
+	  APP;
+	  goto repeat;
+	case ',': case ';':
+	  if (count == 0)
+	    {
+	      /* Put back the token we just read so's we can find it again
+		 after registering the expression.  */
+	      unput(token);
+
+	      lexstate = ST_NORMAL;
+	      token = EXPRESSION_PHRASE;
+	      break;
+	    }
+	  APP;
+	  goto repeat;
+	default:
+	  APP;
+	  goto repeat;
+	}
+      break;
+
+    case ST_TABLE_1:
+      goto repeat;
+
+    case ST_TABLE_2:
+      if (token == IDENT && yyleng == 1 && yytext[0] == 'X')
+	{
+	  token = EXPORT_SYMBOL_KEYW;
+	  lexstate = ST_TABLE_5;
+	  APP;
+	  break;
+	}
+      lexstate = ST_TABLE_6;
+      /* FALLTHRU */
+
+    case ST_TABLE_6:
+      switch (token)
+	{
+	case '{': case '[': case '(':
+	  ++count;
+	  break;
+	case '}': case ']': case ')':
+	  --count;
+	  break;
+	case ',':
+	  if (count == 0)
+	    lexstate = ST_TABLE_2;
+	  break;
+	};
+      goto repeat;
+
+    case ST_TABLE_3:
+      goto repeat;
+
+    case ST_TABLE_4:
+      if (token == ';')
+	lexstate = ST_NORMAL;
+      goto repeat;
+
+    case ST_TABLE_5:
+      switch (token)
+	{
+	case ',':
+	  token = ';';
+	  lexstate = ST_TABLE_2;
+	  APP;
+	  break;
+	default:
+	  APP;
+	  break;
+	}
+      break;
+
+    default:
+      exit(1);
+    }
+fini:
+
+  if (suppress_type_lookup > 0)
+    --suppress_type_lookup;
+  if (dont_want_brace_phrase > 0)
+    --dont_want_brace_phrase;
+
+  yylval = &next_node->next;
+
+  return token;
+}
+
diff -rubBN linux-3.2.51/scripts/genksyms/parse.tab.c ../split-release/release/linux-3.2.51/scripts/genksyms/parse.tab.c
--- linux-3.2.51/scripts/genksyms/parse.tab.c	1970-01-01 01:00:00.000000000 +0100
+++ ../split-release/release/linux-3.2.51/scripts/genksyms/parse.tab.c	2015-09-01 05:42:46.000000000 +0200
@@ -0,0 +1,2399 @@
+/* A Bison parser, made by GNU Bison 2.5.  */
+
+/* Bison implementation for Yacc-like parsers in C
+   
+      Copyright (C) 1984, 1989-1990, 2000-2011 Free Software Foundation, Inc.
+   
+   This program is free software: you can redistribute it and/or modify
+   it under the terms of the GNU General Public License as published by
+   the Free Software Foundation, either version 3 of the License, or
+   (at your option) any later version.
+   
+   This program is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+   
+   You should have received a copy of the GNU General Public License
+   along with this program.  If not, see <http://www.gnu.org/licenses/>.  */
+
+/* As a special exception, you may create a larger work that contains
+   part or all of the Bison parser skeleton and distribute that work
+   under terms of your choice, so long as that work isn't itself a
+   parser generator using the skeleton or a modified version thereof
+   as a parser skeleton.  Alternatively, if you modify or redistribute
+   the parser skeleton itself, you may (at your option) remove this
+   special exception, which will cause the skeleton and the resulting
+   Bison output files to be licensed under the GNU General Public
+   License without this special exception.
+   
+   This special exception was added by the Free Software Foundation in
+   version 2.2 of Bison.  */
+
+/* C LALR(1) parser skeleton written by Richard Stallman, by
+   simplifying the original so-called "semantic" parser.  */
+
+/* All symbols defined below should begin with yy or YY, to avoid
+   infringing on user name space.  This should be done even for local
+   variables, as they might otherwise be expanded by user macros.
+   There are some unavoidable exceptions within include files to
+   define necessary library symbols; they are noted "INFRINGES ON
+   USER NAME SPACE" below.  */
+
+/* Identify Bison output.  */
+#define YYBISON 1
+
+/* Bison version.  */
+#define YYBISON_VERSION "2.5"
+
+/* Skeleton name.  */
+#define YYSKELETON_NAME "yacc.c"
+
+/* Pure parsers.  */
+#define YYPURE 0
+
+/* Push parsers.  */
+#define YYPUSH 0
+
+/* Pull parsers.  */
+#define YYPULL 1
+
+/* Using locations.  */
+#define YYLSP_NEEDED 0
+
+
+
+/* Copy the first part of user declarations.  */
+
+
+
+#include <assert.h>
+#include <stdlib.h>
+#include <string.h>
+#include "genksyms.h"
+
+static int is_typedef;
+static int is_extern;
+static char *current_name;
+static struct string_list *decl_spec;
+
+static void yyerror(const char *);
+
+static inline void
+remove_node(struct string_list **p)
+{
+  struct string_list *node = *p;
+  *p = node->next;
+  free_node(node);
+}
+
+static inline void
+remove_list(struct string_list **pb, struct string_list **pe)
+{
+  struct string_list *b = *pb, *e = *pe;
+  *pb = e;
+  free_list(b, e);
+}
+
+/* Record definition of a struct/union/enum */
+static void record_compound(struct string_list **keyw,
+		       struct string_list **ident,
+		       struct string_list **body,
+		       enum symbol_type type)
+{
+	struct string_list *b = *body, *i = *ident, *r;
+
+	if (i->in_source_file) {
+		remove_node(keyw);
+		(*ident)->tag = type;
+		remove_list(body, ident);
+		return;
+	}
+	r = copy_node(i); r->tag = type;
+	r->next = (*keyw)->next; *body = r; (*keyw)->next = NULL;
+	add_symbol(i->string, type, b, is_extern);
+}
+
+
+
+
+/* Enabling traces.  */
+#ifndef YYDEBUG
+# define YYDEBUG 1
+#endif
+
+/* Enabling verbose error messages.  */
+#ifdef YYERROR_VERBOSE
+# undef YYERROR_VERBOSE
+# define YYERROR_VERBOSE 1
+#else
+# define YYERROR_VERBOSE 0
+#endif
+
+/* Enabling the token table.  */
+#ifndef YYTOKEN_TABLE
+# define YYTOKEN_TABLE 0
+#endif
+
+
+/* Tokens.  */
+#ifndef YYTOKENTYPE
+# define YYTOKENTYPE
+   /* Put the tokens into the symbol table, so that GDB and other debuggers
+      know about them.  */
+   enum yytokentype {
+     ASM_KEYW = 258,
+     ATTRIBUTE_KEYW = 259,
+     AUTO_KEYW = 260,
+     BOOL_KEYW = 261,
+     CHAR_KEYW = 262,
+     CONST_KEYW = 263,
+     DOUBLE_KEYW = 264,
+     ENUM_KEYW = 265,
+     EXTERN_KEYW = 266,
+     EXTENSION_KEYW = 267,
+     FLOAT_KEYW = 268,
+     INLINE_KEYW = 269,
+     INT_KEYW = 270,
+     LONG_KEYW = 271,
+     REGISTER_KEYW = 272,
+     RESTRICT_KEYW = 273,
+     SHORT_KEYW = 274,
+     SIGNED_KEYW = 275,
+     STATIC_KEYW = 276,
+     STRUCT_KEYW = 277,
+     TYPEDEF_KEYW = 278,
+     UNION_KEYW = 279,
+     UNSIGNED_KEYW = 280,
+     VOID_KEYW = 281,
+     VOLATILE_KEYW = 282,
+     TYPEOF_KEYW = 283,
+     EXPORT_SYMBOL_KEYW = 284,
+     ASM_PHRASE = 285,
+     ATTRIBUTE_PHRASE = 286,
+     BRACE_PHRASE = 287,
+     BRACKET_PHRASE = 288,
+     EXPRESSION_PHRASE = 289,
+     CHAR = 290,
+     DOTS = 291,
+     IDENT = 292,
+     INT = 293,
+     REAL = 294,
+     STRING = 295,
+     TYPE = 296,
+     OTHER = 297,
+     FILENAME = 298
+   };
+#endif
+
+
+
+#if ! defined YYSTYPE && ! defined YYSTYPE_IS_DECLARED
+typedef int YYSTYPE;
+# define YYSTYPE_IS_TRIVIAL 1
+# define yystype YYSTYPE /* obsolescent; will be withdrawn */
+# define YYSTYPE_IS_DECLARED 1
+#endif
+
+
+/* Copy the second part of user declarations.  */
+
+
+
+#ifdef short
+# undef short
+#endif
+
+#ifdef YYTYPE_UINT8
+typedef YYTYPE_UINT8 yytype_uint8;
+#else
+typedef unsigned char yytype_uint8;
+#endif
+
+#ifdef YYTYPE_INT8
+typedef YYTYPE_INT8 yytype_int8;
+#elif (defined __STDC__ || defined __C99__FUNC__ \
+     || defined __cplusplus || defined _MSC_VER)
+typedef signed char yytype_int8;
+#else
+typedef short int yytype_int8;
+#endif
+
+#ifdef YYTYPE_UINT16
+typedef YYTYPE_UINT16 yytype_uint16;
+#else
+typedef unsigned short int yytype_uint16;
+#endif
+
+#ifdef YYTYPE_INT16
+typedef YYTYPE_INT16 yytype_int16;
+#else
+typedef short int yytype_int16;
+#endif
+
+#ifndef YYSIZE_T
+# ifdef __SIZE_TYPE__
+#  define YYSIZE_T __SIZE_TYPE__
+# elif defined size_t
+#  define YYSIZE_T size_t
+# elif ! defined YYSIZE_T && (defined __STDC__ || defined __C99__FUNC__ \
+     || defined __cplusplus || defined _MSC_VER)
+#  include <stddef.h> /* INFRINGES ON USER NAME SPACE */
+#  define YYSIZE_T size_t
+# else
+#  define YYSIZE_T unsigned int
+# endif
+#endif
+
+#define YYSIZE_MAXIMUM ((YYSIZE_T) -1)
+
+#ifndef YY_
+# if defined YYENABLE_NLS && YYENABLE_NLS
+#  if ENABLE_NLS
+#   include <libintl.h> /* INFRINGES ON USER NAME SPACE */
+#   define YY_(msgid) dgettext ("bison-runtime", msgid)
+#  endif
+# endif
+# ifndef YY_
+#  define YY_(msgid) msgid
+# endif
+#endif
+
+/* Suppress unused-variable warnings by "using" E.  */
+#if ! defined lint || defined __GNUC__
+# define YYUSE(e) ((void) (e))
+#else
+# define YYUSE(e) /* empty */
+#endif
+
+/* Identity function, used to suppress warnings about constant conditions.  */
+#ifndef lint
+# define YYID(n) (n)
+#else
+#if (defined __STDC__ || defined __C99__FUNC__ \
+     || defined __cplusplus || defined _MSC_VER)
+static int
+YYID (int yyi)
+#else
+static int
+YYID (yyi)
+    int yyi;
+#endif
+{
+  return yyi;
+}
+#endif
+
+#if ! defined yyoverflow || YYERROR_VERBOSE
+
+/* The parser invokes alloca or malloc; define the necessary symbols.  */
+
+# ifdef YYSTACK_USE_ALLOCA
+#  if YYSTACK_USE_ALLOCA
+#   ifdef __GNUC__
+#    define YYSTACK_ALLOC __builtin_alloca
+#   elif defined __BUILTIN_VA_ARG_INCR
+#    include <alloca.h> /* INFRINGES ON USER NAME SPACE */
+#   elif defined _AIX
+#    define YYSTACK_ALLOC __alloca
+#   elif defined _MSC_VER
+#    include <malloc.h> /* INFRINGES ON USER NAME SPACE */
+#    define alloca _alloca
+#   else
+#    define YYSTACK_ALLOC alloca
+#    if ! defined _ALLOCA_H && ! defined EXIT_SUCCESS && (defined __STDC__ || defined __C99__FUNC__ \
+     || defined __cplusplus || defined _MSC_VER)
+#     include <stdlib.h> /* INFRINGES ON USER NAME SPACE */
+#     ifndef EXIT_SUCCESS
+#      define EXIT_SUCCESS 0
+#     endif
+#    endif
+#   endif
+#  endif
+# endif
+
+# ifdef YYSTACK_ALLOC
+   /* Pacify GCC's `empty if-body' warning.  */
+#  define YYSTACK_FREE(Ptr) do { /* empty */; } while (YYID (0))
+#  ifndef YYSTACK_ALLOC_MAXIMUM
+    /* The OS might guarantee only one guard page at the bottom of the stack,
+       and a page size can be as small as 4096 bytes.  So we cannot safely
+       invoke alloca (N) if N exceeds 4096.  Use a slightly smaller number
+       to allow for a few compiler-allocated temporary stack slots.  */
+#   define YYSTACK_ALLOC_MAXIMUM 4032 /* reasonable circa 2006 */
+#  endif
+# else
+#  define YYSTACK_ALLOC YYMALLOC
+#  define YYSTACK_FREE YYFREE
+#  ifndef YYSTACK_ALLOC_MAXIMUM
+#   define YYSTACK_ALLOC_MAXIMUM YYSIZE_MAXIMUM
+#  endif
+#  if (defined __cplusplus && ! defined EXIT_SUCCESS \
+       && ! ((defined YYMALLOC || defined malloc) \
+	     && (defined YYFREE || defined free)))
+#   include <stdlib.h> /* INFRINGES ON USER NAME SPACE */
+#   ifndef EXIT_SUCCESS
+#    define EXIT_SUCCESS 0
+#   endif
+#  endif
+#  ifndef YYMALLOC
+#   define YYMALLOC malloc
+#   if ! defined malloc && ! defined EXIT_SUCCESS && (defined __STDC__ || defined __C99__FUNC__ \
+     || defined __cplusplus || defined _MSC_VER)
+void *malloc (YYSIZE_T); /* INFRINGES ON USER NAME SPACE */
+#   endif
+#  endif
+#  ifndef YYFREE
+#   define YYFREE free
+#   if ! defined free && ! defined EXIT_SUCCESS && (defined __STDC__ || defined __C99__FUNC__ \
+     || defined __cplusplus || defined _MSC_VER)
+void free (void *); /* INFRINGES ON USER NAME SPACE */
+#   endif
+#  endif
+# endif
+#endif /* ! defined yyoverflow || YYERROR_VERBOSE */
+
+
+#if (! defined yyoverflow \
+     && (! defined __cplusplus \
+	 || (defined YYSTYPE_IS_TRIVIAL && YYSTYPE_IS_TRIVIAL)))
+
+/* A type that is properly aligned for any stack member.  */
+union yyalloc
+{
+  yytype_int16 yyss_alloc;
+  YYSTYPE yyvs_alloc;
+};
+
+/* The size of the maximum gap between one aligned stack and the next.  */
+# define YYSTACK_GAP_MAXIMUM (sizeof (union yyalloc) - 1)
+
+/* The size of an array large to enough to hold all stacks, each with
+   N elements.  */
+# define YYSTACK_BYTES(N) \
+     ((N) * (sizeof (yytype_int16) + sizeof (YYSTYPE)) \
+      + YYSTACK_GAP_MAXIMUM)
+
+# define YYCOPY_NEEDED 1
+
+/* Relocate STACK from its old location to the new one.  The
+   local variables YYSIZE and YYSTACKSIZE give the old and new number of
+   elements in the stack, and YYPTR gives the new location of the
+   stack.  Advance YYPTR to a properly aligned location for the next
+   stack.  */
+# define YYSTACK_RELOCATE(Stack_alloc, Stack)				\
+    do									\
+      {									\
+	YYSIZE_T yynewbytes;						\
+	YYCOPY (&yyptr->Stack_alloc, Stack, yysize);			\
+	Stack = &yyptr->Stack_alloc;					\
+	yynewbytes = yystacksize * sizeof (*Stack) + YYSTACK_GAP_MAXIMUM; \
+	yyptr += yynewbytes / sizeof (*yyptr);				\
+      }									\
+    while (YYID (0))
+
+#endif
+
+#if defined YYCOPY_NEEDED && YYCOPY_NEEDED
+/* Copy COUNT objects from FROM to TO.  The source and destination do
+   not overlap.  */
+# ifndef YYCOPY
+#  if defined __GNUC__ && 1 < __GNUC__
+#   define YYCOPY(To, From, Count) \
+      __builtin_memcpy (To, From, (Count) * sizeof (*(From)))
+#  else
+#   define YYCOPY(To, From, Count)		\
+      do					\
+	{					\
+	  YYSIZE_T yyi;				\
+	  for (yyi = 0; yyi < (Count); yyi++)	\
+	    (To)[yyi] = (From)[yyi];		\
+	}					\
+      while (YYID (0))
+#  endif
+# endif
+#endif /* !YYCOPY_NEEDED */
+
+/* YYFINAL -- State number of the termination state.  */
+#define YYFINAL  4
+/* YYLAST -- Last index in YYTABLE.  */
+#define YYLAST   532
+
+/* YYNTOKENS -- Number of terminals.  */
+#define YYNTOKENS  53
+/* YYNNTS -- Number of nonterminals.  */
+#define YYNNTS  49
+/* YYNRULES -- Number of rules.  */
+#define YYNRULES  132
+/* YYNRULES -- Number of states.  */
+#define YYNSTATES  188
+
+/* YYTRANSLATE(YYLEX) -- Bison symbol number corresponding to YYLEX.  */
+#define YYUNDEFTOK  2
+#define YYMAXUTOK   298
+
+#define YYTRANSLATE(YYX)						\
+  ((unsigned int) (YYX) <= YYMAXUTOK ? yytranslate[YYX] : YYUNDEFTOK)
+
+/* YYTRANSLATE[YYLEX] -- Bison symbol number corresponding to YYLEX.  */
+static const yytype_uint8 yytranslate[] =
+{
+       0,     2,     2,     2,     2,     2,     2,     2,     2,     2,
+       2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
+       2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
+       2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
+      47,    49,    48,     2,    46,     2,     2,     2,     2,     2,
+       2,     2,     2,     2,     2,     2,     2,     2,    52,    44,
+       2,    50,     2,     2,     2,     2,     2,     2,     2,     2,
+       2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
+       2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
+       2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
+       2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
+       2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
+       2,     2,     2,    51,     2,    45,     2,     2,     2,     2,
+       2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
+       2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
+       2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
+       2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
+       2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
+       2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
+       2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
+       2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
+       2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
+       2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
+       2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
+       2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
+       2,     2,     2,     2,     2,     2,     1,     2,     3,     4,
+       5,     6,     7,     8,     9,    10,    11,    12,    13,    14,
+      15,    16,    17,    18,    19,    20,    21,    22,    23,    24,
+      25,    26,    27,    28,    29,    30,    31,    32,    33,    34,
+      35,    36,    37,    38,    39,    40,    41,    42,    43
+};
+
+#if YYDEBUG
+/* YYPRHS[YYN] -- Index of the first RHS symbol of rule number YYN in
+   YYRHS.  */
+static const yytype_uint16 yyprhs[] =
+{
+       0,     0,     3,     5,     8,     9,    12,    13,    18,    19,
+      23,    25,    27,    29,    31,    34,    37,    41,    42,    44,
+      46,    50,    55,    56,    58,    60,    63,    65,    67,    69,
+      71,    73,    75,    77,    79,    81,    87,    92,    95,    98,
+     101,   105,   109,   113,   116,   119,   122,   124,   126,   128,
+     130,   132,   134,   136,   138,   140,   142,   144,   147,   148,
+     150,   152,   155,   157,   159,   161,   163,   166,   168,   170,
+     175,   180,   183,   187,   191,   194,   196,   198,   200,   205,
+     210,   213,   217,   221,   224,   226,   230,   231,   233,   235,
+     239,   242,   245,   247,   248,   250,   252,   257,   262,   265,
+     269,   273,   277,   278,   280,   283,   287,   291,   292,   294,
+     296,   299,   303,   306,   307,   309,   311,   315,   318,   321,
+     323,   326,   327,   330,   334,   339,   341,   345,   347,   351,
+     354,   355,   357
+};
+
+/* YYRHS -- A `-1'-separated list of the rules' RHS.  */
+static const yytype_int8 yyrhs[] =
+{
+      54,     0,    -1,    55,    -1,    54,    55,    -1,    -1,    56,
+      57,    -1,    -1,    12,    23,    58,    60,    -1,    -1,    23,
+      59,    60,    -1,    60,    -1,    84,    -1,    99,    -1,   101,
+      -1,     1,    44,    -1,     1,    45,    -1,    64,    61,    44,
+      -1,    -1,    62,    -1,    63,    -1,    62,    46,    63,    -1,
+      74,   100,    95,    85,    -1,    -1,    65,    -1,    66,    -1,
+      65,    66,    -1,    67,    -1,    68,    -1,     5,    -1,    17,
+      -1,    21,    -1,    11,    -1,    14,    -1,    69,    -1,    73,
+      -1,    28,    47,    65,    48,    49,    -1,    28,    47,    65,
+      49,    -1,    22,    37,    -1,    24,    37,    -1,    10,    37,
+      -1,    22,    37,    87,    -1,    24,    37,    87,    -1,    10,
+      37,    96,    -1,    10,    96,    -1,    22,    87,    -1,    24,
+      87,    -1,     7,    -1,    19,    -1,    15,    -1,    16,    -1,
+      20,    -1,    25,    -1,    13,    -1,     9,    -1,    26,    -1,
+       6,    -1,    41,    -1,    48,    71,    -1,    -1,    72,    -1,
+      73,    -1,    72,    73,    -1,     8,    -1,    27,    -1,    31,
+      -1,    18,    -1,    70,    74,    -1,    75,    -1,    37,    -1,
+      75,    47,    78,    49,    -1,    75,    47,     1,    49,    -1,
+      75,    33,    -1,    47,    74,    49,    -1,    47,     1,    49,
+      -1,    70,    76,    -1,    77,    -1,    37,    -1,    41,    -1,
+      77,    47,    78,    49,    -1,    77,    47,     1,    49,    -1,
+      77,    33,    -1,    47,    76,    49,    -1,    47,     1,    49,
+      -1,    79,    36,    -1,    79,    -1,    80,    46,    36,    -1,
+      -1,    80,    -1,    81,    -1,    80,    46,    81,    -1,    65,
+      82,    -1,    70,    82,    -1,    83,    -1,    -1,    37,    -1,
+      41,    -1,    83,    47,    78,    49,    -1,    83,    47,     1,
+      49,    -1,    83,    33,    -1,    47,    82,    49,    -1,    47,
+       1,    49,    -1,    64,    74,    32,    -1,    -1,    86,    -1,
+      50,    34,    -1,    51,    88,    45,    -1,    51,     1,    45,
+      -1,    -1,    89,    -1,    90,    -1,    89,    90,    -1,    64,
+      91,    44,    -1,     1,    44,    -1,    -1,    92,    -1,    93,
+      -1,    92,    46,    93,    -1,    76,    95,    -1,    37,    94,
+      -1,    94,    -1,    52,    34,    -1,    -1,    95,    31,    -1,
+      51,    97,    45,    -1,    51,    97,    46,    45,    -1,    98,
+      -1,    97,    46,    98,    -1,    37,    -1,    37,    50,    34,
+      -1,    30,    44,    -1,    -1,    30,    -1,    29,    47,    37,
+      49,    44,    -1
+};
+
+/* YYRLINE[YYN] -- source line where rule number YYN was defined.  */
+static const yytype_uint16 yyrline[] =
+{
+       0,   123,   123,   124,   128,   128,   134,   134,   136,   136,
+     138,   139,   140,   141,   142,   143,   147,   161,   162,   166,
+     174,   187,   193,   194,   198,   199,   203,   209,   213,   214,
+     215,   216,   217,   221,   222,   223,   224,   228,   230,   232,
+     236,   238,   240,   245,   248,   249,   253,   254,   255,   256,
+     257,   258,   259,   260,   261,   262,   263,   267,   272,   273,
+     277,   278,   282,   282,   282,   283,   291,   292,   296,   305,
+     307,   309,   311,   313,   320,   321,   325,   326,   327,   329,
+     331,   333,   335,   340,   341,   342,   346,   347,   351,   352,
+     357,   362,   364,   368,   369,   377,   381,   383,   385,   387,
+     389,   394,   403,   404,   409,   414,   415,   419,   420,   424,
+     425,   429,   431,   436,   437,   441,   442,   446,   447,   448,
+     452,   456,   457,   461,   462,   466,   467,   470,   475,   483,
+     487,   488,   492
+};
+#endif
+
+#if YYDEBUG || YYERROR_VERBOSE || YYTOKEN_TABLE
+/* YYTNAME[SYMBOL-NUM] -- String name of the symbol SYMBOL-NUM.
+   First, the terminals, then, starting at YYNTOKENS, nonterminals.  */
+static const char *const yytname[] =
+{
+  "$end", "error", "$undefined", "ASM_KEYW", "ATTRIBUTE_KEYW",
+  "AUTO_KEYW", "BOOL_KEYW", "CHAR_KEYW", "CONST_KEYW", "DOUBLE_KEYW",
+  "ENUM_KEYW", "EXTERN_KEYW", "EXTENSION_KEYW", "FLOAT_KEYW",
+  "INLINE_KEYW", "INT_KEYW", "LONG_KEYW", "REGISTER_KEYW", "RESTRICT_KEYW",
+  "SHORT_KEYW", "SIGNED_KEYW", "STATIC_KEYW", "STRUCT_KEYW",
+  "TYPEDEF_KEYW", "UNION_KEYW", "UNSIGNED_KEYW", "VOID_KEYW",
+  "VOLATILE_KEYW", "TYPEOF_KEYW", "EXPORT_SYMBOL_KEYW", "ASM_PHRASE",
+  "ATTRIBUTE_PHRASE", "BRACE_PHRASE", "BRACKET_PHRASE",
+  "EXPRESSION_PHRASE", "CHAR", "DOTS", "IDENT", "INT", "REAL", "STRING",
+  "TYPE", "OTHER", "FILENAME", "';'", "'}'", "','", "'('", "'*'", "')'",
+  "'='", "'{'", "':'", "$accept", "declaration_seq", "declaration", "$@1",
+  "declaration1", "$@2", "$@3", "simple_declaration",
+  "init_declarator_list_opt", "init_declarator_list", "init_declarator",
+  "decl_specifier_seq_opt", "decl_specifier_seq", "decl_specifier",
+  "storage_class_specifier", "type_specifier", "simple_type_specifier",
+  "ptr_operator", "cvar_qualifier_seq_opt", "cvar_qualifier_seq",
+  "cvar_qualifier", "declarator", "direct_declarator", "nested_declarator",
+  "direct_nested_declarator", "parameter_declaration_clause",
+  "parameter_declaration_list_opt", "parameter_declaration_list",
+  "parameter_declaration", "m_abstract_declarator",
+  "direct_m_abstract_declarator", "function_definition", "initializer_opt",
+  "initializer", "class_body", "member_specification_opt",
+  "member_specification", "member_declaration",
+  "member_declarator_list_opt", "member_declarator_list",
+  "member_declarator", "member_bitfield_declarator", "attribute_opt",
+  "enum_body", "enumerator_list", "enumerator", "asm_definition",
+  "asm_phrase_opt", "export_definition", 0
+};
+#endif
+
+# ifdef YYPRINT
+/* YYTOKNUM[YYLEX-NUM] -- Internal token number corresponding to
+   token YYLEX-NUM.  */
+static const yytype_uint16 yytoknum[] =
+{
+       0,   256,   257,   258,   259,   260,   261,   262,   263,   264,
+     265,   266,   267,   268,   269,   270,   271,   272,   273,   274,
+     275,   276,   277,   278,   279,   280,   281,   282,   283,   284,
+     285,   286,   287,   288,   289,   290,   291,   292,   293,   294,
+     295,   296,   297,   298,    59,   125,    44,    40,    42,    41,
+      61,   123,    58
+};
+# endif
+
+/* YYR1[YYN] -- Symbol number of symbol that rule YYN derives.  */
+static const yytype_uint8 yyr1[] =
+{
+       0,    53,    54,    54,    56,    55,    58,    57,    59,    57,
+      57,    57,    57,    57,    57,    57,    60,    61,    61,    62,
+      62,    63,    64,    64,    65,    65,    66,    66,    67,    67,
+      67,    67,    67,    68,    68,    68,    68,    68,    68,    68,
+      68,    68,    68,    68,    68,    68,    69,    69,    69,    69,
+      69,    69,    69,    69,    69,    69,    69,    70,    71,    71,
+      72,    72,    73,    73,    73,    73,    74,    74,    75,    75,
+      75,    75,    75,    75,    76,    76,    77,    77,    77,    77,
+      77,    77,    77,    78,    78,    78,    79,    79,    80,    80,
+      81,    82,    82,    83,    83,    83,    83,    83,    83,    83,
+      83,    84,    85,    85,    86,    87,    87,    88,    88,    89,
+      89,    90,    90,    91,    91,    92,    92,    93,    93,    93,
+      94,    95,    95,    96,    96,    97,    97,    98,    98,    99,
+     100,   100,   101
+};
+
+/* YYR2[YYN] -- Number of symbols composing right hand side of rule YYN.  */
+static const yytype_uint8 yyr2[] =
+{
+       0,     2,     1,     2,     0,     2,     0,     4,     0,     3,
+       1,     1,     1,     1,     2,     2,     3,     0,     1,     1,
+       3,     4,     0,     1,     1,     2,     1,     1,     1,     1,
+       1,     1,     1,     1,     1,     5,     4,     2,     2,     2,
+       3,     3,     3,     2,     2,     2,     1,     1,     1,     1,
+       1,     1,     1,     1,     1,     1,     1,     2,     0,     1,
+       1,     2,     1,     1,     1,     1,     2,     1,     1,     4,
+       4,     2,     3,     3,     2,     1,     1,     1,     4,     4,
+       2,     3,     3,     2,     1,     3,     0,     1,     1,     3,
+       2,     2,     1,     0,     1,     1,     4,     4,     2,     3,
+       3,     3,     0,     1,     2,     3,     3,     0,     1,     1,
+       2,     3,     2,     0,     1,     1,     3,     2,     2,     1,
+       2,     0,     2,     3,     4,     1,     3,     1,     3,     2,
+       0,     1,     5
+};
+
+/* YYDEFACT[STATE-NAME] -- Default reduction number in state STATE-NUM.
+   Performed when YYTABLE doesn't specify something else to do.  Zero
+   means the default is an error.  */
+static const yytype_uint8 yydefact[] =
+{
+       4,     4,     2,     0,     1,     3,     0,    28,    55,    46,
+      62,    53,     0,    31,     0,    52,    32,    48,    49,    29,
+      65,    47,    50,    30,     0,     8,     0,    51,    54,    63,
+       0,     0,     0,    64,    56,     5,    10,    17,    23,    24,
+      26,    27,    33,    34,    11,    12,    13,    14,    15,    39,
+       0,    43,     6,    37,     0,    44,    22,    38,    45,     0,
+       0,   129,    68,     0,    58,     0,    18,    19,     0,   130,
+      67,    25,    42,   127,     0,   125,    22,    40,     0,   113,
+       0,     0,   109,     9,    17,    41,     0,     0,     0,     0,
+      57,    59,    60,    16,     0,    66,   131,   101,   121,    71,
+       0,     0,   123,     0,     7,   112,   106,    76,    77,     0,
+       0,     0,   121,    75,     0,   114,   115,   119,   105,     0,
+     110,   130,     0,    36,     0,    73,    72,    61,    20,   102,
+       0,    93,     0,    84,    87,    88,   128,   124,   126,   118,
+       0,    76,     0,   120,    74,   117,    80,     0,   111,     0,
+      35,   132,   122,     0,    21,   103,    70,    94,    56,     0,
+      93,    90,    92,    69,    83,     0,    82,    81,     0,     0,
+     116,   104,     0,    95,     0,    91,    98,     0,    85,    89,
+      79,    78,   100,    99,     0,     0,    97,    96
+};
+
+/* YYDEFGOTO[NTERM-NUM].  */
+static const yytype_int16 yydefgoto[] =
+{
+      -1,     1,     2,     3,    35,    76,    56,    36,    65,    66,
+      67,    79,    38,    39,    40,    41,    42,    68,    90,    91,
+      43,   121,    70,   112,   113,   132,   133,   134,   135,   161,
+     162,    44,   154,   155,    55,    80,    81,    82,   114,   115,
+     116,   117,   129,    51,    74,    75,    45,    98,    46
+};
+
+/* YYPACT[STATE-NUM] -- Index in YYTABLE of the portion describing
+   STATE-NUM.  */
+#define YYPACT_NINF -135
+static const yytype_int16 yypact[] =
+{
+    -135,    20,  -135,   321,  -135,  -135,    30,  -135,  -135,  -135,
+    -135,  -135,   -28,  -135,     2,  -135,  -135,  -135,  -135,  -135,
+    -135,  -135,  -135,  -135,    -6,  -135,     9,  -135,  -135,  -135,
+      -5,    15,   -17,  -135,  -135,  -135,  -135,    18,   491,  -135,
+    -135,  -135,  -135,  -135,  -135,  -135,  -135,  -135,  -135,   -22,
+      31,  -135,  -135,    19,   106,  -135,   491,    19,  -135,   491,
+      50,  -135,  -135,    11,    -3,    51,    57,  -135,    18,   -14,
+      14,  -135,  -135,    48,    46,  -135,   491,  -135,    33,    32,
+      59,   154,  -135,  -135,    18,  -135,   365,    56,    60,    61,
+    -135,    -3,  -135,  -135,    18,  -135,  -135,  -135,  -135,  -135,
+     202,    74,  -135,   -23,  -135,  -135,  -135,    77,  -135,    16,
+     101,    49,  -135,    34,    92,    93,  -135,  -135,  -135,    94,
+    -135,   110,    95,  -135,    97,  -135,  -135,  -135,  -135,   -20,
+      96,   410,    99,   113,   100,  -135,  -135,  -135,  -135,  -135,
+     103,  -135,   107,  -135,  -135,   111,  -135,   239,  -135,    32,
+    -135,  -135,  -135,   123,  -135,  -135,  -135,  -135,  -135,     3,
+      52,  -135,    38,  -135,  -135,   454,  -135,  -135,   117,   128,
+    -135,  -135,   134,  -135,   135,  -135,  -135,   276,  -135,  -135,
+    -135,  -135,  -135,  -135,   137,   138,  -135,  -135
+};
+
+/* YYPGOTO[NTERM-NUM].  */
+static const yytype_int16 yypgoto[] =
+{
+    -135,  -135,   187,  -135,  -135,  -135,  -135,   -50,  -135,  -135,
+      98,     0,   -59,   -37,  -135,  -135,  -135,   -77,  -135,  -135,
+     -54,   -30,  -135,   -90,  -135,  -134,  -135,  -135,    24,   -58,
+    -135,  -135,  -135,  -135,   -18,  -135,  -135,   109,  -135,  -135,
+      44,    87,    84,   148,  -135,   102,  -135,  -135,  -135
+};
+
+/* YYTABLE[YYPACT[STATE-NUM]].  What to do in state STATE-NUM.  If
+   positive, shift that token.  If negative, reduce the rule which
+   number is the opposite.  If YYTABLE_NINF, syntax error.  */
+#define YYTABLE_NINF -109
+static const yytype_int16 yytable[] =
+{
+      86,    71,   111,    37,   172,    10,    83,    69,    58,    49,
+      92,   152,    88,   169,    73,    20,    96,   140,    97,   142,
+       4,   144,   137,    50,    29,    52,   104,    61,    33,    50,
+     153,    53,   111,    89,   111,    77,   -93,   127,    95,    85,
+     157,   131,    59,   185,   173,    54,    57,    99,    62,    71,
+     159,    64,   -93,   141,   160,    62,    84,   108,    63,    64,
+      54,   100,    60,   109,    64,    63,    64,   146,    73,   107,
+      54,   176,   111,   108,    47,    48,    84,   105,   106,   109,
+      64,   147,   160,   160,   110,   177,   141,    87,   131,   157,
+     108,   102,   103,   173,    71,    93,   109,    64,   101,   159,
+      64,   174,   175,    94,   118,   124,   131,    78,   136,   125,
+     126,     7,     8,     9,    10,    11,    12,    13,   131,    15,
+      16,    17,    18,    19,    20,    21,    22,    23,    24,   110,
+      26,    27,    28,    29,    30,   143,   148,    33,   105,   149,
+      96,   151,   152,   -22,   150,   156,   165,    34,   163,   164,
+     -22,  -107,   166,   -22,   -22,   119,   167,   171,   -22,     7,
+       8,     9,    10,    11,    12,    13,   180,    15,    16,    17,
+      18,    19,    20,    21,    22,    23,    24,   181,    26,    27,
+      28,    29,    30,   182,   183,    33,   186,   187,     5,   179,
+     120,   -22,   128,   170,   139,    34,   145,    72,   -22,  -108,
+       0,   -22,   -22,   130,     0,   138,   -22,     7,     8,     9,
+      10,    11,    12,    13,     0,    15,    16,    17,    18,    19,
+      20,    21,    22,    23,    24,     0,    26,    27,    28,    29,
+      30,     0,     0,    33,     0,     0,     0,     0,   -86,     0,
+     168,     0,     0,    34,     7,     8,     9,    10,    11,    12,
+      13,   -86,    15,    16,    17,    18,    19,    20,    21,    22,
+      23,    24,     0,    26,    27,    28,    29,    30,     0,     0,
+      33,     0,     0,     0,     0,   -86,     0,   184,     0,     0,
+      34,     7,     8,     9,    10,    11,    12,    13,   -86,    15,
+      16,    17,    18,    19,    20,    21,    22,    23,    24,     0,
+      26,    27,    28,    29,    30,     0,     0,    33,     0,     0,
+       0,     0,   -86,     0,     0,     0,     0,    34,     0,     0,
+       0,     0,     6,     0,     0,   -86,     7,     8,     9,    10,
+      11,    12,    13,    14,    15,    16,    17,    18,    19,    20,
+      21,    22,    23,    24,    25,    26,    27,    28,    29,    30,
+      31,    32,    33,     0,     0,     0,     0,     0,   -22,     0,
+       0,     0,    34,     0,     0,   -22,     0,     0,   -22,   -22,
+       7,     8,     9,    10,    11,    12,    13,     0,    15,    16,
+      17,    18,    19,    20,    21,    22,    23,    24,     0,    26,
+      27,    28,    29,    30,     0,     0,    33,     0,     0,     0,
+       0,     0,     0,     0,     0,     0,    34,     0,     0,     0,
+       0,     0,     0,   122,   123,     7,     8,     9,    10,    11,
+      12,    13,     0,    15,    16,    17,    18,    19,    20,    21,
+      22,    23,    24,     0,    26,    27,    28,    29,    30,     0,
+       0,    33,     0,     0,     0,     0,     0,   157,     0,     0,
+       0,   158,     0,     0,     0,     0,     0,   159,    64,     7,
+       8,     9,    10,    11,    12,    13,     0,    15,    16,    17,
+      18,    19,    20,    21,    22,    23,    24,     0,    26,    27,
+      28,    29,    30,     0,     0,    33,     0,     0,     0,     0,
+     178,     0,     0,     0,     0,    34,     7,     8,     9,    10,
+      11,    12,    13,     0,    15,    16,    17,    18,    19,    20,
+      21,    22,    23,    24,     0,    26,    27,    28,    29,    30,
+       0,     0,    33,     0,     0,     0,     0,     0,     0,     0,
+       0,     0,    34
+};
+
+#define yypact_value_is_default(yystate) \
+  ((yystate) == (-135))
+
+#define yytable_value_is_error(yytable_value) \
+  YYID (0)
+
+static const yytype_int16 yycheck[] =
+{
+      59,    38,    79,     3,     1,     8,    56,    37,    26,    37,
+      64,    31,     1,   147,    37,    18,    30,     1,    32,   109,
+       0,   111,    45,    51,    27,    23,    76,    44,    31,    51,
+      50,    37,   109,    63,   111,    53,    33,    91,    68,    57,
+      37,   100,    47,   177,    41,    51,    37,    33,    37,    86,
+      47,    48,    49,    37,   131,    37,    56,    41,    47,    48,
+      51,    47,    47,    47,    48,    47,    48,    33,    37,    37,
+      51,    33,   149,    41,    44,    45,    76,    44,    45,    47,
+      48,    47,   159,   160,    52,    47,    37,    37,   147,    37,
+      41,    45,    46,    41,   131,    44,    47,    48,    50,    47,
+      48,   159,   160,    46,    45,    49,   165,     1,    34,    49,
+      49,     5,     6,     7,     8,     9,    10,    11,   177,    13,
+      14,    15,    16,    17,    18,    19,    20,    21,    22,    52,
+      24,    25,    26,    27,    28,    34,    44,    31,    44,    46,
+      30,    44,    31,    37,    49,    49,    46,    41,    49,    36,
+      44,    45,    49,    47,    48,     1,    49,    34,    52,     5,
+       6,     7,     8,     9,    10,    11,    49,    13,    14,    15,
+      16,    17,    18,    19,    20,    21,    22,    49,    24,    25,
+      26,    27,    28,    49,    49,    31,    49,    49,     1,   165,
+      81,    37,    94,   149,   107,    41,   112,    49,    44,    45,
+      -1,    47,    48,     1,    -1,   103,    52,     5,     6,     7,
+       8,     9,    10,    11,    -1,    13,    14,    15,    16,    17,
+      18,    19,    20,    21,    22,    -1,    24,    25,    26,    27,
+      28,    -1,    -1,    31,    -1,    -1,    -1,    -1,    36,    -1,
+       1,    -1,    -1,    41,     5,     6,     7,     8,     9,    10,
+      11,    49,    13,    14,    15,    16,    17,    18,    19,    20,
+      21,    22,    -1,    24,    25,    26,    27,    28,    -1,    -1,
+      31,    -1,    -1,    -1,    -1,    36,    -1,     1,    -1,    -1,
+      41,     5,     6,     7,     8,     9,    10,    11,    49,    13,
+      14,    15,    16,    17,    18,    19,    20,    21,    22,    -1,
+      24,    25,    26,    27,    28,    -1,    -1,    31,    -1,    -1,
+      -1,    -1,    36,    -1,    -1,    -1,    -1,    41,    -1,    -1,
+      -1,    -1,     1,    -1,    -1,    49,     5,     6,     7,     8,
+       9,    10,    11,    12,    13,    14,    15,    16,    17,    18,
+      19,    20,    21,    22,    23,    24,    25,    26,    27,    28,
+      29,    30,    31,    -1,    -1,    -1,    -1,    -1,    37,    -1,
+      -1,    -1,    41,    -1,    -1,    44,    -1,    -1,    47,    48,
+       5,     6,     7,     8,     9,    10,    11,    -1,    13,    14,
+      15,    16,    17,    18,    19,    20,    21,    22,    -1,    24,
+      25,    26,    27,    28,    -1,    -1,    31,    -1,    -1,    -1,
+      -1,    -1,    -1,    -1,    -1,    -1,    41,    -1,    -1,    -1,
+      -1,    -1,    -1,    48,    49,     5,     6,     7,     8,     9,
+      10,    11,    -1,    13,    14,    15,    16,    17,    18,    19,
+      20,    21,    22,    -1,    24,    25,    26,    27,    28,    -1,
+      -1,    31,    -1,    -1,    -1,    -1,    -1,    37,    -1,    -1,
+      -1,    41,    -1,    -1,    -1,    -1,    -1,    47,    48,     5,
+       6,     7,     8,     9,    10,    11,    -1,    13,    14,    15,
+      16,    17,    18,    19,    20,    21,    22,    -1,    24,    25,
+      26,    27,    28,    -1,    -1,    31,    -1,    -1,    -1,    -1,
+      36,    -1,    -1,    -1,    -1,    41,     5,     6,     7,     8,
+       9,    10,    11,    -1,    13,    14,    15,    16,    17,    18,
+      19,    20,    21,    22,    -1,    24,    25,    26,    27,    28,
+      -1,    -1,    31,    -1,    -1,    -1,    -1,    -1,    -1,    -1,
+      -1,    -1,    41
+};
+
+/* YYSTOS[STATE-NUM] -- The (internal number of the) accessing
+   symbol of state STATE-NUM.  */
+static const yytype_uint8 yystos[] =
+{
+       0,    54,    55,    56,     0,    55,     1,     5,     6,     7,
+       8,     9,    10,    11,    12,    13,    14,    15,    16,    17,
+      18,    19,    20,    21,    22,    23,    24,    25,    26,    27,
+      28,    29,    30,    31,    41,    57,    60,    64,    65,    66,
+      67,    68,    69,    73,    84,    99,   101,    44,    45,    37,
+      51,    96,    23,    37,    51,    87,    59,    37,    87,    47,
+      47,    44,    37,    47,    48,    61,    62,    63,    70,    74,
+      75,    66,    96,    37,    97,    98,    58,    87,     1,    64,
+      88,    89,    90,    60,    64,    87,    65,    37,     1,    74,
+      71,    72,    73,    44,    46,    74,    30,    32,   100,    33,
+      47,    50,    45,    46,    60,    44,    45,    37,    41,    47,
+      52,    70,    76,    77,    91,    92,    93,    94,    45,     1,
+      90,    74,    48,    49,    49,    49,    49,    73,    63,    95,
+       1,    65,    78,    79,    80,    81,    34,    45,    98,    94,
+       1,    37,    76,    34,    76,    95,    33,    47,    44,    46,
+      49,    44,    31,    50,    85,    86,    49,    37,    41,    47,
+      70,    82,    83,    49,    36,    46,    49,    49,     1,    78,
+      93,    34,     1,    41,    82,    82,    33,    47,    36,    81,
+      49,    49,    49,    49,     1,    78,    49,    49
+};
+
+#define yyerrok		(yyerrstatus = 0)
+#define yyclearin	(yychar = YYEMPTY)
+#define YYEMPTY		(-2)
+#define YYEOF		0
+
+#define YYACCEPT	goto yyacceptlab
+#define YYABORT		goto yyabortlab
+#define YYERROR		goto yyerrorlab
+
+
+/* Like YYERROR except do call yyerror.  This remains here temporarily
+   to ease the transition to the new meaning of YYERROR, for GCC.
+   Once GCC version 2 has supplanted version 1, this can go.  However,
+   YYFAIL appears to be in use.  Nevertheless, it is formally deprecated
+   in Bison 2.4.2's NEWS entry, where a plan to phase it out is
+   discussed.  */
+
+#define YYFAIL		goto yyerrlab
+#if defined YYFAIL
+  /* This is here to suppress warnings from the GCC cpp's
+     -Wunused-macros.  Normally we don't worry about that warning, but
+     some users do, and we want to make it easy for users to remove
+     YYFAIL uses, which will produce warnings from Bison 2.5.  */
+#endif
+
+#define YYRECOVERING()  (!!yyerrstatus)
+
+#define YYBACKUP(Token, Value)					\
+do								\
+  if (yychar == YYEMPTY && yylen == 1)				\
+    {								\
+      yychar = (Token);						\
+      yylval = (Value);						\
+      YYPOPSTACK (1);						\
+      goto yybackup;						\
+    }								\
+  else								\
+    {								\
+      yyerror (YY_("syntax error: cannot back up")); \
+      YYERROR;							\
+    }								\
+while (YYID (0))
+
+
+#define YYTERROR	1
+#define YYERRCODE	256
+
+
+/* YYLLOC_DEFAULT -- Set CURRENT to span from RHS[1] to RHS[N].
+   If N is 0, then set CURRENT to the empty location which ends
+   the previous symbol: RHS[0] (always defined).  */
+
+#define YYRHSLOC(Rhs, K) ((Rhs)[K])
+#ifndef YYLLOC_DEFAULT
+# define YYLLOC_DEFAULT(Current, Rhs, N)				\
+    do									\
+      if (YYID (N))                                                    \
+	{								\
+	  (Current).first_line   = YYRHSLOC (Rhs, 1).first_line;	\
+	  (Current).first_column = YYRHSLOC (Rhs, 1).first_column;	\
+	  (Current).last_line    = YYRHSLOC (Rhs, N).last_line;		\
+	  (Current).last_column  = YYRHSLOC (Rhs, N).last_column;	\
+	}								\
+      else								\
+	{								\
+	  (Current).first_line   = (Current).last_line   =		\
+	    YYRHSLOC (Rhs, 0).last_line;				\
+	  (Current).first_column = (Current).last_column =		\
+	    YYRHSLOC (Rhs, 0).last_column;				\
+	}								\
+    while (YYID (0))
+#endif
+
+
+/* This macro is provided for backward compatibility. */
+
+#ifndef YY_LOCATION_PRINT
+# define YY_LOCATION_PRINT(File, Loc) ((void) 0)
+#endif
+
+
+/* YYLEX -- calling `yylex' with the right arguments.  */
+
+#ifdef YYLEX_PARAM
+# define YYLEX yylex (YYLEX_PARAM)
+#else
+# define YYLEX yylex ()
+#endif
+
+/* Enable debugging if requested.  */
+#if YYDEBUG
+
+# ifndef YYFPRINTF
+#  include <stdio.h> /* INFRINGES ON USER NAME SPACE */
+#  define YYFPRINTF fprintf
+# endif
+
+# define YYDPRINTF(Args)			\
+do {						\
+  if (yydebug)					\
+    YYFPRINTF Args;				\
+} while (YYID (0))
+
+# define YY_SYMBOL_PRINT(Title, Type, Value, Location)			  \
+do {									  \
+  if (yydebug)								  \
+    {									  \
+      YYFPRINTF (stderr, "%s ", Title);					  \
+      yy_symbol_print (stderr,						  \
+		  Type, Value); \
+      YYFPRINTF (stderr, "\n");						  \
+    }									  \
+} while (YYID (0))
+
+
+/*--------------------------------.
+| Print this symbol on YYOUTPUT.  |
+`--------------------------------*/
+
+/*ARGSUSED*/
+#if (defined __STDC__ || defined __C99__FUNC__ \
+     || defined __cplusplus || defined _MSC_VER)
+static void
+yy_symbol_value_print (FILE *yyoutput, int yytype, YYSTYPE const * const yyvaluep)
+#else
+static void
+yy_symbol_value_print (yyoutput, yytype, yyvaluep)
+    FILE *yyoutput;
+    int yytype;
+    YYSTYPE const * const yyvaluep;
+#endif
+{
+  if (!yyvaluep)
+    return;
+# ifdef YYPRINT
+  if (yytype < YYNTOKENS)
+    YYPRINT (yyoutput, yytoknum[yytype], *yyvaluep);
+# else
+  YYUSE (yyoutput);
+# endif
+  switch (yytype)
+    {
+      default:
+	break;
+    }
+}
+
+
+/*--------------------------------.
+| Print this symbol on YYOUTPUT.  |
+`--------------------------------*/
+
+#if (defined __STDC__ || defined __C99__FUNC__ \
+     || defined __cplusplus || defined _MSC_VER)
+static void
+yy_symbol_print (FILE *yyoutput, int yytype, YYSTYPE const * const yyvaluep)
+#else
+static void
+yy_symbol_print (yyoutput, yytype, yyvaluep)
+    FILE *yyoutput;
+    int yytype;
+    YYSTYPE const * const yyvaluep;
+#endif
+{
+  if (yytype < YYNTOKENS)
+    YYFPRINTF (yyoutput, "token %s (", yytname[yytype]);
+  else
+    YYFPRINTF (yyoutput, "nterm %s (", yytname[yytype]);
+
+  yy_symbol_value_print (yyoutput, yytype, yyvaluep);
+  YYFPRINTF (yyoutput, ")");
+}
+
+/*------------------------------------------------------------------.
+| yy_stack_print -- Print the state stack from its BOTTOM up to its |
+| TOP (included).                                                   |
+`------------------------------------------------------------------*/
+
+#if (defined __STDC__ || defined __C99__FUNC__ \
+     || defined __cplusplus || defined _MSC_VER)
+static void
+yy_stack_print (yytype_int16 *yybottom, yytype_int16 *yytop)
+#else
+static void
+yy_stack_print (yybottom, yytop)
+    yytype_int16 *yybottom;
+    yytype_int16 *yytop;
+#endif
+{
+  YYFPRINTF (stderr, "Stack now");
+  for (; yybottom <= yytop; yybottom++)
+    {
+      int yybot = *yybottom;
+      YYFPRINTF (stderr, " %d", yybot);
+    }
+  YYFPRINTF (stderr, "\n");
+}
+
+# define YY_STACK_PRINT(Bottom, Top)				\
+do {								\
+  if (yydebug)							\
+    yy_stack_print ((Bottom), (Top));				\
+} while (YYID (0))
+
+
+/*------------------------------------------------.
+| Report that the YYRULE is going to be reduced.  |
+`------------------------------------------------*/
+
+#if (defined __STDC__ || defined __C99__FUNC__ \
+     || defined __cplusplus || defined _MSC_VER)
+static void
+yy_reduce_print (YYSTYPE *yyvsp, int yyrule)
+#else
+static void
+yy_reduce_print (yyvsp, yyrule)
+    YYSTYPE *yyvsp;
+    int yyrule;
+#endif
+{
+  int yynrhs = yyr2[yyrule];
+  int yyi;
+  unsigned long int yylno = yyrline[yyrule];
+  YYFPRINTF (stderr, "Reducing stack by rule %d (line %lu):\n",
+	     yyrule - 1, yylno);
+  /* The symbols being reduced.  */
+  for (yyi = 0; yyi < yynrhs; yyi++)
+    {
+      YYFPRINTF (stderr, "   $%d = ", yyi + 1);
+      yy_symbol_print (stderr, yyrhs[yyprhs[yyrule] + yyi],
+		       &(yyvsp[(yyi + 1) - (yynrhs)])
+		       		       );
+      YYFPRINTF (stderr, "\n");
+    }
+}
+
+# define YY_REDUCE_PRINT(Rule)		\
+do {					\
+  if (yydebug)				\
+    yy_reduce_print (yyvsp, Rule); \
+} while (YYID (0))
+
+/* Nonzero means print parse trace.  It is left uninitialized so that
+   multiple parsers can coexist.  */
+int yydebug;
+#else /* !YYDEBUG */
+# define YYDPRINTF(Args)
+# define YY_SYMBOL_PRINT(Title, Type, Value, Location)
+# define YY_STACK_PRINT(Bottom, Top)
+# define YY_REDUCE_PRINT(Rule)
+#endif /* !YYDEBUG */
+
+
+/* YYINITDEPTH -- initial size of the parser's stacks.  */
+#ifndef	YYINITDEPTH
+# define YYINITDEPTH 200
+#endif
+
+/* YYMAXDEPTH -- maximum size the stacks can grow to (effective only
+   if the built-in stack extension method is used).
+
+   Do not make this value too large; the results are undefined if
+   YYSTACK_ALLOC_MAXIMUM < YYSTACK_BYTES (YYMAXDEPTH)
+   evaluated with infinite-precision integer arithmetic.  */
+
+#ifndef YYMAXDEPTH
+# define YYMAXDEPTH 10000
+#endif
+
+
+#if YYERROR_VERBOSE
+
+# ifndef yystrlen
+#  if defined __GLIBC__ && defined _STRING_H
+#   define yystrlen strlen
+#  else
+/* Return the length of YYSTR.  */
+#if (defined __STDC__ || defined __C99__FUNC__ \
+     || defined __cplusplus || defined _MSC_VER)
+static YYSIZE_T
+yystrlen (const char *yystr)
+#else
+static YYSIZE_T
+yystrlen (yystr)
+    const char *yystr;
+#endif
+{
+  YYSIZE_T yylen;
+  for (yylen = 0; yystr[yylen]; yylen++)
+    continue;
+  return yylen;
+}
+#  endif
+# endif
+
+# ifndef yystpcpy
+#  if defined __GLIBC__ && defined _STRING_H && defined _GNU_SOURCE
+#   define yystpcpy stpcpy
+#  else
+/* Copy YYSRC to YYDEST, returning the address of the terminating '\0' in
+   YYDEST.  */
+#if (defined __STDC__ || defined __C99__FUNC__ \
+     || defined __cplusplus || defined _MSC_VER)
+static char *
+yystpcpy (char *yydest, const char *yysrc)
+#else
+static char *
+yystpcpy (yydest, yysrc)
+    char *yydest;
+    const char *yysrc;
+#endif
+{
+  char *yyd = yydest;
+  const char *yys = yysrc;
+
+  while ((*yyd++ = *yys++) != '\0')
+    continue;
+
+  return yyd - 1;
+}
+#  endif
+# endif
+
+# ifndef yytnamerr
+/* Copy to YYRES the contents of YYSTR after stripping away unnecessary
+   quotes and backslashes, so that it's suitable for yyerror.  The
+   heuristic is that double-quoting is unnecessary unless the string
+   contains an apostrophe, a comma, or backslash (other than
+   backslash-backslash).  YYSTR is taken from yytname.  If YYRES is
+   null, do not copy; instead, return the length of what the result
+   would have been.  */
+static YYSIZE_T
+yytnamerr (char *yyres, const char *yystr)
+{
+  if (*yystr == '"')
+    {
+      YYSIZE_T yyn = 0;
+      char const *yyp = yystr;
+
+      for (;;)
+	switch (*++yyp)
+	  {
+	  case '\'':
+	  case ',':
+	    goto do_not_strip_quotes;
+
+	  case '\\':
+	    if (*++yyp != '\\')
+	      goto do_not_strip_quotes;
+	    /* Fall through.  */
+	  default:
+	    if (yyres)
+	      yyres[yyn] = *yyp;
+	    yyn++;
+	    break;
+
+	  case '"':
+	    if (yyres)
+	      yyres[yyn] = '\0';
+	    return yyn;
+	  }
+    do_not_strip_quotes: ;
+    }
+
+  if (! yyres)
+    return yystrlen (yystr);
+
+  return yystpcpy (yyres, yystr) - yyres;
+}
+# endif
+
+/* Copy into *YYMSG, which is of size *YYMSG_ALLOC, an error message
+   about the unexpected token YYTOKEN for the state stack whose top is
+   YYSSP.
+
+   Return 0 if *YYMSG was successfully written.  Return 1 if *YYMSG is
+   not large enough to hold the message.  In that case, also set
+   *YYMSG_ALLOC to the required number of bytes.  Return 2 if the
+   required number of bytes is too large to store.  */
+static int
+yysyntax_error (YYSIZE_T *yymsg_alloc, char **yymsg,
+                yytype_int16 *yyssp, int yytoken)
+{
+  YYSIZE_T yysize0 = yytnamerr (0, yytname[yytoken]);
+  YYSIZE_T yysize = yysize0;
+  YYSIZE_T yysize1;
+  enum { YYERROR_VERBOSE_ARGS_MAXIMUM = 5 };
+  /* Internationalized format string. */
+  const char *yyformat = 0;
+  /* Arguments of yyformat. */
+  char const *yyarg[YYERROR_VERBOSE_ARGS_MAXIMUM];
+  /* Number of reported tokens (one for the "unexpected", one per
+     "expected"). */
+  int yycount = 0;
+
+  /* There are many possibilities here to consider:
+     - Assume YYFAIL is not used.  It's too flawed to consider.  See
+       <http://lists.gnu.org/archive/html/bison-patches/2009-12/msg00024.html>
+       for details.  YYERROR is fine as it does not invoke this
+       function.
+     - If this state is a consistent state with a default action, then
+       the only way this function was invoked is if the default action
+       is an error action.  In that case, don't check for expected
+       tokens because there are none.
+     - The only way there can be no lookahead present (in yychar) is if
+       this state is a consistent state with a default action.  Thus,
+       detecting the absence of a lookahead is sufficient to determine
+       that there is no unexpected or expected token to report.  In that
+       case, just report a simple "syntax error".
+     - Don't assume there isn't a lookahead just because this state is a
+       consistent state with a default action.  There might have been a
+       previous inconsistent state, consistent state with a non-default
+       action, or user semantic action that manipulated yychar.
+     - Of course, the expected token list depends on states to have
+       correct lookahead information, and it depends on the parser not
+       to perform extra reductions after fetching a lookahead from the
+       scanner and before detecting a syntax error.  Thus, state merging
+       (from LALR or IELR) and default reductions corrupt the expected
+       token list.  However, the list is correct for canonical LR with
+       one exception: it will still contain any token that will not be
+       accepted due to an error action in a later state.
+  */
+  if (yytoken != YYEMPTY)
+    {
+      int yyn = yypact[*yyssp];
+      yyarg[yycount++] = yytname[yytoken];
+      if (!yypact_value_is_default (yyn))
+        {
+          /* Start YYX at -YYN if negative to avoid negative indexes in
+             YYCHECK.  In other words, skip the first -YYN actions for
+             this state because they are default actions.  */
+          int yyxbegin = yyn < 0 ? -yyn : 0;
+          /* Stay within bounds of both yycheck and yytname.  */
+          int yychecklim = YYLAST - yyn + 1;
+          int yyxend = yychecklim < YYNTOKENS ? yychecklim : YYNTOKENS;
+          int yyx;
+
+          for (yyx = yyxbegin; yyx < yyxend; ++yyx)
+            if (yycheck[yyx + yyn] == yyx && yyx != YYTERROR
+                && !yytable_value_is_error (yytable[yyx + yyn]))
+              {
+                if (yycount == YYERROR_VERBOSE_ARGS_MAXIMUM)
+                  {
+                    yycount = 1;
+                    yysize = yysize0;
+                    break;
+                  }
+                yyarg[yycount++] = yytname[yyx];
+                yysize1 = yysize + yytnamerr (0, yytname[yyx]);
+                if (! (yysize <= yysize1
+                       && yysize1 <= YYSTACK_ALLOC_MAXIMUM))
+                  return 2;
+                yysize = yysize1;
+              }
+        }
+    }
+
+  switch (yycount)
+    {
+# define YYCASE_(N, S)                      \
+      case N:                               \
+        yyformat = S;                       \
+      break
+      YYCASE_(0, YY_("syntax error"));
+      YYCASE_(1, YY_("syntax error, unexpected %s"));
+      YYCASE_(2, YY_("syntax error, unexpected %s, expecting %s"));
+      YYCASE_(3, YY_("syntax error, unexpected %s, expecting %s or %s"));
+      YYCASE_(4, YY_("syntax error, unexpected %s, expecting %s or %s or %s"));
+      YYCASE_(5, YY_("syntax error, unexpected %s, expecting %s or %s or %s or %s"));
+# undef YYCASE_
+    }
+
+  yysize1 = yysize + yystrlen (yyformat);
+  if (! (yysize <= yysize1 && yysize1 <= YYSTACK_ALLOC_MAXIMUM))
+    return 2;
+  yysize = yysize1;
+
+  if (*yymsg_alloc < yysize)
+    {
+      *yymsg_alloc = 2 * yysize;
+      if (! (yysize <= *yymsg_alloc
+             && *yymsg_alloc <= YYSTACK_ALLOC_MAXIMUM))
+        *yymsg_alloc = YYSTACK_ALLOC_MAXIMUM;
+      return 1;
+    }
+
+  /* Avoid sprintf, as that infringes on the user's name space.
+     Don't have undefined behavior even if the translation
+     produced a string with the wrong number of "%s"s.  */
+  {
+    char *yyp = *yymsg;
+    int yyi = 0;
+    while ((*yyp = *yyformat) != '\0')
+      if (*yyp == '%' && yyformat[1] == 's' && yyi < yycount)
+        {
+          yyp += yytnamerr (yyp, yyarg[yyi++]);
+          yyformat += 2;
+        }
+      else
+        {
+          yyp++;
+          yyformat++;
+        }
+  }
+  return 0;
+}
+#endif /* YYERROR_VERBOSE */
+
+/*-----------------------------------------------.
+| Release the memory associated to this symbol.  |
+`-----------------------------------------------*/
+
+/*ARGSUSED*/
+#if (defined __STDC__ || defined __C99__FUNC__ \
+     || defined __cplusplus || defined _MSC_VER)
+static void
+yydestruct (const char *yymsg, int yytype, YYSTYPE *yyvaluep)
+#else
+static void
+yydestruct (yymsg, yytype, yyvaluep)
+    const char *yymsg;
+    int yytype;
+    YYSTYPE *yyvaluep;
+#endif
+{
+  YYUSE (yyvaluep);
+
+  if (!yymsg)
+    yymsg = "Deleting";
+  YY_SYMBOL_PRINT (yymsg, yytype, yyvaluep, yylocationp);
+
+  switch (yytype)
+    {
+
+      default:
+	break;
+    }
+}
+
+
+/* Prevent warnings from -Wmissing-prototypes.  */
+#ifdef YYPARSE_PARAM
+#if defined __STDC__ || defined __cplusplus
+int yyparse (void *YYPARSE_PARAM);
+#else
+int yyparse ();
+#endif
+#else /* ! YYPARSE_PARAM */
+#if defined __STDC__ || defined __cplusplus
+int yyparse (void);
+#else
+int yyparse ();
+#endif
+#endif /* ! YYPARSE_PARAM */
+
+
+/* The lookahead symbol.  */
+int yychar;
+
+/* The semantic value of the lookahead symbol.  */
+YYSTYPE yylval;
+
+/* Number of syntax errors so far.  */
+int yynerrs;
+
+
+/*----------.
+| yyparse.  |
+`----------*/
+
+#ifdef YYPARSE_PARAM
+#if (defined __STDC__ || defined __C99__FUNC__ \
+     || defined __cplusplus || defined _MSC_VER)
+int
+yyparse (void *YYPARSE_PARAM)
+#else
+int
+yyparse (YYPARSE_PARAM)
+    void *YYPARSE_PARAM;
+#endif
+#else /* ! YYPARSE_PARAM */
+#if (defined __STDC__ || defined __C99__FUNC__ \
+     || defined __cplusplus || defined _MSC_VER)
+int
+yyparse (void)
+#else
+int
+yyparse ()
+
+#endif
+#endif
+{
+    int yystate;
+    /* Number of tokens to shift before error messages enabled.  */
+    int yyerrstatus;
+
+    /* The stacks and their tools:
+       `yyss': related to states.
+       `yyvs': related to semantic values.
+
+       Refer to the stacks thru separate pointers, to allow yyoverflow
+       to reallocate them elsewhere.  */
+
+    /* The state stack.  */
+    yytype_int16 yyssa[YYINITDEPTH];
+    yytype_int16 *yyss;
+    yytype_int16 *yyssp;
+
+    /* The semantic value stack.  */
+    YYSTYPE yyvsa[YYINITDEPTH];
+    YYSTYPE *yyvs;
+    YYSTYPE *yyvsp;
+
+    YYSIZE_T yystacksize;
+
+  int yyn;
+  int yyresult;
+  /* Lookahead token as an internal (translated) token number.  */
+  int yytoken;
+  /* The variables used to return semantic value and location from the
+     action routines.  */
+  YYSTYPE yyval;
+
+#if YYERROR_VERBOSE
+  /* Buffer for error messages, and its allocated size.  */
+  char yymsgbuf[128];
+  char *yymsg = yymsgbuf;
+  YYSIZE_T yymsg_alloc = sizeof yymsgbuf;
+#endif
+
+#define YYPOPSTACK(N)   (yyvsp -= (N), yyssp -= (N))
+
+  /* The number of symbols on the RHS of the reduced rule.
+     Keep to zero when no symbol should be popped.  */
+  int yylen = 0;
+
+  yytoken = 0;
+  yyss = yyssa;
+  yyvs = yyvsa;
+  yystacksize = YYINITDEPTH;
+
+  YYDPRINTF ((stderr, "Starting parse\n"));
+
+  yystate = 0;
+  yyerrstatus = 0;
+  yynerrs = 0;
+  yychar = YYEMPTY; /* Cause a token to be read.  */
+
+  /* Initialize stack pointers.
+     Waste one element of value and location stack
+     so that they stay on the same level as the state stack.
+     The wasted elements are never initialized.  */
+  yyssp = yyss;
+  yyvsp = yyvs;
+
+  goto yysetstate;
+
+/*------------------------------------------------------------.
+| yynewstate -- Push a new state, which is found in yystate.  |
+`------------------------------------------------------------*/
+ yynewstate:
+  /* In all cases, when you get here, the value and location stacks
+     have just been pushed.  So pushing a state here evens the stacks.  */
+  yyssp++;
+
+ yysetstate:
+  *yyssp = yystate;
+
+  if (yyss + yystacksize - 1 <= yyssp)
+    {
+      /* Get the current used size of the three stacks, in elements.  */
+      YYSIZE_T yysize = yyssp - yyss + 1;
+
+#ifdef yyoverflow
+      {
+	/* Give user a chance to reallocate the stack.  Use copies of
+	   these so that the &'s don't force the real ones into
+	   memory.  */
+	YYSTYPE *yyvs1 = yyvs;
+	yytype_int16 *yyss1 = yyss;
+
+	/* Each stack pointer address is followed by the size of the
+	   data in use in that stack, in bytes.  This used to be a
+	   conditional around just the two extra args, but that might
+	   be undefined if yyoverflow is a macro.  */
+	yyoverflow (YY_("memory exhausted"),
+		    &yyss1, yysize * sizeof (*yyssp),
+		    &yyvs1, yysize * sizeof (*yyvsp),
+		    &yystacksize);
+
+	yyss = yyss1;
+	yyvs = yyvs1;
+      }
+#else /* no yyoverflow */
+# ifndef YYSTACK_RELOCATE
+      goto yyexhaustedlab;
+# else
+      /* Extend the stack our own way.  */
+      if (YYMAXDEPTH <= yystacksize)
+	goto yyexhaustedlab;
+      yystacksize *= 2;
+      if (YYMAXDEPTH < yystacksize)
+	yystacksize = YYMAXDEPTH;
+
+      {
+	yytype_int16 *yyss1 = yyss;
+	union yyalloc *yyptr =
+	  (union yyalloc *) YYSTACK_ALLOC (YYSTACK_BYTES (yystacksize));
+	if (! yyptr)
+	  goto yyexhaustedlab;
+	YYSTACK_RELOCATE (yyss_alloc, yyss);
+	YYSTACK_RELOCATE (yyvs_alloc, yyvs);
+#  undef YYSTACK_RELOCATE
+	if (yyss1 != yyssa)
+	  YYSTACK_FREE (yyss1);
+      }
+# endif
+#endif /* no yyoverflow */
+
+      yyssp = yyss + yysize - 1;
+      yyvsp = yyvs + yysize - 1;
+
+      YYDPRINTF ((stderr, "Stack size increased to %lu\n",
+		  (unsigned long int) yystacksize));
+
+      if (yyss + yystacksize - 1 <= yyssp)
+	YYABORT;
+    }
+
+  YYDPRINTF ((stderr, "Entering state %d\n", yystate));
+
+  if (yystate == YYFINAL)
+    YYACCEPT;
+
+  goto yybackup;
+
+/*-----------.
+| yybackup.  |
+`-----------*/
+yybackup:
+
+  /* Do appropriate processing given the current state.  Read a
+     lookahead token if we need one and don't already have one.  */
+
+  /* First try to decide what to do without reference to lookahead token.  */
+  yyn = yypact[yystate];
+  if (yypact_value_is_default (yyn))
+    goto yydefault;
+
+  /* Not known => get a lookahead token if don't already have one.  */
+
+  /* YYCHAR is either YYEMPTY or YYEOF or a valid lookahead symbol.  */
+  if (yychar == YYEMPTY)
+    {
+      YYDPRINTF ((stderr, "Reading a token: "));
+      yychar = YYLEX;
+    }
+
+  if (yychar <= YYEOF)
+    {
+      yychar = yytoken = YYEOF;
+      YYDPRINTF ((stderr, "Now at end of input.\n"));
+    }
+  else
+    {
+      yytoken = YYTRANSLATE (yychar);
+      YY_SYMBOL_PRINT ("Next token is", yytoken, &yylval, &yylloc);
+    }
+
+  /* If the proper action on seeing token YYTOKEN is to reduce or to
+     detect an error, take that action.  */
+  yyn += yytoken;
+  if (yyn < 0 || YYLAST < yyn || yycheck[yyn] != yytoken)
+    goto yydefault;
+  yyn = yytable[yyn];
+  if (yyn <= 0)
+    {
+      if (yytable_value_is_error (yyn))
+        goto yyerrlab;
+      yyn = -yyn;
+      goto yyreduce;
+    }
+
+  /* Count tokens shifted since error; after three, turn off error
+     status.  */
+  if (yyerrstatus)
+    yyerrstatus--;
+
+  /* Shift the lookahead token.  */
+  YY_SYMBOL_PRINT ("Shifting", yytoken, &yylval, &yylloc);
+
+  /* Discard the shifted token.  */
+  yychar = YYEMPTY;
+
+  yystate = yyn;
+  *++yyvsp = yylval;
+
+  goto yynewstate;
+
+
+/*-----------------------------------------------------------.
+| yydefault -- do the default action for the current state.  |
+`-----------------------------------------------------------*/
+yydefault:
+  yyn = yydefact[yystate];
+  if (yyn == 0)
+    goto yyerrlab;
+  goto yyreduce;
+
+
+/*-----------------------------.
+| yyreduce -- Do a reduction.  |
+`-----------------------------*/
+yyreduce:
+  /* yyn is the number of a rule to reduce with.  */
+  yylen = yyr2[yyn];
+
+  /* If YYLEN is nonzero, implement the default value of the action:
+     `$$ = $1'.
+
+     Otherwise, the following line sets YYVAL to garbage.
+     This behavior is undocumented and Bison
+     users should not rely upon it.  Assigning to YYVAL
+     unconditionally makes the parser a bit smaller, and it avoids a
+     GCC warning that YYVAL may be used uninitialized.  */
+  yyval = yyvsp[1-yylen];
+
+
+  YY_REDUCE_PRINT (yyn);
+  switch (yyn)
+    {
+        case 4:
+
+    { is_typedef = 0; is_extern = 0; current_name = NULL; decl_spec = NULL; }
+    break;
+
+  case 5:
+
+    { free_list(*(yyvsp[(2) - (2)]), NULL); *(yyvsp[(2) - (2)]) = NULL; }
+    break;
+
+  case 6:
+
+    { is_typedef = 1; }
+    break;
+
+  case 7:
+
+    { (yyval) = (yyvsp[(4) - (4)]); }
+    break;
+
+  case 8:
+
+    { is_typedef = 1; }
+    break;
+
+  case 9:
+
+    { (yyval) = (yyvsp[(3) - (3)]); }
+    break;
+
+  case 14:
+
+    { (yyval) = (yyvsp[(2) - (2)]); }
+    break;
+
+  case 15:
+
+    { (yyval) = (yyvsp[(2) - (2)]); }
+    break;
+
+  case 16:
+
+    { if (current_name) {
+		    struct string_list *decl = (*(yyvsp[(3) - (3)]))->next;
+		    (*(yyvsp[(3) - (3)]))->next = NULL;
+		    add_symbol(current_name,
+			       is_typedef ? SYM_TYPEDEF : SYM_NORMAL,
+			       decl, is_extern);
+		    current_name = NULL;
+		  }
+		  (yyval) = (yyvsp[(3) - (3)]);
+		}
+    break;
+
+  case 17:
+
+    { (yyval) = NULL; }
+    break;
+
+  case 19:
+
+    { struct string_list *decl = *(yyvsp[(1) - (1)]);
+		  *(yyvsp[(1) - (1)]) = NULL;
+		  add_symbol(current_name,
+			     is_typedef ? SYM_TYPEDEF : SYM_NORMAL, decl, is_extern);
+		  current_name = NULL;
+		  (yyval) = (yyvsp[(1) - (1)]);
+		}
+    break;
+
+  case 20:
+
+    { struct string_list *decl = *(yyvsp[(3) - (3)]);
+		  *(yyvsp[(3) - (3)]) = NULL;
+		  free_list(*(yyvsp[(2) - (3)]), NULL);
+		  *(yyvsp[(2) - (3)]) = decl_spec;
+		  add_symbol(current_name,
+			     is_typedef ? SYM_TYPEDEF : SYM_NORMAL, decl, is_extern);
+		  current_name = NULL;
+		  (yyval) = (yyvsp[(3) - (3)]);
+		}
+    break;
+
+  case 21:
+
+    { (yyval) = (yyvsp[(4) - (4)]) ? (yyvsp[(4) - (4)]) : (yyvsp[(3) - (4)]) ? (yyvsp[(3) - (4)]) : (yyvsp[(2) - (4)]) ? (yyvsp[(2) - (4)]) : (yyvsp[(1) - (4)]); }
+    break;
+
+  case 22:
+
+    { decl_spec = NULL; }
+    break;
+
+  case 24:
+
+    { decl_spec = *(yyvsp[(1) - (1)]); }
+    break;
+
+  case 25:
+
+    { decl_spec = *(yyvsp[(2) - (2)]); }
+    break;
+
+  case 26:
+
+    { /* Version 2 checksumming ignores storage class, as that
+		     is really irrelevant to the linkage.  */
+		  remove_node((yyvsp[(1) - (1)]));
+		  (yyval) = (yyvsp[(1) - (1)]);
+		}
+    break;
+
+  case 31:
+
+    { is_extern = 1; (yyval) = (yyvsp[(1) - (1)]); }
+    break;
+
+  case 32:
+
+    { is_extern = 0; (yyval) = (yyvsp[(1) - (1)]); }
+    break;
+
+  case 37:
+
+    { remove_node((yyvsp[(1) - (2)])); (*(yyvsp[(2) - (2)]))->tag = SYM_STRUCT; (yyval) = (yyvsp[(2) - (2)]); }
+    break;
+
+  case 38:
+
+    { remove_node((yyvsp[(1) - (2)])); (*(yyvsp[(2) - (2)]))->tag = SYM_UNION; (yyval) = (yyvsp[(2) - (2)]); }
+    break;
+
+  case 39:
+
+    { remove_node((yyvsp[(1) - (2)])); (*(yyvsp[(2) - (2)]))->tag = SYM_ENUM; (yyval) = (yyvsp[(2) - (2)]); }
+    break;
+
+  case 40:
+
+    { record_compound((yyvsp[(1) - (3)]), (yyvsp[(2) - (3)]), (yyvsp[(3) - (3)]), SYM_STRUCT); (yyval) = (yyvsp[(3) - (3)]); }
+    break;
+
+  case 41:
+
+    { record_compound((yyvsp[(1) - (3)]), (yyvsp[(2) - (3)]), (yyvsp[(3) - (3)]), SYM_UNION); (yyval) = (yyvsp[(3) - (3)]); }
+    break;
+
+  case 42:
+
+    { record_compound((yyvsp[(1) - (3)]), (yyvsp[(2) - (3)]), (yyvsp[(3) - (3)]), SYM_ENUM); (yyval) = (yyvsp[(3) - (3)]); }
+    break;
+
+  case 43:
+
+    { add_symbol(NULL, SYM_ENUM, NULL, 0); (yyval) = (yyvsp[(2) - (2)]); }
+    break;
+
+  case 44:
+
+    { (yyval) = (yyvsp[(2) - (2)]); }
+    break;
+
+  case 45:
+
+    { (yyval) = (yyvsp[(2) - (2)]); }
+    break;
+
+  case 56:
+
+    { (*(yyvsp[(1) - (1)]))->tag = SYM_TYPEDEF; (yyval) = (yyvsp[(1) - (1)]); }
+    break;
+
+  case 57:
+
+    { (yyval) = (yyvsp[(2) - (2)]) ? (yyvsp[(2) - (2)]) : (yyvsp[(1) - (2)]); }
+    break;
+
+  case 58:
+
+    { (yyval) = NULL; }
+    break;
+
+  case 61:
+
+    { (yyval) = (yyvsp[(2) - (2)]); }
+    break;
+
+  case 65:
+
+    { /* restrict has no effect in prototypes so ignore it */
+		  remove_node((yyvsp[(1) - (1)]));
+		  (yyval) = (yyvsp[(1) - (1)]);
+		}
+    break;
+
+  case 66:
+
+    { (yyval) = (yyvsp[(2) - (2)]); }
+    break;
+
+  case 68:
+
+    { if (current_name != NULL) {
+		    error_with_pos("unexpected second declaration name");
+		    YYERROR;
+		  } else {
+		    current_name = (*(yyvsp[(1) - (1)]))->string;
+		    (yyval) = (yyvsp[(1) - (1)]);
+		  }
+		}
+    break;
+
+  case 69:
+
+    { (yyval) = (yyvsp[(4) - (4)]); }
+    break;
+
+  case 70:
+
+    { (yyval) = (yyvsp[(4) - (4)]); }
+    break;
+
+  case 71:
+
+    { (yyval) = (yyvsp[(2) - (2)]); }
+    break;
+
+  case 72:
+
+    { (yyval) = (yyvsp[(3) - (3)]); }
+    break;
+
+  case 73:
+
+    { (yyval) = (yyvsp[(3) - (3)]); }
+    break;
+
+  case 74:
+
+    { (yyval) = (yyvsp[(2) - (2)]); }
+    break;
+
+  case 78:
+
+    { (yyval) = (yyvsp[(4) - (4)]); }
+    break;
+
+  case 79:
+
+    { (yyval) = (yyvsp[(4) - (4)]); }
+    break;
+
+  case 80:
+
+    { (yyval) = (yyvsp[(2) - (2)]); }
+    break;
+
+  case 81:
+
+    { (yyval) = (yyvsp[(3) - (3)]); }
+    break;
+
+  case 82:
+
+    { (yyval) = (yyvsp[(3) - (3)]); }
+    break;
+
+  case 83:
+
+    { (yyval) = (yyvsp[(2) - (2)]); }
+    break;
+
+  case 85:
+
+    { (yyval) = (yyvsp[(3) - (3)]); }
+    break;
+
+  case 86:
+
+    { (yyval) = NULL; }
+    break;
+
+  case 89:
+
+    { (yyval) = (yyvsp[(3) - (3)]); }
+    break;
+
+  case 90:
+
+    { (yyval) = (yyvsp[(2) - (2)]) ? (yyvsp[(2) - (2)]) : (yyvsp[(1) - (2)]); }
+    break;
+
+  case 91:
+
+    { (yyval) = (yyvsp[(2) - (2)]) ? (yyvsp[(2) - (2)]) : (yyvsp[(1) - (2)]); }
+    break;
+
+  case 93:
+
+    { (yyval) = NULL; }
+    break;
+
+  case 94:
+
+    { /* For version 2 checksums, we don't want to remember
+		     private parameter names.  */
+		  remove_node((yyvsp[(1) - (1)]));
+		  (yyval) = (yyvsp[(1) - (1)]);
+		}
+    break;
+
+  case 95:
+
+    { remove_node((yyvsp[(1) - (1)]));
+		  (yyval) = (yyvsp[(1) - (1)]);
+		}
+    break;
+
+  case 96:
+
+    { (yyval) = (yyvsp[(4) - (4)]); }
+    break;
+
+  case 97:
+
+    { (yyval) = (yyvsp[(4) - (4)]); }
+    break;
+
+  case 98:
+
+    { (yyval) = (yyvsp[(2) - (2)]); }
+    break;
+
+  case 99:
+
+    { (yyval) = (yyvsp[(3) - (3)]); }
+    break;
+
+  case 100:
+
+    { (yyval) = (yyvsp[(3) - (3)]); }
+    break;
+
+  case 101:
+
+    { struct string_list *decl = *(yyvsp[(2) - (3)]);
+		  *(yyvsp[(2) - (3)]) = NULL;
+		  add_symbol(current_name, SYM_NORMAL, decl, is_extern);
+		  (yyval) = (yyvsp[(3) - (3)]);
+		}
+    break;
+
+  case 102:
+
+    { (yyval) = NULL; }
+    break;
+
+  case 104:
+
+    { remove_list((yyvsp[(2) - (2)]), &(*(yyvsp[(1) - (2)]))->next); (yyval) = (yyvsp[(2) - (2)]); }
+    break;
+
+  case 105:
+
+    { (yyval) = (yyvsp[(3) - (3)]); }
+    break;
+
+  case 106:
+
+    { (yyval) = (yyvsp[(3) - (3)]); }
+    break;
+
+  case 107:
+
+    { (yyval) = NULL; }
+    break;
+
+  case 110:
+
+    { (yyval) = (yyvsp[(2) - (2)]); }
+    break;
+
+  case 111:
+
+    { (yyval) = (yyvsp[(3) - (3)]); }
+    break;
+
+  case 112:
+
+    { (yyval) = (yyvsp[(2) - (2)]); }
+    break;
+
+  case 113:
+
+    { (yyval) = NULL; }
+    break;
+
+  case 116:
+
+    { (yyval) = (yyvsp[(3) - (3)]); }
+    break;
+
+  case 117:
+
+    { (yyval) = (yyvsp[(2) - (2)]) ? (yyvsp[(2) - (2)]) : (yyvsp[(1) - (2)]); }
+    break;
+
+  case 118:
+
+    { (yyval) = (yyvsp[(2) - (2)]); }
+    break;
+
+  case 120:
+
+    { (yyval) = (yyvsp[(2) - (2)]); }
+    break;
+
+  case 121:
+
+    { (yyval) = NULL; }
+    break;
+
+  case 123:
+
+    { (yyval) = (yyvsp[(3) - (3)]); }
+    break;
+
+  case 124:
+
+    { (yyval) = (yyvsp[(4) - (4)]); }
+    break;
+
+  case 127:
+
+    {
+			const char *name = strdup((*(yyvsp[(1) - (1)]))->string);
+			add_symbol(name, SYM_ENUM_CONST, NULL, 0);
+		}
+    break;
+
+  case 128:
+
+    {
+			const char *name = strdup((*(yyvsp[(1) - (3)]))->string);
+			struct string_list *expr = copy_list_range(*(yyvsp[(3) - (3)]), *(yyvsp[(2) - (3)]));
+			add_symbol(name, SYM_ENUM_CONST, expr, 0);
+		}
+    break;
+
+  case 129:
+
+    { (yyval) = (yyvsp[(2) - (2)]); }
+    break;
+
+  case 130:
+
+    { (yyval) = NULL; }
+    break;
+
+  case 132:
+
+    { export_symbol((*(yyvsp[(3) - (5)]))->string); (yyval) = (yyvsp[(5) - (5)]); }
+    break;
+
+
+
+      default: break;
+    }
+  /* User semantic actions sometimes alter yychar, and that requires
+     that yytoken be updated with the new translation.  We take the
+     approach of translating immediately before every use of yytoken.
+     One alternative is translating here after every semantic action,
+     but that translation would be missed if the semantic action invokes
+     YYABORT, YYACCEPT, or YYERROR immediately after altering yychar or
+     if it invokes YYBACKUP.  In the case of YYABORT or YYACCEPT, an
+     incorrect destructor might then be invoked immediately.  In the
+     case of YYERROR or YYBACKUP, subsequent parser actions might lead
+     to an incorrect destructor call or verbose syntax error message
+     before the lookahead is translated.  */
+  YY_SYMBOL_PRINT ("-> $$ =", yyr1[yyn], &yyval, &yyloc);
+
+  YYPOPSTACK (yylen);
+  yylen = 0;
+  YY_STACK_PRINT (yyss, yyssp);
+
+  *++yyvsp = yyval;
+
+  /* Now `shift' the result of the reduction.  Determine what state
+     that goes to, based on the state we popped back to and the rule
+     number reduced by.  */
+
+  yyn = yyr1[yyn];
+
+  yystate = yypgoto[yyn - YYNTOKENS] + *yyssp;
+  if (0 <= yystate && yystate <= YYLAST && yycheck[yystate] == *yyssp)
+    yystate = yytable[yystate];
+  else
+    yystate = yydefgoto[yyn - YYNTOKENS];
+
+  goto yynewstate;
+
+
+/*------------------------------------.
+| yyerrlab -- here on detecting error |
+`------------------------------------*/
+yyerrlab:
+  /* Make sure we have latest lookahead translation.  See comments at
+     user semantic actions for why this is necessary.  */
+  yytoken = yychar == YYEMPTY ? YYEMPTY : YYTRANSLATE (yychar);
+
+  /* If not already recovering from an error, report this error.  */
+  if (!yyerrstatus)
+    {
+      ++yynerrs;
+#if ! YYERROR_VERBOSE
+      yyerror (YY_("syntax error"));
+#else
+# define YYSYNTAX_ERROR yysyntax_error (&yymsg_alloc, &yymsg, \
+                                        yyssp, yytoken)
+      {
+        char const *yymsgp = YY_("syntax error");
+        int yysyntax_error_status;
+        yysyntax_error_status = YYSYNTAX_ERROR;
+        if (yysyntax_error_status == 0)
+          yymsgp = yymsg;
+        else if (yysyntax_error_status == 1)
+          {
+            if (yymsg != yymsgbuf)
+              YYSTACK_FREE (yymsg);
+            yymsg = (char *) YYSTACK_ALLOC (yymsg_alloc);
+            if (!yymsg)
+              {
+                yymsg = yymsgbuf;
+                yymsg_alloc = sizeof yymsgbuf;
+                yysyntax_error_status = 2;
+              }
+            else
+              {
+                yysyntax_error_status = YYSYNTAX_ERROR;
+                yymsgp = yymsg;
+              }
+          }
+        yyerror (yymsgp);
+        if (yysyntax_error_status == 2)
+          goto yyexhaustedlab;
+      }
+# undef YYSYNTAX_ERROR
+#endif
+    }
+
+
+
+  if (yyerrstatus == 3)
+    {
+      /* If just tried and failed to reuse lookahead token after an
+	 error, discard it.  */
+
+      if (yychar <= YYEOF)
+	{
+	  /* Return failure if at end of input.  */
+	  if (yychar == YYEOF)
+	    YYABORT;
+	}
+      else
+	{
+	  yydestruct ("Error: discarding",
+		      yytoken, &yylval);
+	  yychar = YYEMPTY;
+	}
+    }
+
+  /* Else will try to reuse lookahead token after shifting the error
+     token.  */
+  goto yyerrlab1;
+
+
+/*---------------------------------------------------.
+| yyerrorlab -- error raised explicitly by YYERROR.  |
+`---------------------------------------------------*/
+yyerrorlab:
+
+  /* Pacify compilers like GCC when the user code never invokes
+     YYERROR and the label yyerrorlab therefore never appears in user
+     code.  */
+  if (/*CONSTCOND*/ 0)
+     goto yyerrorlab;
+
+  /* Do not reclaim the symbols of the rule which action triggered
+     this YYERROR.  */
+  YYPOPSTACK (yylen);
+  yylen = 0;
+  YY_STACK_PRINT (yyss, yyssp);
+  yystate = *yyssp;
+  goto yyerrlab1;
+
+
+/*-------------------------------------------------------------.
+| yyerrlab1 -- common code for both syntax error and YYERROR.  |
+`-------------------------------------------------------------*/
+yyerrlab1:
+  yyerrstatus = 3;	/* Each real token shifted decrements this.  */
+
+  for (;;)
+    {
+      yyn = yypact[yystate];
+      if (!yypact_value_is_default (yyn))
+	{
+	  yyn += YYTERROR;
+	  if (0 <= yyn && yyn <= YYLAST && yycheck[yyn] == YYTERROR)
+	    {
+	      yyn = yytable[yyn];
+	      if (0 < yyn)
+		break;
+	    }
+	}
+
+      /* Pop the current state because it cannot handle the error token.  */
+      if (yyssp == yyss)
+	YYABORT;
+
+
+      yydestruct ("Error: popping",
+		  yystos[yystate], yyvsp);
+      YYPOPSTACK (1);
+      yystate = *yyssp;
+      YY_STACK_PRINT (yyss, yyssp);
+    }
+
+  *++yyvsp = yylval;
+
+
+  /* Shift the error token.  */
+  YY_SYMBOL_PRINT ("Shifting", yystos[yyn], yyvsp, yylsp);
+
+  yystate = yyn;
+  goto yynewstate;
+
+
+/*-------------------------------------.
+| yyacceptlab -- YYACCEPT comes here.  |
+`-------------------------------------*/
+yyacceptlab:
+  yyresult = 0;
+  goto yyreturn;
+
+/*-----------------------------------.
+| yyabortlab -- YYABORT comes here.  |
+`-----------------------------------*/
+yyabortlab:
+  yyresult = 1;
+  goto yyreturn;
+
+#if !defined(yyoverflow) || YYERROR_VERBOSE
+/*-------------------------------------------------.
+| yyexhaustedlab -- memory exhaustion comes here.  |
+`-------------------------------------------------*/
+yyexhaustedlab:
+  yyerror (YY_("memory exhausted"));
+  yyresult = 2;
+  /* Fall through.  */
+#endif
+
+yyreturn:
+  if (yychar != YYEMPTY)
+    {
+      /* Make sure we have latest lookahead translation.  See comments at
+         user semantic actions for why this is necessary.  */
+      yytoken = YYTRANSLATE (yychar);
+      yydestruct ("Cleanup: discarding lookahead",
+                  yytoken, &yylval);
+    }
+  /* Do not reclaim the symbols of the rule which action triggered
+     this YYABORT or YYACCEPT.  */
+  YYPOPSTACK (yylen);
+  YY_STACK_PRINT (yyss, yyssp);
+  while (yyssp != yyss)
+    {
+      yydestruct ("Cleanup: popping",
+		  yystos[*yyssp], yyvsp);
+      YYPOPSTACK (1);
+    }
+#ifndef yyoverflow
+  if (yyss != yyssa)
+    YYSTACK_FREE (yyss);
+#endif
+#if YYERROR_VERBOSE
+  if (yymsg != yymsgbuf)
+    YYSTACK_FREE (yymsg);
+#endif
+  /* Make sure YYID is used.  */
+  return YYID (yyresult);
+}
+
+
+
+
+
+static void
+yyerror(const char *e)
+{
+  error_with_pos("%s", e);
+}
+
diff -rubBN linux-3.2.51/scripts/genksyms/parse.tab.h ../split-release/release/linux-3.2.51/scripts/genksyms/parse.tab.h
--- linux-3.2.51/scripts/genksyms/parse.tab.h	1970-01-01 01:00:00.000000000 +0100
+++ ../split-release/release/linux-3.2.51/scripts/genksyms/parse.tab.h	2015-09-01 05:42:46.000000000 +0200
@@ -0,0 +1,95 @@
+/* A Bison parser, made by GNU Bison 2.5.  */
+
+/* Bison interface for Yacc-like parsers in C
+   
+      Copyright (C) 1984, 1989-1990, 2000-2011 Free Software Foundation, Inc.
+   
+   This program is free software: you can redistribute it and/or modify
+   it under the terms of the GNU General Public License as published by
+   the Free Software Foundation, either version 3 of the License, or
+   (at your option) any later version.
+   
+   This program is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+   
+   You should have received a copy of the GNU General Public License
+   along with this program.  If not, see <http://www.gnu.org/licenses/>.  */
+
+/* As a special exception, you may create a larger work that contains
+   part or all of the Bison parser skeleton and distribute that work
+   under terms of your choice, so long as that work isn't itself a
+   parser generator using the skeleton or a modified version thereof
+   as a parser skeleton.  Alternatively, if you modify or redistribute
+   the parser skeleton itself, you may (at your option) remove this
+   special exception, which will cause the skeleton and the resulting
+   Bison output files to be licensed under the GNU General Public
+   License without this special exception.
+   
+   This special exception was added by the Free Software Foundation in
+   version 2.2 of Bison.  */
+
+
+/* Tokens.  */
+#ifndef YYTOKENTYPE
+# define YYTOKENTYPE
+   /* Put the tokens into the symbol table, so that GDB and other debuggers
+      know about them.  */
+   enum yytokentype {
+     ASM_KEYW = 258,
+     ATTRIBUTE_KEYW = 259,
+     AUTO_KEYW = 260,
+     BOOL_KEYW = 261,
+     CHAR_KEYW = 262,
+     CONST_KEYW = 263,
+     DOUBLE_KEYW = 264,
+     ENUM_KEYW = 265,
+     EXTERN_KEYW = 266,
+     EXTENSION_KEYW = 267,
+     FLOAT_KEYW = 268,
+     INLINE_KEYW = 269,
+     INT_KEYW = 270,
+     LONG_KEYW = 271,
+     REGISTER_KEYW = 272,
+     RESTRICT_KEYW = 273,
+     SHORT_KEYW = 274,
+     SIGNED_KEYW = 275,
+     STATIC_KEYW = 276,
+     STRUCT_KEYW = 277,
+     TYPEDEF_KEYW = 278,
+     UNION_KEYW = 279,
+     UNSIGNED_KEYW = 280,
+     VOID_KEYW = 281,
+     VOLATILE_KEYW = 282,
+     TYPEOF_KEYW = 283,
+     EXPORT_SYMBOL_KEYW = 284,
+     ASM_PHRASE = 285,
+     ATTRIBUTE_PHRASE = 286,
+     BRACE_PHRASE = 287,
+     BRACKET_PHRASE = 288,
+     EXPRESSION_PHRASE = 289,
+     CHAR = 290,
+     DOTS = 291,
+     IDENT = 292,
+     INT = 293,
+     REAL = 294,
+     STRING = 295,
+     TYPE = 296,
+     OTHER = 297,
+     FILENAME = 298
+   };
+#endif
+
+
+
+#if ! defined YYSTYPE && ! defined YYSTYPE_IS_DECLARED
+typedef int YYSTYPE;
+# define YYSTYPE_IS_TRIVIAL 1
+# define yystype YYSTYPE /* obsolescent; will be withdrawn */
+# define YYSTYPE_IS_DECLARED 1
+#endif
+
+extern YYSTYPE yylval;
+
+
diff -rubBN linux-3.2.51/security/tomoyo/builtin-policy.h ../split-release/release/linux-3.2.51/security/tomoyo/builtin-policy.h
--- linux-3.2.51/security/tomoyo/builtin-policy.h	1970-01-01 01:00:00.000000000 +0100
+++ ../split-release/release/linux-3.2.51/security/tomoyo/builtin-policy.h	2016-04-19 15:20:02.224574000 +0200
@@ -0,0 +1,12 @@
+static char tomoyo_builtin_profile[] __initdata =
+"";
+static char tomoyo_builtin_exception_policy[] __initdata =
+"initialize_domain /sbin/modprobe from any\n"
+"initialize_domain /sbin/hotplug from any\n"
+"";
+static char tomoyo_builtin_domain_policy[] __initdata =
+"";
+static char tomoyo_builtin_manager[] __initdata =
+"";
+static char tomoyo_builtin_stat[] __initdata =
+"";
diff -rubBN linux-3.2.51/security/tomoyo/policy/exception_policy.conf ../split-release/release/linux-3.2.51/security/tomoyo/policy/exception_policy.conf
--- linux-3.2.51/security/tomoyo/policy/exception_policy.conf	1970-01-01 01:00:00.000000000 +0100
+++ ../split-release/release/linux-3.2.51/security/tomoyo/policy/exception_policy.conf	2016-04-19 15:20:02.188592000 +0200
@@ -0,0 +1,2 @@
+initialize_domain /sbin/modprobe from any
+initialize_domain /sbin/hotplug from any
